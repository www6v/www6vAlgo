[{"id":0,"href":"/www6vAlgo/docs/RL/framework/verl/","title":"HybridFlow[veRL]","section":"Framework","content":" 论文 # HybridFlow: A Flexible and Efficient RLHF Framework\n解读 # HybridFlow[veRL]\n"},{"id":1,"href":"/www6vAlgo/docs/Agent/Agentic-RL/survey/survey/","title":"(Survey)Agentic RL","section":"Survey","content":" 论文 # The Landscape of Agentic Reinforcement Learning for LLMs: A Survey git 解读 # (Survey)Agentic RL\n参考 # The Landscape of Agentic Reinforcement Learning for LLMs: A Survey\nThe Landscape of Agentic Reinforcement Learning for LLMs: A Survey\n"},{"id":2,"href":"/www6vAlgo/docs/Agent/Agentic-RL/Tool/OTC/","title":"OTC","section":"Tool","content":" 参考 # 2025年大模型agent rl训练多轮planning技术TORL,ToolRL, RAGEN,OTC,SkyRL-v0, GiGPO,Tool-N1 ,ARTIST, ZeroTIR, GRPO\n"},{"id":3,"href":"/www6vAlgo/docs/Agent/Agentic-RL/Tool/ReTool/","title":"ReTool","section":"Tool","content":" 论文 # ReTool: Reinforcement Learning for Strategic Tool Use in LLMs\nhttps://retool-rl.github.io/\n引言 # 大型语言模型（LLM）通过思维链提示和强化学习等技术，在推理任务中取得了显著进展。然而，这些基于文本的方法在需要精确数值计算或符号操作的任务中常常力不从心。ReTool 通过引入一个强化学习框架来解决这一限制，该框架教导 LLM 如何战略性地将代码解释器整合到其推理过程中。\n图1：传统基于文本的强化学习训练（上）与 ReTool 采用交错代码执行的方法（下）的比较。ReTool 允许在生成过程中与代码沙盒进行动态交互。\n这项工作表明，LLM 不仅能学习如何使用计算工具，还能通过基于结果的反馈学习何时以及为何调用它们。这代表着与仅仅模仿预定义工具使用模式的监督微调方法相比，取得了重大进步。\n方法论 # ReTool 采用两阶段训练框架，结合监督微调和强化学习，以开发战略性工具使用能力。\n冷启动监督微调 # 第一阶段通过细致的数据整理和监督训练，建立基础的工具使用能力：\n数据构建过程：\n从 Open-Thoughts 等来源初步收集高质量数学推理数据 （ \\( D_{init} \\) ） 使用人类专家和 DeepSeek-R1 进行双重验证以确保质量 使用结构化提示将基于文本的推理自动转换为代码集成推理（ \\( D_{CI} \\) ） 对代码集成数据集进行额外的格式和答案验证 转换过程使用系统化的提示模板，该模板识别文本推理中的计算步骤，并将其替换为可执行的代码片段及其输出。这创建了演示何时以及如何调用代码解释器的训练数据。\n带有交错代码执行的强化学习 # 核心创新在于 ReTool 的强化学习训练过程，该过程动态集成了实时代码执行：\n训练算法： 采用简单基于准确性的奖励函数的近端策略优化（PPO）：\n\\[\rR(a, \\hat{a}) = \\begin{cases} 1 \u0026 \\text{if predicted answer } \\hat{a} \\text{ equals ground truth } a \\\\ -1 \u0026 \\text{otherwise} \\end{cases}\r\\] 展开过程： 与传统模型生成纯文本的强化学习不同，ReTool 的展开过程包括：\n模型生成自然语言推理 当代码调用被触发（通过 \u0026lt;code\u0026gt; 标签）时，生成暂停 代码被提取并在外部沙盒环境中执行 执行结果（成功或错误）通过 \u0026lt;interpreter\u0026gt; 标签反馈给模型 模型使用解释器反馈继续生成 过程重复直到得到最终答案或达到最大长度 技术优化：\n解释器反馈token在损失计算中被屏蔽，以保证训练稳定性 KV缓存重用最小化了展开过程中的内存开销 异步代码沙盒支持并行执行和更快的训练 结果与分析 # ReTool 在具有挑战性的数学推理基准测试中，相较于基线方法展现出显著改进。\n性能指标 # AIME 基准测试结果：\nAIME 2024：67.0% 准确率（对比基于文本的强化学习基线为 40.0%） AIME 2025：49.3% 准确率（对比基于文本的强化学习基线为 36.7%） 训练效率：在 400 步内实现了卓越性能，而基线则需要 1000+ 步 图2：训练曲线显示了 ReTool 在 AIME 基准测试中相较于基于文本的强化学习方法，具有快速的改进和卓越的最终性能。\n高级骨干模型结果： 以 DeepSeek-R1-Distill-Qwen-32B 作为基础模型，ReTool 实现了更高的性能：\nAIME 2024：72.5% 准确率 AIME 2025：54.3% 准确率 涌现认知行为 # 分析揭示了在强化学习训练过程中出现的几个显著的涌现特性：\n图3：ReTool 训练过程中行为变化的详细分析，展示了响应长度、代码使用模式和执行成功率的演变。\n关键涌现行为：\n令牌效率： 平均响应长度减少 40%（从 10k 减少到 6k 令牌） 代码采用： 代码使用率增加到近 98% 的响应 战略时机： 模型学会了在推理过程的早期调用代码 自我纠正： 自主错误检测和代码修订能力 多样化工具使用： 扩展了基本计算之外的功能，包括验证、优化和分析 自我纠正能力 # 最重要的发现之一是模型能够自主纠正代码执行错误：\n图4：涌现的自我纠正行为示例，模型识别并修复了其生成的代码中的 NameError。\n当遇到 NameError: name 'greedy' is not defined 等错误时，模型通过反思错误并生成修正后的代码，展示了元认知意识，而无需为此行为进行明确的训练。\n工具使用演变 # 这项研究提供了关于代码使用模式在训练期间如何演变的见解：\n图5：词云分析显示了 ReTool 训练前后代码目的的演变，展示了工具使用策略多样性的增加。\n分析表明，虽然“计算”仍然是主要目的，但强化学习训练导致了更多样化的代码应用，包括几何分析、概率计算和解决方案验证。\n比较分析 # ReTool 的方法比传统的基于文本的推理具有明显的优势：\n图6：在复杂数学问题上，基于文本的推理（左）与代码集成推理（右）的并排比较，突出了精度和效率的提升。\n比较表明，代码集成消除了容易出错的冗长手动计算，使模型能够专注于更高级别的战略推理，同时将精确计算卸载到解释器。\n意义和影响 # ReTool 代表了混合神经-符号人工智能系统的重大进步，它证明了大型语言模型可以通过强化学习而不是单纯的模仿来学习战略性工具使用。这项工作的贡献包括：\n技术贡献：\n首次成功将强化学习应用于大规模战略性代码解释器使用 展示了无需明确训练即可涌现的自我纠正能力 高效的训练框架，以更少的步骤实现了卓越的性能 更广泛的影响：\n弥合了神经网络模式识别与符号计算之间的鸿沟 为跨不同领域更通用化的工具使用奠定了基础 通过明确的计算步骤增强了可解释性 展示了自主发现问题解决策略的潜力 这项研究确立了 ReTool 作为开发更强大、更通用的人工智能系统的一种引人注目的方法，这些系统能够有效地结合神经网络和符号计算范式的优势。\n参考 # 通过工具增强 LLM Agent 能力：veRL+ReTool 的完整实践指南 https://www.alphaxiv.org/zh/overview/2504.11536v2 "},{"id":4,"href":"/www6vAlgo/docs/Agent/Deep-Research/Survey/Survey/","title":"Survey","section":"Survey","content":" 论文 # Reinforcement Learning Foundations for Deep Research Systems: A Survey\nMethod # 方法 优化目标 数据形式 关键短板 SFT 模仿单步 (q, a) 对 暴露偏差、无法纠错 DPO 偏好排序 (q, a⁺, a⁻) 无状态、信用分配短视 RL 最大化回报 (q, τ, r) 需可验证奖励+探索策略 RL METHODS FOR AGENTIC RESEARCH # TRAINING REGIME AND OPTIMIZATION STRUCTURE # REWARD DESIGN AND CREDIT ASSIGNMENT # 结果奖励（Outcome-only）\n步骤奖励（Step-level）\nframework # 参考 # 2篇最新论文，把Deep Research讲透了~\n"},{"id":5,"href":"/www6vAlgo/docs/Agent/Agentic-RL/Tool/ToolRL/","title":"ToolRL","section":"Tool","content":" 论文 # ToolRL: Reward is All Tool Learning Needs\n为了确定最佳奖励策略，探索了四个关键维度的各种奖励配置：(1) 奖励类型（奖励哪些方面），(2) 奖励尺度（奖励多少），(3) 奖励粒度（奖励信号的详细程度），以及 (4) 奖励动态（奖励如何随时间演变）。通过大量的实验确定了最符合主体工具使用情况的奖励设计，并揭示了奖励对于调用工具的 LLM 而言“有用”的原因。论文得出的核心见解总结如下：\n推理轨迹越长并不一定越好，而且过长的奖励可能会降低性能。 动态奖励尺度有助于模型从简单行为平稳过渡到复杂行为。 细粒度的奖励分解可实现更稳定、更有效的学习。 论文得出的核心见解总结如下：\n推理轨迹越长并不一定越好，而且过长的奖励可能会降低性能。 动态奖励尺度有助于模型从简单行为平稳过渡到复杂行为。 细粒度的奖励分解可实现更稳定、更有效的学习。 Format Reward Correctness Reward\n参考 # 2025年大模型agent rl训练多轮planning技术TORL,ToolRL, RAGEN,OTC,SkyRL-v0, GiGPO,Tool-N1 ,ARTIST, ZeroTIR, GRPO\n"},{"id":6,"href":"/www6vAlgo/docs/Agent/Deep-Research/Search/Search-R1/","title":"Search-R1","section":"Search","content":" 论文 # Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning https://github.com/PeterGriffinJin/Search-R1 Methods # 详细方法和步骤:\n将搜索引擎建模为环境的一部分： SEARCH-R1将搜索引起作为环境的一部分， 让模型与环境交互，从而得到 reward。 支持多轮检索和推理： SEARCH-R1通过特定的标签（\u0026lt;search\u0026gt;, \u0026lt;/search\u0026gt;, \u0026lt;information\u0026gt;, \u0026lt;/information\u0026gt;, \u0026lt;think\u0026gt;, \u0026lt;/think\u0026gt;, \u0026lt;answer\u0026gt;, \u0026lt;/answer\u0026gt;）来支持多轮检索和推理。 优化算法兼容性： SEARCH-R1 与各种 RL 算法兼容，包括 PPO 和 GRPO。 简单结果奖励函数： 避免复杂的基于过程的奖励, 采用简单的基于结果的奖励函数 （字符串匹配作为reward!!!）。 总结 # 结论1: SEARCH-R1 显著提升了LLM在需要实时外部知识的复杂推理任务中的能力。 通过强化学习，LLM可以自主生成查询并有效利用检索到的信息，优于传统的RAG方法。\n结论2: SEARCH-R1在不同LLM架构和训练方法上具有广泛的适用性。 实验结果表明，无论使用基础模型还是指令调整模型，SEARCH-R1都能带来显著的性能提升，且对不同的RL算法（如PPO和GRPO）具有兼容性。\n结论3: SEARCH-R1有很强的实用价值。 SEARCH-R1能够显著提高LLM在需要实时外部知识的复杂推理任务中的能力。 可以用于智能问答，智能助手等领域。\n参考 # Search-R1：让大模型学会“检索+推理”的新范式 1xx. 【论文解读】Search-R1：强化学习如何教会 LLM 自主搜索？\n1xx. 有个学术的会议\n1xx. Search-R1：让 LLM 学会 “边搜边想”，强化学习赋能检索增强推理\n"},{"id":7,"href":"/www6vAlgo/docs/LLM/MoE/ModelMOE/","title":"(原理)Visual  MOE","section":"MOE","content":" Visual MOE # (原理)Visual MOE\n"},{"id":8,"href":"/www6vAlgo/docs/RL/core/GRPO-family/GRPODeepseek/","title":"(实战)GRPO","section":"GRPO Family","content":"\nGRPO # (实战)GRPO\n"},{"id":9,"href":"/www6vAlgo/docs/LLM/Reasoning/kimi/k2/","title":"(翻译)kimi k2","section":"kimi","content":" 论文 # https://github.com/MoonshotAI/Kimi-K2?tab=readme-ov-file\nhttps://moonshotai.github.io/Kimi-K2/\nhttps://arxiv.org/pdf/2507.20534\nKimi K2 # 概述 什么是 Kimi K2？ 模型架构 预训练（Pre-Training） 后训练（Post-Training） 评测结果 MuonClip 优化器 代理型智能（Agentic Intelligence）作为一种新范式 参考文献 概述 # Kimi K2 标志着大语言模型（LLM）从传统静态模型向代理型智能（agentic intelligence） 的根本性转变——模型不再仅依赖预收集的数据集进行模仿学习，而是通过与动态环境主动交互来持续学习。这一范式旨在赋予模型自主感知、规划、推理与行动的能力，使其行为可超越训练分布的边界。\n其深远意义在于：模型不再受限于复现人类撰写语料的被动角色，而是能通过合成数据、探索环境、实时适应，发展出全新能力（novel competencies），为超人推理、工具编排、软件开发及现实世界自主性开辟路径。\n代理型智能也重构了训练挑战本身：\n预训练阶段不仅需高效灌输广泛先验知识，还需在万亿 token 规模下保持训练稳定； 后训练阶段则需生成可执行、可验证的行为——即便真实语料中这类“代理轨迹”极为稀少。 Kimi K2 通过三大核心技术应对上述挑战：\n✅ MuonClip 稳定化训练\n✅ 大规模代理行为数据合成\n✅ 多信号强化学习（RL）\n最终打造出当前最强开源非“思考型”模型之一（即非 CoT-heavy 推理延迟模型，强调 reflexive 快速响应能力）。\n👉 您可在线体验：kimi.com\n下图为 Kimi K2 整体训练流程概览：\n什么是 Kimi K2？ # Kimi K2 是一个混合专家模型（Mixture-of-Experts, MoE），总参数量达 1 万亿（1T），但每次前向传播仅激活其中 320 亿（32B） 参数——在享受超大规模表征能力的同时，显著提升计算效率。\n更重要的是：Kimi K2 不只是聊天模型。其核心设计目标是实现代理行为（agentic behavior）——即具备规划、推理、调用工具、自主执行多步骤任务的能力。\n月之暗面（Moonshot AI）开源了两个版本：\nKimi-K2-Base：原始预训练模型，适用于科研与微调； Kimi-K2-Instruct：经后训练优化的指令遵循模型，专为快速响应类任务（reflexive tasks） 设计。 模型架构 # Kimi K2 采用与 DeepSeek-V3 类似的 MoE 架构，但存在关键差异：\n🔹 更多专家（experts）\n🔹 更少注意力头（attention heads）\n🔹 改进的负载均衡机制——防止“专家坍缩”（仅少数专家被激活）\n具体结构为 32-of-1024 MoE：即每步仅激活 32 位专家（激活率约 3.1%）。尽管稀疏性极高，模型在各类任务上仍保持强劲性能。\n下图为简化架构对比图（来源：Sebastian Raschka）：\n此架构使 Kimi K2 能在不显著增加推理成本的前提下，扩展至万亿参数规模；路由机制确保不同模块专业化分工，训练动态则保障专家均衡参与。 预训练（Pre-Training） # MuonClip 优化器 # Token 效率与稳定性融合：K2 引入 MuonClip，将高 token 效率的 Muon 优化器与QK-Clip 机制相结合。\nMuon 可提升单位 token 的学习信号强度，但在超大规模下易导致注意力 logits 爆炸； QK-Clip 通过动态缩放查询（Query）与键（Key）权重，抑制 logits 增长（当其超过阈值 τ 时）。 关键优势：缩放不改变当前步的前向/反向计算，从而保持优化动力学不变。 最终实现：15.5T token 全程无发散稳定训练【7†source】。 公式化裁剪：对每个注意力头 $h$，计算最大注意力 logit：\n$$ S^{h}{\\max} = \\frac{1}{\\sqrt{d}} \\max{X \\in B} \\max_{i,j} Q^{h}i {K^h_j}^\\top $$ 若 $S^{h}{\\max} \u0026gt; \\tau$，则对权重缩放：\n$$ W^h_q \\leftarrow W^h_q \\cdot \\sqrt{\\gamma_h}, \\quad W^h_k \\leftarrow W^h_k \\cdot \\sqrt{\\gamma_h}, \\quad \\text{其中}\\ \\gamma_h = \\min(1, \\tau / S^{h}_{\\max}) $$ → 逐头裁剪，最小化干预，确保 logits 有界。\nToken 利用率与合成改写 # 知识改写（Knowledge rephrasing）：\n替代易导致过拟合的多轮重复训练； 采用合成改写流水线：\n✅ 多风格/多视角 prompt 指导，提升语言多样性；\n✅ 分块自回归重写，保障全局连贯性；\n✅ 语义保真度校验，确保内容一致。 效果：SimpleQA 准确率从 23.8%（多轮训练）→ 28.9%（10 倍改写）。 数学数据增强：\n将数学语料重写为分步学习笔记（learning notes）（灵感源自 SwallowMath），迫使模型内化推理步骤； 多语言数学资料统一翻译为英文，增强多样性与推理鲁棒性。 模型架构细节 # 万亿参数 MoE 的稀疏扩展：\n实际使用 384 专家，稀疏度 48（每 token 激活 8 专家）； 实验表明：更高稀疏度可在同等 FLOPs 下降低验证损失； 相比稀疏度 8，性能相当时 FLOPs 减少 1.69 倍。 注意力头数量权衡：\n未像 DeepSeek-V3 那样将头数设为层数 2 倍，而是减半至 64 头； 原因：增加头数仅带来 0.5–1.2% 损失改善，却导致 128k 上下文推理 FLOPs 增加高达 83%； K2 选择优先保障长上下文效率。 训练基础设施 # 集群与并行：\n运行于 NVIDIA H800 集群（每节点 2TB RAM）； 采用 16 路流水线并行 + 16 路专家并行 + ZeRO-1 数据并行。 显存优化：\nFP8-E4M3 激活压缩； SwiGLU/LayerNorm 层选择性重计算； CPU 激活卸载——在有限 GPU 显存下实现万亿级稳定训练。 训练方案：\n总 token 数：15.5T； 学习率：前 10T token 固定 2e⁻⁴，后 5.5T 采用余弦衰减，末期加温退火； 上下文长度渐进扩展：4k → 32k → 128k（通过 YaRN 实现）； 全程训练曲线零震荡。 后训练（Post-Training） # 基于代理行为的监督微调（SFT） # 指令多样性：通过人类标注、prompt 工程改写、判别模型自动过滤构建高质量指令数据。\n代理行为合成流水线（受 ACEBench 启发）：\n1️⃣ 生成合成/真实工具规范（\u0026gt;20k 工具，\u0026gt;3k MCP 协议）；\n2️⃣ 构建具不同工具集的多样化代理；\n3️⃣ 生成基于评分标准（rubric）验证的任务；\n4️⃣ 在模拟/真实执行沙盒（如带单元测试的编程环境）中生成多轮轨迹，并按 rubric 评分。\n→ 同时保障覆盖广度与行为真实性【7†source】。\n强化学习（RL） # 双奖励信号设计：\n🔹 可验证任务奖励：数学、逻辑、编程（通过可执行判官，如单元测试）；\n🔹 自评 rubric 奖励：创意、安全性等主观维度；\n判别模型（critic）持续优化 rubric 权重，将主观判断锚定于可验证性能之上。 训练创新点：\n预算控制：惩罚冗长输出； PTX-loss 融合：保留高质量预训练知识； 温度衰减调度：平衡探索与收敛；\n→ 不仅提升准确率，更实现与人类复杂价值观的对齐。 RL 基础设施 # 检查点引擎：\n摒弃传统 NFS 参数重排，改用分布式全参数广播； 实现万亿参数规模下 \u0026lt;30 秒 的检查点同步更新。 代理 rollout 优化：\n部分 rollout（partial rollouts）； 环境并行化； 延迟均摊技术； RL 框架采用 Gym 式接口，可无缝集成任意新环境。 评测结果 # 编程与工程能力 # 基准 Kimi K2 对比模型 SWE-bench Verified（代理单次尝试） 65.8% Claude 4 Opus: 72.5% LiveCodeBench v6 53.7% \u0026gt; GPT-4.1, Claude Sonnet OJBench 27.1% \u0026gt; Gemini 2.5 Flash (19.5%) → K2 是当前最强开源模型，适用于真实工程与竞赛编程场景。\n代理工具使用能力 # 基准 Kimi K2 表现 Tau2-Bench（多轮工具编排） 66.1 Pass@1 — ACEBench 76.5% 准确率 \u0026gt; DeepSeek-V3 \u0026amp; Claude Sonnet → 展现强大具身化工具推理能力（grounded tool-use reasoning），为代理智能核心支柱。\n数学与 STEM 能力 # 基准 Kimi K2 说明 AIME 2025 49.5% \u0026gt; Qwen3 (24.7%) GPQA-Diamond 75.1% ≈ Claude Opus HMMT 2025 38.8% 开源最佳 ✅ 验证了学习笔记改写与可验证 RL 任务整合的有效性。\n通用能力与长上下文 # 基准 Kimi K2 备注 MMLU 89.5% ≈ 闭源顶尖模型 MMLU-Redux 92.7% 开源最佳 LongBench v2 49.1% ≈ GPT-4.1 DROP 93.5% 事实推理准确率 → K2 不仅专精特定领域，更是全面稳健的通用模型，覆盖推理、事实性与长上下文理解。\n下图为 Kimi K2 与 DeepSeek-V3、Claude 4 Opus、GPT-4.1、Gemini 2.5、Qwen3 的多任务性能对比：\nMuonClip 优化器详解 # 训练万亿参数模型的最大难点在于训练不稳定性，尤其表现为注意力 logits 爆炸。 Kimi K2 提出定制优化器 MuonClip——Muon 优化器的增强版，专为大规模 MoE 设计。 核心创新：qk-clip 机制 在每步训练中动态重缩放 Q/K 权重，使 logits 始终处于安全范围； 保障训练过程平滑收敛，为超大模型训练提供基础设施级保障。 代理型智能（Agentic Intelligence）作为一种新范式 # 代理型智能从根本上重构了 LLM 的运作范式——从被动文本补全引擎，进化为自适应、自主的智能体系统。\nKimi K2 通过以下机制实现这一跃迁：\n🔹 工具自主发现\n🔹 自我评估（self-evaluation）\n🔹 基于评分标准（rubric）的价值对齐\n🔹 混合真实/合成环境训练\n→ 模型由此能持续拓展其能力边界（competence frontier）。\n相比在静态预训练数据中遭遇收益递减，代理模型可自主生成：\n✅ 行动轨迹（trajectories of action）\n✅ 错误驱动的改进循环（error-driven improvements）\n→ 本质上成为自维持学习体（self-sustaining learners）。\n这一范式不仅为研究突破（当 scaling law 遇到瓶颈时）提供新路径，更对现实部署至关重要——因自主性、可靠性与具身工具使用已成为下一代 AI 的刚需。\nKimi K2 证明：构建此类系统需全栈创新：\n优化层：MuonClip 数据层：合成改写 + 工具流水线 强化学习层：RLVR + rubric 奖励 基础设施层：检查点引擎 + 代理 rollout → 它不仅是模型，更是一套通向未来 AI 的框架，模糊了静态基座模型与交互式演进智能体之间的界限。\n参考文献 # （原文未列具体文献，此处保留空节；可依需补充）\n如需将本文导出为 PDF、Markdown 或制作 PPT 汇报版，我可为您进一步整理。\n参考 # 以下是对 https://aman.ai/primers/ai/kimi-K2/ 的完整中文翻译，严格保留原文结构、技术术语与所有图片（含图注），便于读者对照理解 Kimi-K2 的核心技术突破。\nKimi K2 qwen 翻译 最新重量级报告，Kimi 开源 K2技术「Make Kimi Great Again」 翻译 最上面的总结\nhttps://www.alphaxiv.org/zh/overview/2507.20534v1\n"},{"id":10,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningLearningRate/","title":"(原理)学习率","section":"网络优化","content":"\n学习率 # (原理)学习率\n"},{"id":11,"href":"/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/WeightDecay/","title":"(原理\u0026实战)权重衰减","section":"正则化","content":"\n权重衰减 WeightDecay # (原理\u0026amp;实战)权重衰减\n"},{"id":12,"href":"/www6vAlgo/docs/LLM/Core/ScalingLaw/","title":"Scaling Law","section":"Core","content":" Scaling Law[10] # Scaling Law # 参数量 vs 数据量 # 参数量 vs 数据量 # 参考 # Scaling Law # 解析大模型中的Scaling Law\n1xx. 论文阅读，大模型的缩放定律，Scaling Laws for Neural Language Models\n2xx. Training Compute-Optimal Large Language Models 简读 2xx. 【预训练模型】推翻OpenAI结论, DeepMind重新定义预训练的训练参数和训练规模的关系！\n《Scaling Laws for Neural Language Models》\n《Training Compute-Optimal Large Language Models》\n"},{"id":13,"href":"/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLData/","title":"机器学习-数据","section":"机器学习","content":"\n机器学习-数据 # 机器学习-数据\n"},{"id":14,"href":"/www6vAlgo/docs/LLM/Dense/Family/","title":"GPT 系列","section":"Dense","content":" 进化时间线 # {% asset_img \u0026lsquo;family.jpg\u0026rsquo; %}\nGPT1 [1] # 它是最早一批提出在 NLP 任务上使用 pre-train + fine-tuning 范式的工作。 GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间 预训练模型具有 zero-shot 的能力，并且能随着预训练的进行不断增强 GPT2 [1] # 核心思想 # 当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，不需要在下游任务微调。\nGPT-2 vs. GPT-1 # 主推 zero-shot，而 GPT-1 为 pre-train + fine-tuning； 训练数据规模更大，GPT-2 为 800w 文档 40G，GPT-1 为 5GB； 模型大小，GPT-2 最大 15 亿参数，GPT-1为 1 亿参数； 模型结构调整，层归一化和参数初始化方式； 训练参数，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024，等等； GPT3 [1] # 下游任务评估方法 # GPT-3 在下游任务的评估与预测时，提供了三种不同的方法： Zero-shot：仅使用当前任务的自然语言描述，不进行任何梯度更新； One-shot：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新； Few-shot：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；\nShot[2] One-shot Few-Shot Zero-Shot Few-shot vs fine-tuning # 其中 Few-shot 也被称为 in-context learning，虽然它与 fine-tuning 一样都需要一些有监督标注数据，但是两者的区别是： 【本质区别】 fine-tuning 基于标注数据对模型参数进行更新 而in-context learning使用标注数据时不做任何的梯度回传, 模型参数不更新\nGPT-3 vs. GPT-2 # 效果上，超出 GPT-2 非常多，能生成人类难以区分的新闻文章； 主推 few-shot，相比于 GPT-2 的 zero-shot，具有很强的创新性； 模型结构略微变化，采用 sparse attention 模块； 海量训练语料 45TB（清洗后 570GB），相比于 GPT-2 的 40GB； 海量模型参数，最大模型为 1750 亿，GPT-2 最大为 15 亿参数； InstructGPT [1] # 步骤 # 有监督微调， 奖励模型训练， 强化学习训练 技术方案 # 有监督微调（SFT） 本质上来说，SFT 可以理解为人工标注了一批数据，然后去微调 GPT-3。但是值得一提的是，这里标注的数据与 GPT-3 之前用来做下游任务使用的 few-shot 格式，有非常本质的区别。 InstructGPT 在 SFT 中标注的数据，正是为了消除这种模型预测与用户表达习惯之间的 gap。在标注过程中，他们从 GPT-3 的用户真实请求中采样大量下游任务的描述，然后让标注人员对任务描述进行续写，从而得到该问题的高质量回答。\n基于人类反馈的强化学习（RLHF） {% asset_img \u0026lsquo;instructGPT.jpg\u0026rsquo; %}\n总结 # 解决 GPT-3 的输出与人类意图之间的Align问题； 让具备丰富世界知识的大模型，学习“人类偏好”； 标注人员明显感觉 InstructGPT 的输出比 GPT-3 的输出更好，更可靠； InstructGPT 在真实性，丰富度上表现更好； InstructGPT 对有害结果的生成控制的更好，但是对于**“偏见”没有明显改善**； ChatGPT 训练 [3] # 基于人类反馈的强化学习微调技术 RLHF 使用有监督微调 Supervised Fine-tuning（SFT）预训练语言模型 Supervised fine-tuning (SFT) = Instruction Tuning 训练奖励模型 Reward Model（RM） 使用强化学习算法微调语言模型 RLHF [本质 基于强化学习, 强化学习算法] 参考 # GPT / GPT-2 / GPT-3 / InstructGPT 进化之路 ***\nFew-Shot, Zero-Shot \u0026amp; One-shot 的通俗理解\nAI 大模型微调训练营大纲\n1xx. 万字拆解！追溯ChatGPT各项能力的起源 符尧\n1xx. [Transformer 101系列] ChatGPT是怎么炼成的? 未\n"},{"id":15,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Transformer/","title":"(原理)Transformer","section":"Transformer","content":"\n(原理)Transformer # (原理)Transformer\n"},{"id":16,"href":"/www6vAlgo/docs/LLM/Survey/LargeModelSurvey/","title":"(综述)大模型","section":"Survey","content":" LLMs的背景[1] # Scaling law of LLMs # KM scaling law Chinchilla Scaling law LLMs的涌现能力 # in-context learning instruction following step-by-step reasoning 大语言模型的关键技术 *** # Scaling Training Ability Eliciting Alignment Tuning Tool Manipulation Pre-training[1] # 数据收集 # 架构 # 模型训练 *** # 优化设置\nBatch Training Learning Rate Optimizer Stabilizing the Training 可扩展的训练技巧\n3D并行 数据并行 + 流水线并行 + 张量并行 ZeRO 混合精度训练 总体训练建议 Adaptation Tuning of LLMs[1] # 指令调优 *** # 本质上，指令微调是在自然语言格式的实例（instance）集合上微调预训练后的 LLM 的方法 [62]。\n指令微调后，LLM 可以展现出泛化到未见过任务的卓越能力 [28, 62, 64]，即使在多语言场景下也能有不错表现 [98]。\n格式化实例的构建 # 格式化已有数据集 格式化人类需求 构建实例的关键因素 增加指令 设计格式 总的来说，指令多样性似乎比实例数量更重要\n指令微调策略 # 平衡数据分布 一种广泛使用的方法是实例比例混合策略 [87]，即将所有数据集合并，然后从混合数据集中按比例采样每种实例。 此外，根据最近的研究发现 [64, 99]，提高高质量数据集（例如 FLAN [62] 和 P3 [209]）的采样比例通常可以带来性能提升。\n结合指令微调和预训练 为了使微调过程更加有效和稳定，OPT-IML [99] 在指令微调期间加入了预训练数据，这可以看作是对模型的正则化（regularization）。\n具体而言，GLM-130B [97] 和 Galactica [34] 将指令格式数据集作为预训练语料库的一小部分来预训练 LLM，这有可能同时获得预训练和指令微调的优势。\n指令微调的效果 # 性能改进 最近的研究在多个规模上（从 7700 百万到 5400 亿不等）对 LM 进行了实验，表明不同规模的模型都可以从指令微调中受益 [64, 216]，随着参数规模的增加，性能也得到了提升 [98]。 【普适性】 此外，经过指令微调的较小模型甚至可以比未经微调的较大模型表现更好 [28, 64]。\n任务泛化性 todo 对齐调优 # 高效调优 # 参考 # 大语言模型综述 中文 v10\n大语言模型综述 中文\nLLMSurvey Repo git\n[论文]大语言模型综述\n详谈大模型训练中的数据收集、处理与模型影响：A Survey of Large Language Models工作中的数据总结\n大模型综述-A Survey of Large Language Models 1xx. 值得一看的大模型最新综述：兼看多语种大模型微调数据集Aya 1xx. 43页预训练模型综述（清华、复旦、人大）\n"},{"id":17,"href":"/www6vAlgo/docs/DeepLearning/basic/DeepLearning/","title":"Deep Learning","section":"basic","content":"\nDeep Learning # Deep Learning\n"},{"id":18,"href":"/www6vAlgo/docs/RL/framework/ROLL/","title":"(原理\u0026实践)ROLL","section":"Framework","content":" 论文 # 原理\u0026amp;实践 # (原理\u0026amp;实践)ROLL\n"},{"id":19,"href":"/www6vAlgo/docs/RL/framework/AREAL/","title":"(原理|实践)AREAL","section":"Framework","content":" 论文 # 解读 # AREAL\n"},{"id":20,"href":"/www6vAlgo/docs/LLM/Core/token-sampling/token-sampling/","title":"(翻译)token sampling","section":"Token Sampling","content":" Token 采样方法（Token Sampling Methods） # 概述 # 生成式大语言模型（LLM）将输入和输出文本理解为“token”序列；这些 token 可以是单词，也可以是标点符号或单词的一部分。 LLM 提供若干 token 选择参数，用以控制推理/运行时输出的随机性。选择输出 token 的方法（具体称为 token 采样方法 或 解码策略），是语言模型文本生成中的一个核心概念。 从技术底层来看，token 采样的核心是：模型不断生成一个称为概率分布的数学函数，用于决定下一个 token（例如单词）——这一决策会考虑所有先前已输出的 token。简单来说，LLM 在生成文本时执行的是采样：即根据条件概率分布随机选择下一个单词。 以 OpenAI 托管的系统（例如 ChatGPT）为例：在生成概率分布后，OpenAI 的服务器会根据该分布进行 token 采样。该过程存在一定随机性，因此相同的输入提示可能产生不同的输出。 本指南将介绍不同的 token 采样方法及相关概念，包括：温度（Temperature）、贪心解码、穷举搜索解码、束搜索、Top-$k$、Top-$p$（核心采样）以及 Min-$p$。 背景 # 自回归解码（Autoregressive Decoding） # 在使用语言模型生成文本序列时，我们通常从一段文本前缀（即提示 prompt）开始，然后按以下步骤循环： 使用语言模型预测下一个 token； 将该 token 加入当前输入序列； 重复上述过程。 通过这种持续生成下一个 token 的方式（即 自回归解码），我们可以生成整个文本序列（见下图；来源）。 Token 概率 # 那么，我们该如何选择/预测下一个 token（即上述第 1 步）？ 语言模型并不直接输出下一个 token，而是输出一个所有可能 token 的概率分布。简言之，LLM 本质上是在词汇表（所有唯一 token 的集合）上进行分类任务的神经网络。 基于该概率分布，我们可以采用多种策略来选择下一个 token。例如，后文将介绍的贪心解码（greedy decoding）直接选择概率最高的 token 作为下一个输出。 Logits 与 Softmax # LLM 通过 logits 向量 $\\mathbf{z} = (z_1, \\dots, z_n)$ 表示类别打分，并使用 softmax 函数 将其转化为概率向量 $\\mathbf{q} = (q_1, \\dots, q_n)$： $$ q_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)} $$ Softmax 函数通过对 logits 取指数并归一化，使得模型在每个时间步的输出均落在 $[0, 1]$ 区间，且总和为 1，从而便于将输出解释为概率（见下图；来源）。 相关概念：温度（Temperature） # 尽管温度本身并非一种 token 采样方法，但它显著影响采样过程，因此本篇纳入讨论。 温度参数允许我们调整 token 的概率分布。它作为 softmax 变换中的一个超参数（见下图；来源），在应用 softmax 前对 logits 进行缩放，从而控制预测的随机性。 例如在 TensorFlow 的 Magenta 项目中（LSTM 实现），温度参数控制 logits 在 softmax 前被缩放（或除以）的程度。 温度在 Softmax 中的作用 # 标准 softmax 引入温度超参数 $T$ 后的形式为： $$ q_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)} $$ 其中 $T$ 为温度（默认为 1）。\n当 $T=1$，即直接对 logits 计算 softmax；\n若 $T=0.6$，则对 $\\frac{\\text{logits}}{0.6}$ 计算 softmax —— 此时数值被放大，softmax 结果更“尖锐”；\n→ 模型更自信（更少输入即可激活输出层），但也更保守（不太可能采样低概率候选）。\n不同温度范围及其影响 # 低温（$T \\approx 0.0$–$0.5$） # 特征： 强烈偏好高概率 token； 输出确定性强、随机性低； 文本常重复，多样性差。 适用场景： 需要高精度或高置信度的场景（例如生成事实性回答）。 局限性： 可能过于保守，陷入重复环（repetitive loops）。 中温（$T \\approx 0.6$–$1.0$） # 特征： 在多样性与连贯性之间取得平衡； 允许探索较低概率选项，但不显著损伤文本合理性。 适用场景： 生成类人语句、代码补全、音乐作曲等需“有创意但合理”的任务。 局限性： 仍会偏向高概率 token，抑制极低概率创意探索。 高温（$T \u0026gt; 1.0$） # 特征： 生成更“平缓”的概率分布； 输出更随机、多样；更倾向低概率选项； 有助于跳出重复环，探索更广空间。 适用场景： 头脑风暴、高度创意内容生成（如艺术、故事、诗歌）。 局限性： 易出现逻辑错误或语义混乱； 采样到不合理 token 的风险增大。 Softmax 函数的深层理解（引自维基百科） # 当温度 $\\tau \\to \\infty$ 时，所有样本概率趋近相等；温度越低（$\\tau \\to 0^+$），预期奖励最高的样本概率趋近于 1。\n温度影响总结 # 低温 → 更自信、更保守 → 适合确定性任务； 中温 → 平衡随机性与连贯性 → 通用推荐区间； 高温 → 更富创意、更随机 → 适合探索性任务。 通过调节温度，可依任务需求灵活调整模型行为，极大增强 LLM 的适用性。 贪心解码（Greedy Decoding） # 贪心解码在每一步都使用 argmax 选择当前概率最高的 token 作为输出。 问题：它无法回溯修正之前生成的 token。\n举例：输入法语 “il a m’entarté”（他用派砸了我），若贪心解码已生成 “he hit a”，即使后续发现应为 “me”，也无法回头修改。 模型逐 token 生成序列，每步仅考虑当前最优——不评估该选择对未来步骤的影响。 解码通常持续至生成 \u0026lt;END\u0026gt; token 为止。例如：\n\u0026lt;START\u0026gt; he hit me with a pie \u0026lt;END\u0026gt;（来源） 优点：计算高效、实现简单；\n缺点：不保证全局最优输出序列。 改进方向：采用穷举搜索或束搜索（beam search）。 穷举搜索解码（Exhaustive Search Decoding） # 顾名思义，穷举搜索考察所有可能的输出序列组合，并选出评分最高的那个。 在序列到序列任务（如神经机器翻译）中，这意味着生成所有可能的译文，再用评分函数评估其与目标的匹配度。 问题：计算复杂度极高——候选数量随输出长度呈指数级增长。 时间复杂度为 $O(V^T)$，其中 $V$ 为词表大小，$T$ 为输出长度；实际中几乎不可行。 尽管理论上可得最优解，但因其高昂开销，极少用于真实场景。 束搜索（Beam Search） # 束搜索是机器翻译等任务中常用的搜索算法，用于高效生成最可能的词序列。 核心思想：在每步解码时，仅保留 $k$ 个最高分的部分候选序列（partial hypotheses），$k$ 即为束宽（beam size），通常取 5–10。 具体流程（见下图，束宽=2）： 每步计算若干候选项及其累积得分（通常为对数概率之和）； 保留 top-$k$ 路径继续扩展； 后续通过回溯获得完整输出。 不同候选可能在不同时间步生成 \u0026lt;END\u0026gt;： 一旦某候选产出 \u0026lt;END\u0026gt;，视为完成，暂存； 继续扩展其余候选，直至： 达到预设最大长度 $T$，或 已获得足够多（如 $n$ 个）完成候选。 得分归一化问题：较长序列通常累积得分更低（因每步概率 \u0026lt;1，连乘/求和后更小）→ 需按长度归一化（如使用平均对数概率）后再比较。 注意：束搜索不保证全局最优，但远优于穷举，兼顾质量与效率。\n详见：D2L.ai《动手学深度学习》— 束搜索章节。 约束束搜索（Constrained Beam Search） # 适用场景：需强制输出中包含特定词/短语（如机器翻译中必须包含某术语）。 基本思想：在束搜索过程中加入硬性约束条件，仅保留满足约束的候选路径。 实现方式： 修改评分函数，或 在每步生成后剔除违反约束的候选， 或引入惩罚项降低违规路径得分， 或用独立模块动态反馈约束满足情况。 示例：生成句子时需包含短语 “is fast”；\n除常规高概率词（如 “dog”、“nice”）外，强制加入 “is” 以推进约束达成（见下图）。 银行机制（Banking） # 强制插入 token 是否会导致荒谬输出？银行机制可解决此问题： 将候选按满足约束的程度分为多个“银行”（Bank）； Bank 2：已满足全部约束；\nBank 1：接近满足；\nBank 0：尚未开始满足。 采用轮询选择（round-robin）：依次从 Bank 2、1、0 中各选最高分候选，再从 Bank 2、1、0 选次高……\n（例：若用 3 束，则选出：[\u0026quot;The is fast\u0026quot;, \u0026quot;The dog is\u0026quot;, \u0026quot;The dog and\u0026quot;]） 这样既保证约束逐步满足，又维持高概率合理序列的竞争力。 下图为全流程结果： Top-$k$ 采样 # 核心思想：每步从概率最高的 $k$ 个 token中采样，而非仅选最大者。 采样方式可为： 均匀采样：top-$k$ 内各 token 等概率 → 提升多样性； 按原概率采样：保持分布权重 → 提升连贯性。 $k=1$ 时退化为贪心解码。 $k$ 越小 → 选择越窄 → 多样性↓、控制性↑；\n$k$ 越大 → 选择越宽 → 多样性↑、控制性↓。 适用于需平衡多样性与可控性的任务（如对话生成）。 Top-$p$（核心采样 / Nucleus Sampling） # 动机：Top-$k$ 中 $k$ 难以选取 → 需动态调整候选集大小。 Top-$p$ 方法： 按概率降序排列所有 token； 取最小的前缀子集，使其累积概率 ≥ $p$（如 $p=0.9$）； 重新归一化该子集概率（使其和为 1）； 从中按新概率采样。 与 Top-$k$ 关键区别：\nTop-$k$ 固定数量，Top-$p$ 固定概率质量；\n后者可根据分布“自适应”调整候选数。 实用价值 # 适合需精细调控多样性与流畅度的任务（如语言建模、摘要生成）； 实际中 $p$ 常设为 0.75 左右，以过滤长尾低概率噪声 token； 特殊情形： 若某 token 概率 \u0026gt; $p$，则必然被选（退化为贪心）； 若概率分布平坦，则候选集变大 → 更富创意。 注意：Top-$k$ 与 Top-$p$ 可联用（先取 top-$k$，再在其中做 top-$p$），但 $p$ 作用于 $k$ 之后。 与温度参数的关系 # OpenAI GPT-3 API 提示：温度与 top-$p$ 互斥（见下图）；\n二者是不同且互斥的随机性控制机制。 贪心 vs. Top-$k$ 与 Top-$p$ # 对比维度 贪心解码 Top-$k$/Top-$p$ 确定性 确定性（总是选最高概率） 随机性（引入采样） 采样方式 无（直接 argmax） 可均匀或按概率 文本风格倾向 安全、保守、缺乏创意 更新颖、多样，但可能不连贯 Min-$p$ 采样 # Min-$p$ 是 Hugging Face Transformers 库引入的新型解码策略，旨在改进 Top-$k$ 与 Top-$p$ 的不足。 现有方法局限回顾 # Top-$k$： 固定截断 → 可能丢弃高质量低频 token → 降低词汇多样性。 Top-$p$： 包含极低概率 token → 可能破坏连贯性。 Min-$p$ 核心思想 # 引入动态阈值：\n设定最小概率因子 min_p（如 0.05），\n计算阈值 = min_p × 最高概率 token 的概率；\n→ 仅保留概率 ≥ 该阈值的 token。 优势： 当存在绝对主导 token时 → 严格过滤，保证聚焦与连贯； 当概率分布较平坦时 → 宽松保留，支持创意发散。 推荐配置（尤其适合创意生成）： min_p ∈ [0.05, 0.1] 配合高温（$T \u0026gt; 1$）→ 充分激发创造力； 可减少甚至无需使用“重复惩罚（repetition penalty）”等补丁技巧。 简言之，Min-$p$ 通过自适应概率截断，在多样性与连贯性间取得更优平衡，有望成为新一代解码标准。 参考文献 # Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015). What is Temperature in LSTM (and neural networks generally)? Stanford CS224n Ketan Doshi, Foundations of NLP Explained Visually: Beam Search, How it Works Cohere: Top-k and Top-p HuggingFace: Constrained Beam Search 如需 PDF 或 Markdown 格式文档，我可为您整理输出。\n参考 # Token Sampling Methods qwen 翻译\n总结（Kimi） # 方法 是否随机 控制方式 优点 缺点 Greedy 否 argmax 快速、确定 易重复、局部最优 Beam Search 否 保留 top-k 序列 比贪心更优 不保证全局最优 Top-k 是 固定数量 简单有效 可能包含低质量词 Top-p 是 动态累积概率 更灵活 可能引入低概率词 Min-p 是 动态阈值 高温度下更稳定 新方法，需调参 "},{"id":21,"href":"/www6vAlgo/docs/LLM/Reasoning/kimi/k2-thinking/","title":"Kimi K2 Thinking","section":"kimi","content":" 0. 背景 # 核心关注三个问题：\n预训练阶段：如何用 MuonClip 优化器实现更高的 token 效率？ 后训练阶段：如何通过大规模 Agentic 数据合成和通用强化学习，让模型学会使用工具？ Test-Time Scaling：如何让模型在推理时进行长程思考和工具调用？ 1. 整体架构：从 K2 到 K2 Thinking # 1.2 K2 Thinking：加入 Test-Time Scaling # Kimi K2 Thinking 是在 K2 的基础上，通过额外的训练，让模型具备了 thinking 能力。它的核心特点是：\n边思考边使用工具：模型在推理过程中，会进行 think → search → browse → think → code 的循环，动态生成和验证假设 长程推理：可以执行 200-300 步连续的工具调用，保持推理的连贯性。（这点是让人比较惊喜的） Test-Time Scaling：通过增加推理时的 thinking tokens 和工具调用步数，提升模型性能 从架构上看，K2 Thinking = K2 + Thinking Ability + Test-Time Scaling。因此，理解 K2 的训练方法，就能理解 K2 Thinking 的 80%。\n2. 预训练 # 2.1 基于 MuonClip 优化器的 Token 效率优化 # 2.2 文本的改写优化 # K2 相比 K1.5 的一个关键进步是引入了合成数据生成策略来提高 token 利用率。核心思想是：通过精心设计的改写 pipeline，在不引入显著过拟合的情况下，扩大高质量 tokens 的数量。改写（Rephrasing） 就是数据合成的一种方式，主要是为了提高「高质量数据的占比」，尤其是「知识领域」和「数学领域」。：\n2.2.1 知识领域数据改写 # 2.2.2 数学领域数据改写 # 3. 后训练(重点) # K2 的增强 Agentic 能力源于两个重要方面：\n大规模 Agentic 数据合成 通用强化学习 3.1 大规模 Agentic 数据合成：教会模型使用工具 # 3.1.1 数据合成流程 # 具体流程如下：\n定义领域和工具：涵盖各种真实场景，如数据分析、网页开发、系统管理等 生成任务：所有任务都是基于 rubric 的（有明确的评分标准），确保一致的评估 模拟交互：Agents 与模拟环境和用户 agents 交互，创建真实的多轮工具使用场景 LLM 评判(LLM as judge)：根据任务 rubrics 评估模拟结果，过滤出高质量的训练数据 这个可扩展的 pipeline 生成了多样化、高质量的数据，为大规模拒绝采样和强化学习铺平了道路。\n3.2 通用强化学习：不可验证奖励 # 传统的强化学习主要应用于可验证奖励的任务，比如数学题（答案对错明确）和竞赛编程（能否通过测试用例）。但对于不可验证奖励的任务（如写研究报告、创意写作），传统 RL 就无能为力了。\n3.2.1 Self-Judging 机制 # 核心思想是：模型作为自己的评判者，为不可验证的任务提供可扩展的、基于 rubric 的反馈。\n具体做法：\n对于不可验证的任务，模型生成多个候选答案 模型自己根据 rubric 评估这些答案，给出分数 使用这些分数作为奖励信号，进行强化学习 3.2.2 用可验证奖励改进 Critic # 小结：通过大规模 Agentic 数据合成和通用强化学习，K2 学会了在各种场景下使用工具，并且能够处理可验证和不可验证的任务。这为 K2 Thinking 的长程推理能力打下了基础。\n4. K2 Thinking # 4.1 什么是 Test-Time Scaling？ # Test-Time Scaling 是指在推理时增加计算量，以提升模型性能。对于 K2 Thinking，这体现在两个方面：\n增加 thinking tokens：模型在生成答案前，会先生成大量的思考过程（类似 OpenAI o1，这其实就是 Long-CoT，这种技术在 Kimi-k1.5 就已经开始做了） 增加工具调用步数：模型可以执行 200-300 步连续的工具调用，进行长程规划（这是新增的，为了 Agentic 能力的提升） 这两者结合，使得 K2 Thinking 能够解决需要深度推理和多步操作的复杂问题。\n4.2 边思考边使用工具：Interleaved Reasoning # K2 Thinking 的核心能力是边思考边使用工具。它会进行动态的 think → search → browse → think → code 循环，这个循环可以重复数百次，直到找到答案：\nThink：分析问题，生成假设 Search：搜索相关信息 Browse：浏览网页，提取关键信息 Think：验证假设，调整策略 Code：编写代码，执行计算 4.4 小结 # 通过 test-time scaling，K2 Thinking 能够在推理时进行长程思考和工具调用，从而解决需要深度推理和多步操作的复杂问题。这使得它在 Agentic Reasoning、Agentic Search 和 Agentic Coding 任务上都达到了 SOTA 性能。（有点 claude 那味道了）\n5. 技术细节对比：K2 vs K2 Thinking # 维度 K2 (Instruct) K2 Thinking 模型类型 Non-thinking（无长思考） Thinking model（有长思考） 推理方式 直接生成答案 边思考边使用工具 工具调用 支持，但步数有限（其实也挺好的） 200-300 步连续调用 Test-Time Scaling 不支持 支持（thinking tokens + 工具调用） 适用场景 通用对话、快速响应 复杂推理、长程规划 参考 # Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）\nKimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程\n"},{"id":22,"href":"/www6vAlgo/docs/LLM/Reasoning/Qwen/Qwen3/","title":"Qwen3","section":"Qwen","content":" 论文 # https://github.com/QwenLM/Qwen3/\nArch [2] # Compare [1] # Post-training # 阶段三：思考模式融合 # **两种模式使用/think和/no_think标志进行区分，**注意“非思考模式”也有开始和结束的标志符，只是其思考过程置为空。并且在训练过程中，会针对多轮对话进行“思考”和“非思考”模式的混合训练。\n参考 # The Big LLM Architecture Comparison Understanding and Implementing Qwen3 From Scratch Qwen3技术报告的几点细节、ArXiv论文翻译实现方案及试错历程\n【LLM4】Qwen3-RL训练详解 ***\nup: 卢老师， 怀中猫\n"},{"id":23,"href":"/www6vAlgo/docs/LLM/Reasoning/Qwen/Qwen3-report/","title":"Qwen3 Report","section":"Qwen","content":" 简介 # Qwen 团队发布了 Qwen3，这是一个新的大型语言模型（LLM）系列，代表了开源人工智能领域的重大进步。Qwen3 涵盖了各种参数规模的密集和混合专家（MoE）架构，旨在平衡性能、效率和可访问性。\n这个最新版本建立在以前的 Qwen 模型之上，在推理能力、多语言支持和推理效率方面进行了重大改进。Qwen3 的一项关键创新是在单个模型中集成思考和非思考模式，无需在不同任务的专用模型之间切换。这些模型在 Apache 2.0 许可下发布，有助于开源人工智能技术生态系统的发展，并将 Qwen3 定位为 GPT-4o、Claude 3.7 和 Gemini 2.5 等专有模型的有竞争力的替代方案。\n模型架构与创新 # Qwen3 在 Qwen2.5 的基础上进行了多项架构增强：\n双模架构：该模型集成了思考和非思考模式，允许用户根据任务需求选择合适的模式。 混合专家（MoE）：Qwen3 系列包括 MoE 模型，这些模型以较少的激活参数在推理期间实现高性能，从而提高计算效率。 核心组件：该架构包含以下高级功能： 分组查询注意力（GQA） SwiGLU 激活函数 旋转位置嵌入（RoPE） RMSNorm 和 QK-Norm 模型变体：Qwen3 系列包括针对不同用例量身定制的多个模型： Qwen3-235B-A22B：一个旗舰级的 MoE 模型，总参数为 235B，但在推理期间只有 22B 处于激活状态 Qwen3-32B：一个旗舰级的密集模型 参数范围从 14B 到 0.6B 的较小模型 扩展上下文长度：这些模型支持高达 128K 的上下文长度，从而能够处理更长的文档和更复杂的交互。 训练方法 # Qwen3 采用一种复杂的、多阶段的训练过程，旨在增强各种能力：\n预训练数据：这些模型在包含 119 种语言和方言的 36 万亿个 token 的庞大数据集上进行训练，与以前版本中支持的 29 种语言相比，这是一个显着的扩展。\n三阶段预训练过程：\n一般知识获取 推理能力增强 长上下文适应 后训练对齐：一个四阶段的过程使模型与人类偏好对齐：\n基础模型 → Long-CoT 冷启动 → 推理 RL → 思考模式融合 → 通用 RL 每个阶段都针对特定的能力：\nLong-CoT 冷启动：引入思维链推理 推理 RL：强化准确的推理路径 思考模式融合：集成思考和非思考模式 通用 RL：增强整体模型对齐 数据整理：训练过程涉及多模态数据增强，包括从 PDF 中提取文本和生成合成数据，以及实例级数据混合优化。\n思考模式和预算 # Qwen3 最具创新性的功能之一是其思考模式和预算机制，这为用户提供了对模型推理深度进行细粒度控制的能力：\n思考模式与非思考模式： 非思考模式：针对简单任务进行了优化，可直接提供答案，无需大量推理。 思考模式：进行更深入的推理，展示复杂问题的工作和中间步骤。 思考预算： 用户可以指定一个“思考预算”（以千个 tokens 为单位），以控制模型应应用的推理量。这在两种模式之间创建了一个频谱，而不是二元选择。 性能相关性：如上图所示，增加思考预算可以持续提高各种基准测试的性能，包括 AIME 数学推理、LiveCodeBench 编程和 GPQA Diamond 科学推理。 数学表达式：思考预算和模型性能之间的关系可以近似表示为： \\(P(b) = P_{\\text{non-thinking}} + (P_{\\text{thinking}} - P_{\\text{non-thinking}}) \\cdot \\min\\left(\\frac{b}{b_{\\text{max}}}, 1\\right) \\) 其中 \\( P(b) \\) 是预算为 \\(b\\) 时的性能， \\(b_{\\text{max}}\\) 是最大有效预算。\n多语言能力 # Qwen3 代表了多语言支持方面的重大进步：\n扩展的语言覆盖范围：该模型支持 119 种语言和方言，比以前版本中的 29 种语言有了大幅增加。 预训练方法：多语言能力嵌入在预训练期间，并经过精心的数据管理，以确保不同语言的平衡表示。 性能改进：Qwen3 展示了增强的跨语言理解和生成能力，使其对全球用户更具可访问性。 语言分布：训练数据包括英语以外的各种语言，其中普通话和其他广泛使用的语言占了很大比例，并且改进了对低资源语言的覆盖。 强到弱的知识蒸馏 # 为了增强 Qwen3 的可访问性，该团队采用了强到弱的知识蒸馏方法：\n知识转移：来自较大模型（Qwen3-235B-A22B 和 Qwen3-32B）的知识被提炼到较小的模型中（范围从 14B 到 0.6B 参数）。 蒸馏过程：该过程包括训练较小的模型来模仿较大模型的行为，同时保持双重模式能力。 优点： 减少了部署所需的计算资源 保持了各种任务的性能 保持了思考和非思考模式能力 实施： # 用于强到弱的知识蒸馏的简化伪代码 def distill_knowledge(teacher_model, student_model, training_data): for batch in training_data: # 获取教师模型在思考和非思考模式下的输出 teacher_thinking_output = teacher_model(batch, mode=\u0026#34;thinking\u0026#34;) teacher_nonthinking_output = teacher_model(batch, mode=\u0026#34;non-thinking\u0026#34;) # 训练学生模型以匹配两种模式 student_thinking_loss = loss_fn(student_model(batch, mode=\u0026#34;thinking\u0026#34;), teacher_thinking_output) student_nonthinking_loss = loss_fn(student_model(batch, mode=\u0026#34;non-thinking\u0026#34;), teacher_nonthinking_output) # 更新学生模型的参数 total_loss = student_thinking_loss + student_nonthinking_loss total_loss.backward() optimizer.step() 性能和基准测试 # Qwen3 在各种基准测试中表现出具有竞争力的性能：\n通用任务：在衡量常识推理、阅读理解和知识检索的基准测试中表现出色。 数学与 STEM：数学推理能力显著提高，在 MATH、AIME 和 GPQA 等基准测试中得到验证。 编码：在 HumanEval 和 LiveCodeBench 等编程基准测试中表现出竞争优势，并且随着思考预算的增加，思考模式明显优于非思考模式。 多语言任务：在不同语言中性能得到增强，表明扩展多语言支持的有效性。 与其他模型的比较：Qwen3 在开源模型中取得了最先进的结果，并缩小了与更大的专有模型之间的差距。 效率优势：与具有相似能力的密集模型相比，MoE 模型以更少的激活参数表现出高性能。 开源贡献 # 在 Apache 2.0 许可下发布 Qwen3 代表了对开源 AI 社区的重大贡献：\n模型可访问性：完整的模型权重和代码通过 Hugging Face、ModelScope 和 GitHub 等平台提供。 文档和示例：提供全面的文档和示例代码，以方便采用和实验。 社区参与：开源发布能够更广泛地研究、开发和部署先进的 LLM。 AI 民主化：通过免费提供最先进的模型，Qwen3 有助于缩小大型组织与小型实体或个体研究人员之间的资源差距。 未来方向 # Qwen 团队概述了未来研究和开发的几个方向：\n扩展预训练：进一步增加预训练数据的大小和多样性，以增强模型能力。 架构改进：继续改进模型架构，以提高效率和性能。 扩展上下文长度：探索处理超出当前 128K 限制的更长上下文的技术。 增强推理：开发更复杂的推理控制和验证方法。 多模态能力：扩展到多模态理解和生成。 强化学习：增加专门用于强化学习的计算资源，以提高模型对齐和指令遵循能力。 总而言之，Qwen3 代表了开源大型语言模型的重大进步，提供了一套全面的模型，具有竞争力的性能、创新的思考控制机制、扩展的多语言支持和高效的架构设计。 在单个模型中集成思考和非思考模式，以及思考预算机制，为用户提供了前所未有的控制，可以控制应用于不同任务的推理深度，从而优化性能和效率之间的平衡。\n参考 # https://www.alphaxiv.org/zh/overview/2505.09388v1\n"},{"id":24,"href":"/www6vAlgo/docs/LLM/MoE/ModelMOECode/","title":"(代码)MOE","section":"MOE","content":" import torch import torch.nn as nn import torch.nn.functional as F import torch_npu from torch_npu.contrib import transfer_to_npu class Expert(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super().__init__() self.net = nn.Sequential( nn.Linear(input_dim, hidden_dim), nn.GELU(), nn.Linear(hidden_dim, output_dim)) def forward(self, x): return self.net(x) class MoE(nn.Module): def __init__(self, input_dim, num_experts, top_k, expert_capacity, hidden_dim, output_dim): super().__init__() self.num_experts = num_experts self.top_k = top_k self.expert_capacity = expert_capacity # 路由网络 self.gate = nn.Linear(input_dim, num_experts) # 专家集合 self.experts = nn.ModuleList( [Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)]) def forward(self, x): batch_size, input_dim = x.shape device = x.device # 路由计算 logits = self.gate(x) probs = torch.softmax(logits, dim=-1) topk_probs, topk_indices = torch.topk(probs, self.top_k, dim=-1) # 辅助损失计算 if self.training: # 重要性损失（专家利用率均衡） importance = probs.sum(0) importance_loss = torch.var(importance) / (self.num_experts ** 2) # 负载均衡损失（样本分配均衡） mask = torch.zeros_like(probs, dtype=torch.bool) mask.scatter_(1, topk_indices, True) routing_probs = probs * mask expert_usage = mask.float().mean(0) routing_weights = routing_probs.mean(0) load_balance_loss = self.num_experts * (expert_usage * routing_weights).sum() aux_loss = importance_loss + load_balance_loss else: aux_loss = 0.0 # 专家分配逻辑 flat_indices = topk_indices.view(-1) flat_probs = topk_probs.view(-1) sample_indices = torch.arange(batch_size, device=device)[:, None]\\ .expand(-1, self.top_k).flatten() # 初始化输出 outputs = torch.zeros(batch_size, self.experts[0].net[-1].out_features, device=device) # 处理每个专家 for expert_idx in range(self.num_experts): # 获取分配给当前专家的样本 expert_mask = flat_indices == expert_idx expert_samples = sample_indices[expert_mask] expert_weights = flat_probs[expert_mask] # 容量控制 if len(expert_samples) \u0026gt; self.expert_capacity: expert_samples = expert_samples[:self.expert_capacity] expert_weights = expert_weights[:self.expert_capacity] if len(expert_samples) == 0: continue # 处理专家计算 expert_input = x[expert_samples] expert_output = self.experts[expert_idx](expert_input) weighted_output = expert_output * expert_weights.unsqueeze(-1) # 累加输出 outputs.index_add_(0, expert_samples, weighted_output) return outputs, aux_loss # 测试示例 if __name__ == \u0026#34;__main__\u0026#34;: input_dim = 128 output_dim = 256 num_experts = 8 top_k = 2 expert_capacity = 32 hidden_dim = 512 batch_size = 64 # add device = torch.device(\u0026#34;npu:4\u0026#34; if torch.npu.is_available() else \u0026#34;cpu\u0026#34;) moe = MoE(input_dim, num_experts, top_k, expert_capacity, hidden_dim, output_dim).to(device) x = torch.randn(batch_size, input_dim).to(device) experimental_config = torch_npu.profiler._ExperimentalConfig( export_type=torch_npu.profiler.ExportType.Text, profiler_level=torch_npu.profiler.ProfilerLevel.Level0, msprof_tx=False, aic_metrics=torch_npu.profiler.AiCMetrics.AiCoreNone, l2_cache=False, op_attr=False, data_simplification=False, record_op_args=False, gc_detect_threshold=None ) with torch_npu.profiler.profile( activities=[ torch_npu.profiler.ProfilerActivity.CPU, torch_npu.profiler.ProfilerActivity.NPU ], schedule=torch_npu.profiler.schedule(wait=0, warmup=0, active=1, repeat=1, skip_first=1), on_trace_ready=torch_npu.profiler.tensorboard_trace_handler(\u0026#34;./moe_stand_npu_result\u0026#34;), record_shapes=False, profile_memory=False, with_stack=False, with_modules=False, with_flops=False, experimental_config=experimental_config) as prof: # 训练模式 for _ in range(10): moe.train() output, loss = moe(x) print(f\u0026#34;Using device: {x.device}\u0026#34;) print(f\u0026#34;Training output shape: {output.shape}\u0026#34;) # torch.Size([64, 256]) print(f\u0026#34;Training auxiliary loss: {loss.item():.4f}\u0026#34;) # 示例值，如0.1234 prof.step() print(\u0026#34;=\u0026#34; * 80) # 推理模式 moe.eval() output, _ = moe(x) print(f\u0026#34;Eval output shape: {output.shape}\u0026#34;) # torch.Size([64, 256]) 参考 # MoE git\n"},{"id":25,"href":"/www6vAlgo/docs/LLM/MoE/MoE/","title":"(原理)MoE","section":"MOE","content":" 专家混合模型 (MoE) 架构技术 # -\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n1.0 引言：重新定义 AI 模型的可扩展性 # 专家混合模型（Mixture-of-Experts, MoE）是一种先进的神经网络架构，它基于“条件计算”范式，为人工智能领域最核心的挑战之一——模型规模扩展，提供了优雅且高效的解决方案。其核心价值主张在于：允许模型参数数量实现巨大规模的扩展，而无需按比例增加训练和推理所需的计算成本。通过在每次前向传播中仅激活模型参数的一个小子集（即“专家”），MoE 架构成功地打破了传统稠密模型中性能提升与计算开销之间的强耦合关系。\n在当今追求万亿参数规模的时代，这一特性使 MoE 成为构建尖端人工智能系统（如业界传闻的 GPT-4 架构及开源领域的 Mixtral 模型）的基石技术。它不仅解决了模型扩展的瓶颈，更催生了一系列关于模型专业化、计算效率和分布式系统设计的新思路。\n本文档旨在提供一份关于专家混合模型架构的综合性技术白皮书。我们将首先深入剖析 MoE 的核心工作原理，追溯其从理论雏形到成为现代大规模 AI 系统计算支柱的发展历程。随后，我们将系统性地探讨实施 MoE 所面临的关键技术挑战，如专家容量管理、负载均衡和令牌丢弃，并分析其主流解决方案。此外，本文还将对现代 MoE 架构进行分类，探讨高级路由机制的演进，并分析其在 Transformer 模型不同组件中的扩展应用。最终，通过对知名 MoE 模型的案例分析，我们将展望这一架构在塑造下一代人工智能系统中的深远影响。\n现在，让我们从探究 MoE 架构最基本的工作原理开始。\n2.0 MoE 的核心原理与发展历程 # 要全面掌握专家混合模型在现代人工智能中的应用，首先必须理解其基础的构成要素和演进脉络。MoE 的设计思想并非一蹴而就，而是历经三十余年的持续演进，从一个精巧的理论雏形，逐步发展成为支撑万亿参数级别模型的关键计算支柱。本章将追溯这一历程，阐释其核心工作原理，并梳理其发展过程中的关键里程碑。\n2.1 核心原理阐释 # 经典的 MoE 架构建立在一个直观而强大的“分而治之”思想之上。它将一个复杂的、宏观的预测问题分解为多个子任务，并训练一组专门化的“专家”来分别处理这些子任务。其核心组件协同工作，确保每个输入都能被最合适的专家高效处理。\n专家模型 (Expert Models): 每个专家本身就是一个小型的、专门化的神经网络（例如，一个前馈网络）。与试图解决整个问题的单一大型模型不同，每个专家被训练来专注于解决特定类型的数据或任务的某个方面，从而在其专长领域内达到更高的性能。 门控网络 (Gating Network / Router): 门控网络是 MoE 架构的动态控制器和决策核心。它的职责是接收输入数据，并判断哪个或哪些专家最适合处理该输入。它会输出一个权重分布，该分布决定了每个专家的输出在最终结果中所占的比重。在现代稀疏 MoE 架构中，这个网络通常被称为“路由器”，它会为每个输入令牌（token）选择一到两个最相关的专家。 分而治之 (Divide-and-Conquer): 综合来看，MoE 通过门控网络将复杂的任务智能地委托给最合适的专家，实现了高效的任务分解与协作。这种方法使得模型能够更有效地处理异构和复杂的数据分布，因为不同的专家可以学习数据中不同的潜在模式，从而共同提升模型的整体处理效率和准确性。 2.2 发展历程中的关键里程碑 # MoE 架构从一个理论概念演变为支撑前沿大语言模型的核心技术，其发展历程中包含了几个决定性的创新节点。\n早期起源 (1991): MoE 的概念最早由 Jacobs 等人在 1991 年提出。他们确立了“门控网络”动态选择“专家模型”处理输入的基础原则。这个早期的框架虽然尚未应用于大规模深度学习，但为后来的条件计算和集成学习思想奠定了理论基石。 稀疏门控革命 (2017): 随着深度学习的发展，Shazeer 等人在其里程碑式的论文《Outrageously Large Neural Networks》中，为 MoE 适应现代大规模训练带来了革命性突破。他们引入了两大核心创新：“top-k 路由”****“辅助负载均衡损失”，用于确保所有专家得到相对均匀的利用，解决了训练不稳定和部分专家未得到充分训练的问题。这些创新使得训练拥有数十亿甚至更多参数的神经网络成为可能。 简化与规模化 (2021): Fedus 等人提出的 Switch Transformer 进一步简化并优化了稀疏 MoE 架构。通过采用更激进的 top-1 路由（即每个令牌仅被分配给一个专家），Switch Transformer 显著降低了通信开销，同时在多个大规模自然语言处理基准测试中取得了顶尖性能。这一简洁而高效的设计为后续的大规模系统（如 Google 的 T5 家族以及 GPT 和 PaLM 等模型）的架构探索提供了重要参考，并证明了 MoE 在实现万亿参数模型方面的巨大潜力。 结构化稀疏与效率 (2022): 针对早期 MoE 实现中的令牌丢弃和硬件利用率问题，Gale 等人提出了 Dropless MoE，并在 MegaBlocks 系统中实现。他们将稀疏 MoE 计算重构为块稀疏矩阵乘法，这种硬件感知的范式消除了令牌丢弃和容量限制的必要性。MegaBlocks 实现了卓越的扩展效率和硬件利用率，代表了向工业级、高性能稀疏 MoE 框架演进的关键一步。 从最初的理论模型到现代万亿参数系统的计算支柱，MoE 的演变清晰地展示了人工智能领域在平衡模型规模、计算效率和专业化能力方面取得的巨大进步。然而，要成功实施这些强大的模型，必须首先解决一系列独特的技术挑战。\n3.0 关键技术挑战与解决方案 # 专家混合模型架构在赋予模型前所未有的扩展性的同时，也引入了一系列独特的系统性挑战。这些挑战源于其条件计算和动态路由的核心机制，若不加以妥善处理，将严重影响模型的训练稳定性和最终性能。本章将聚焦于三个最核心的技术难题——专家容量、负载均衡和令牌丢弃，并深入分析其背后的机制与主流解决方案。\n3.1 专家容量与容量因子 (Expert Capacity and Capacity Factor) # 在 MoE 模型中，为了确保计算负载的可预测性和分布式训练的稳定性，每个专家在处理一个批次的数据时，能够接收的令牌数量是有限的。这个上限被称为“专家容量”。Switch Transformer (Fedus et al., 2021) 的研究正式地提出了专家容量的定义，其公式近似为：\nexpert_capacity = (T / N) × α 其中，公式中的每个变量含义如下：\nT: 一个训练批次（batch）中所包含的令牌总数。 N: 模型中专家的总数量。 α: 容量因子（Capacity Factor），一个可调的超参数，通常设置大于 1.0。 容量因子 α 在此扮演着双重角色。首先，它是一种控制路由平衡的机制。理想情况下，路由器应将 T 个令牌均匀分配给 N 个专家，每个专家处理 T/N 个令牌。然而，由于路由决策是动态的，完美的均衡几乎不可能实现。α \u0026gt; 1.0 提供了一个“安全边际”或缓冲区，允许专家处理比平均值更多的令牌，以应对路由不均的情况。例如，当 α = 1.25 时，每个专家可以处理比其平均份额多 25% 的令牌。其次，专家容量是一个硬件层面的稳定性约束，它确保了在分布式训练中，每个设备（如 GPU）上的内存分配是固定的，从而防止因某个专家接收过多令牌而导致的计算资源过载。\n3.2 负载均衡 (Load Balancing) # 负载均衡是 MoE 训练中至关重要的一个环节。其本质在于防止“富者愈富”的效应，即少数专家被路由器频繁选中，处理了绝大多数令牌，而其他专家则长期处于闲置或未充分利用的状态。这种不均衡会导致两个严重问题：一是训练过程不稳定，因为负载过重的专家可能会成为计算瓶颈；二是模型性能下降，因为未被充分训练的专家无法学习到有效的专业知识，从而浪费了大量的模型参数。\n为了解决这个问题，研究者引入了一种辅助负载均衡损失 (Auxiliary Load-Balancing Loss) 函数，它作为正则化项被添加到模型的总损失中，以激励路由器将令牌更均匀地分配给所有专家。Switch Transformer (Fedus et al., 2021) 中使用的公式如下：\nL_balance = λ * N * Σ(f_i * P_i) 在这个公式中，f_i 是实际分配给第 i 个专家的令牌占总令牌数的比例，而 P_i 则是门控网络为第 i 个专家输出的平均门控概率。通过最小化这个损失项，模型被鼓励去平衡 f_i 和 P_i 的乘积，从而实现更均匀的专家利用率。值得注意的是，该公式取代了 Shazeer et al. (2017) 中使用的基于变异系数（coefficient-of-variation）的早期损失函数，提供了一个更简洁、梯度信号更稳定的解决方案。\n除了辅助损失函数，研究界还探索了其他负载均衡解决方案，主要包括：\n带噪声的 top-k 门控: 在 Shazeer 等人 (2017) 的早期工作中提出，通过在门控网络的输出上添加少量高斯噪声，可以增加路由决策的随机性，鼓励模型探索并利用更多的专家。 Expert Choice 路由: 这是一种创新的路由范式，它将选择方向反转，从“令牌选择专家”变为“专家选择令牌”。每个专家主动从一批令牌中选择它最想处理的 top-k 个，这种机制从根本上保证了每个专家的负载是固定的，从而实现了固有的负载均衡。 3.3 令牌丢弃 (Token Dropping) # 令牌丢弃现象直接与专家容量相关。当路由器分配给某个专家的令牌数量超过了其预设的容量上限时，多余的令牌将被“丢弃”（dropped）。这些被丢弃的令牌会跳过专家计算，其表示通常通过残差连接直接传递到下一层。\n令牌丢弃会对模型性能产生潜在的负面影响，因为它意味着一部分输入信息没有经过专家网络的专门处理，可能导致信息损失。然而，也有观点认为，适度的令牌丢弃（例如，低于 1%）可能起到一种隐性的正则化作用，防止模型对某些专家过度依赖。尽管如此，过高的丢弃率是训练不稳定的明确信号，需要及时解决。\n为了有效缓解令牌丢弃问题，研究界已经开发了多种策略：\n提高容量因子 (α): 这是最直接的方法。通过增大 α 的值，可以为每个专家提供更大的处理缓冲区，从而容纳更多令牌，直接减少因容量不足而导致的丢弃。但这种方法的代价是增加了计算和内存成本，因为系统需要为可能出现的峰值负载预留更多资源。 辅助负载均衡损失: 这种方法从根源上解决问题。通过在训练目标中加入负载均衡损失，可以激励路由器做出更均匀的分配决策，从而减少任何单个专家接收到超额令牌的可能性，间接降低了令牌丢弃率。 令牌重路由 (Rerouting): 一些先进的路由机制，如 Expert Choice 路由，并非简单地丢弃溢出的令牌。它们设计了重路由（rerouting）逻辑，将一个专家无法处理的令牌重新分配给其他容量尚未饱和的专家。这种策略最大限度地确保了每个令牌都能得到专家处理，从而减少了信息损失。 动态容量分配: 更复杂的系统会根据训练过程中的实时负载动态调整每个专家的容量。例如，系统可以监测各个专家的利用率，并周期性地将容量从利用率较低的专家重新分配给负载较重的专家，以实现更灵活和高效的资源利用。 令牌优先级与丢弃策略: 当必须丢弃令牌时，可以采用智能策略而非随机丢弃。例如，Switch Transformer 采用的策略是优先丢弃那些门控网络分配概率最低的令牌。这确保了与专家最相关的、置信度最高的令牌被保留下来进行计算，从而最大限度地减少了对模型性能的影响。 监控丢弃指标: 在系统层面，持续监控每一步的令牌丢弃率、每个专家的负载分布等关键指标至关重要。这些指标是诊断路由健康状况和训练稳定性的核心依据。持续的高丢弃率或严重的负载不均衡通常预示着需要调整容量因子或负载均衡损失的权重。 架构替代方案: 一些前沿的 MoE 架构从设计层面就旨在消除令牌丢弃。例如，Soft MoE 通过一种软性的、可微的加权机制，让每个令牌与所有专家都进行交互，从而避免了硬性的 top-k 选择和容量限制。而 MegaBlocks 等框架则通过将 MoE 计算重构为块稀疏矩阵乘法，从系统层面实现了无需丢弃令牌的高效训练。 总而言之，专家容量、负载均衡和令牌丢弃这三大挑战及其解决方案共同构成了 MoE 系统设计的核心权衡空间。工程师和研究人员必须在模型性能、训练稳定性和计算效率之间做出精心的平衡与选择。这些权衡也催生了现代 MoE 架构的多样化发展。\n4.0 现代 MoE 架构分类体系 # 随着研究的深入，专家混合模型已从单一的稀疏激活范式，演化为一个丰富多样的架构家族。不同的 MoE 架构为特定的目标（如极致的计算效率、更深层次的专家专业化，或强大的多模态处理能力）进行了专门的设计与优化。本章旨在对这些现代 MoE 系统进行系统性分类，并揭示其背后独特的设计思想。\n4.1 稀疏 MoE 架构 (Sparse MoE Architectures) # 这类架构是 MoE 的主流形式，其核心特性是每个输入仅激活一小部分专家，从而在实现巨大参数扩展的同时保持计算成本的可控性。\n代表模型: Switch Transformer, GLaM, Mixtral-8x7B 关键属性与适用场景: 它们采用 top-k 令牌路由机制，确保每个令牌只由少数几个专家处理。这种稀疏激活的特性使得它们成为进行大规模语言模型预训练的理想选择，因为它们能够以相对较低的计算预算训练出参数量极大的模型。 4.2 稠密-混合 MoE 架构 (Dense–Hybrid MoE Architectures) # 这类架构巧妙地将稀疏的 MoE 层与传统的稠密 Transformer 模块相结合，旨在同时利用两者的优势。\n代表模型: T5-MoE, DeepSeek-V2 关键属性与适用场景: 通常，这类模型会在网络的特定部分（如较深的层或专门的前馈网络层）嵌入 MoE 模块，而其他部分（如注意力层）则保持稠密。这种混合设计保留了稠密模型训练的稳定性，同时利用 MoE 的稀疏性来有效扩展模型的容量，实现了稳定性与效率的平衡。 4.3 层级化与结构化 MoE 架构 (Hierarchical and Structured MoE Architectures) # 这类架构引入了多层次的路由结构，使专家能够以更具组织性的方式进行专业化分工。\n代表模型: Hierarchical Mixture of Experts (HMoE), Sparse-Transformer++ 关键属性与适用场景: 在层级化 MoE 中，可能存在一个高层级的“全局”路由器，负责将任务分配给一个专家组，然后再由组内的“局部”路由器选择最终的专家。这种多粒度的路由机制不仅提升了模型的效率，还有助于模型学习到不同抽象层次的知识，增强了模型的可解释性。 4.4 多模态与联合 MoE 架构 (Multimodal and Joint MoE Architectures) # 这类架构将 MoE 的原理扩展到处理多种数据模态（如文本、图像、音频）的场景中，通过共享专家或专门的路由机制实现跨模态的高效学习。\n代表模型: Uni-MoE, Union of Experts 关键属性与适用场景: 在这些模型中，来自不同模态的令牌可以被路由到同一组共享的专家中（联合 MoE），或者通过分层路由进行 modality 融合。这不仅促进了跨模态知识的融合与对齐，还通过参数共享极大地提升了多模态大模型的扩展能力和泛化性能。 4.5 自适应与动态 MoE 架构 (Adaptive and Dynamic MoE Architectures) # 这类架构的核心思想是根据输入的复杂性或重要性，动态地调整计算资源的分配。\n代表模型: AdaMoE, Expert Choice Routing (EC-MoE) 关键属性与适用场景: 例如，AdaMoE 引入了“空专家”（null expert）的概念，对于简单或不重要的令牌，可以选择跳过专家计算，从而节省算力。而 Expert Choice 路由则反转了路由方向，让专家主动选择它们想要处理的令牌，这不仅能实现更好的负载均衡，还能促进更具语义一致性的专家专业化。这类架构的目标是让计算开销与信息复杂性相匹配。 现代 MoE 架构概览 # 下表对上述现代 MoE 架构的关键特性进行了总结：\n类型 代表模型 路由机制 核心优势 稀疏 MoE Switch Transformer, GLaM, Mixtral Top-k 令牌路由 极致的可扩展性 稠密-混合 MoE T5-MoE, DeepSeek-V2 部分层级 MoE 集成 稳定性与效率的结合 层级化 MoE HMoE, Sparse-Transformer++ 多级专家路由 可解释性，多尺度推理 多模态/联合 MoE Uni-MoE, Union of Experts 跨模态路由 统一的多模态处理 自适应/动态 MoE AdaMoE, EC-MoE 令牌或专家自适应路由 计算量与复杂度成正比 综上所述，现代 MoE 系统正在从简单的令牌路由器演变为复杂的、层级化的、多模态的、自适应的专家生态系统。这种演进凸显了 MoE 架构在构建下一代人工智能系统中的核心地位。而这一切演进的背后，都离不开路由机制本身的不断创新。\n5.0 高级路由机制：超越传统令牌分配 # 路由（Routing）是专家混合模型架构的灵魂，它决定了信息如何在庞大的专家网络中流动。路由机制的每一次演进，都直接推动了 MoE 模型在性能和效率上的飞跃。传统的 top-k 路由虽然开创了稀疏 MoE 的时代，但其固有的负载不均衡和忽略令牌间关系的局限性也日益凸显。本章将探讨两种超越传统范式的先进路由机制：Expert Choice 路由和结构化路由，它们分别从不同的维度重新定义了令牌与专家之间的交互方式。\n5.1 Expert Choice 路由：反转选择方向 # Expert Choice（EC）路由机制通过一个简单而深刻的变革，从根本上解决了传统路由中的负载均衡难题。其核心思想是将路由的选择方向从“令牌选择专家”反转为“专家选择令牌”。\n在传统路由中，每个令牌独立地通过门控网络选择最适合自己的专家，这常常导致某些“受欢迎”的专家被过度选择，而另一些则被冷落。EC 路由则颠覆了这一流程：\n令牌-专家评分: 首先，系统依然会计算一个评分矩阵，其中每个元素代表了某个令牌与某个专家之间的亲和度或相关性。 专家容量定义: 每个专家被预先设定一个固定的容量（capacity），即它在一个批次中最多能处理的令牌数量。 专家选择令牌: 接下来，每个专家会审视评分矩阵中与自己相关的那一列分数，并从中选择得分最高的 k 个令牌（k 即为其容量）。 数据重排: 系统根据专家的选择结果，通过一次高效的置换（permutation）操作，将所有被选中的令牌重新排列，使得被同一个专家选中的令牌在内存中是连续的。 专家计算: 每个专家对其选择的令牌块进行并行计算。 输出重组: 计算完成后，通过一次逆置换（inverse permutation）操作，将专家的输出重新组合，恢复到原始令牌的顺序。 EC 路由的主要优势体现在以下几个方面：\n固有的负载均衡: 由于每个专家处理的令牌数量是预先固定的，EC 路由从机制上消除了负载不均衡问题，不再需要额外的辅助损失函数来进行正则化。 更优的专家专业化: 专家能够主动选择最符合其“口味”的令牌，这使得它们的专业化方向更加明确和一致，有助于提升模型的整体性能。 从机制上消除令牌丢弃: 传统 top-k 路由中因专家容量溢出而导致的令牌丢弃问题，在 EC 路由中被根本性地解决了。因为每个专家的负载由其自身预设的容量决定，从机制上避免了过载情况的发生。 通过这种设计，EC 路由直接解决了在第 8.0 节中将要讨论的“训练不稳定与负载不均衡”以及“模型容量的未充分利用”等核心局限性。\n5.2 结构化与层级化路由：捕捉更高阶关系 # 传统路由机制的另一个核心局限是它将每个令牌视为独立的决策单元，完全忽略了令牌之间存在的丰富语义和结构关系（例如，一句话中的词语或一张图片中的邻近像素块）。为了克服这一局限，研究者们开始探索从独立的“令牌级路由”转向更具上下文感知能力的“结构感知路由”。\n以下是几种前沿的结构化路由范式：\n基于聚类的路由 (Clustering-Based Routing): 在这种模式下，门控网络被训练来隐式地学习将语义上相似的令牌进行聚类，并将整个簇（cluster）的令牌路由到同一个专家。如 Dikkala et al. (2023) 的研究所示，这确保了相关的上下文信息能够被同一个专家处理，从而促进了更深层次的语义理解和专家专业化。 层级化路由架构 (Hierarchical Routing Architectures): 这种架构通过构建多级路由结构来模拟人类的决策过程。例如，一个顶层的“全局路由器”首先根据输入的粗粒度特征（如主题或任务类型）选择一个合适的专家组；然后，组内的“局部路由器”再根据输入的细粒度特征，从该组中选择最终处理任务的一到两个专家。这种分层决策不仅提高了路由效率，也使得模型的内部工作机制更具可解释性。 基于图与注意力的路由 (Graph- and Attention-Based Routing): 最新的研究开始利用令牌间的图结构或注意力关系来指导路由决策。例如，Nguyen et al. (2025) 提出构建一个“令牌图”，其中节点是令牌，边代表它们之间的语义或句法关联。路由决策可以被建模为在这个图上的消息传递过程，使得每个令牌的路由选择都能考虑到其邻近令牌的信息。这种方法使得路由决策更具上下文感知能力，能够捕捉到更复杂的语言结构。 总而言之，高级路由机制的出现，正推动 MoE 架构中的路由决策从纯粹的统计匹配，转变为一个更加智能、更具语义驱动的决策过程。这种转变不仅提升了模型性能，也为 MoE 在更复杂的应用场景中的部署铺平了道路，例如在 Transformer 的其他核心模块中的应用。\n6.0 MoE 架构的扩展与实现 # 专家混合模型的应用潜力已不再局限于传统的 MLP（多层感知机）层。随着研究的深入，其核心的条件计算原理正被创造性地扩展到 Transformer 架构的其他核心组件中，以期在更广泛的层面提升模型的表示多样性与计算效率。与此同时，为了在硬件层面支撑起这些庞大的 MoE 模型，一种专门的分布式实现技术——专家并行主义——也应运而生。本章将探讨 MoE 原理的架构扩展，并分析其关键的系统实现技术。\n6.1 超越 MLP 层：MoE 在注意力与其他模块中的应用 # 将 MoE 扩展到非 MLP 组件的主要动机在于，Transformer 的不同部分负责不同的功能，对专业化的需求也各不相同。通过在更多组件中引入专家，可以使模型在处理不同类型信息时，能够调用更多样化的、专门化的计算单元。\n注意力层 (MoE in Attention Layers): 注意力机制是 Transformer 的核心，但传统的自注意力（self-attention）在处理所有令牌时都使用相同的参数。为了引入专业化，研究者提出了**“注意力混合 (Mixture-of-Attention, MoA)”**的概念。其核心思想是用一组专业的“注意力专家”来替代单一的自注意力机制。每个注意力专家都有自己独特的查询（Query）和输出（Output）投影矩阵，但通常共享键（Key）和值（Value）投影矩阵以控制参数量。路由网络会根据每个令牌的特性，为其选择最合适的注意力专家组合。这使得模型能够学习到多种不同的注意力模式，例如，某些专家可能专注于捕捉局部句法关系，而另一些则可能更擅长识别长距离的语义依赖。 多模态编码器与连接器 (MoE in Modality Encoders and Connectors): 在处理图像、文本等多种数据模态的大型视觉语言模型（LVLM）中，MoE 也展现出巨大的潜力。像 CuMo (Li et al., 2024) 和 MoE-LLaVA 等前沿模型，已成功将 MoE 模块集成到视觉编码器和连接视觉与语言模块的跨模态连接器中。在视觉编码器中引入 MoE，可以让不同的专家专注于识别不同类型的视觉特征（如纹理、形状、物体等）。在跨模态连接器中应用 MoE，则可以帮助模型学习到更丰富、更精准的视觉-语言对齐方式。实验证明，这种做法能有效提升模型在复杂视觉问答和指令遵循任务上的表现。 6.2 专家并行主义 (Expert Parallelism) # 专家并行主义（Expert Parallelism, EP）是为高效训练和部署大规模 MoE 模型而设计的一种特定的模型并行策略。它的核心思想是将 MoE 层中的不同专家子网络分布到不同的计算设备（如多个 GPU）上。\n与传统的并行策略相比，专家并行有其独特性：\n数据并行 (DP): 在每个设备上都保留一份完整的模型副本，并将数据批次切分给不同设备处理。 张量并行 (TP): 将单个大权重矩阵（如 MLP 或注意力层中的矩阵）切分到不同设备上，协同完成一次矩阵运算。 流水线并行 (PP): 将模型的不同层（stages）放置在不同设备上，形成一个计算流水线。 专家并行 (EP): 它不对单个权重矩阵或整个模型进行切分，而是将完整的、独立的专家单元分配到不同设备。例如，在一个有 8 个专家和 8 个 GPU 的系统中，每个 GPU 可以独立负责一个专家的全部参数和计算。 专家并行之所以是扩展 MoE 模型至万亿参数规模的唯一可行策略，其原因在于其他并行方式的根本局限性。数据并行（DP）会要求在每个设备上都复制模型的全部参数，对于一个 1.8 万亿参数的模型来说，这在内存上是完全不可行的。张量并行（TP）虽然能切分单个大矩阵，但 MoE 的核心是由许多独立的专家网络构成，TP 无法有效地对这些离散的模块进行分区。流水线并行（PP）虽然能将不同层分布到不同设备，但它本身无法解决单层内部参数量过大的问题。因此，只有专家并行（EP）能够通过将独立的专家模块分布到不同设备上，从根本上解决万亿级 MoE 模型的内存瓶颈。\n专家并行为何至关重要？其核心优势可归纳为：\n参数规模扩展: 这是 EP 最核心的价值。它允许模型的总参数量随着设备数量的增加而线性增长，而无需增加单个设备的内存负担。这是训练万亿参数 MoE 模型的关键技术前提。 内存效率: 由于每个设备只需存储一小部分专家，而不是整个模型的全部参数，因此极大地降低了单个设备的显存（VRAM）需求。 可扩展性: EP 能够与其他并行策略（如数据并行和张量并行）相结合，形成混合并行方案，从而在超大规模的计算集群上实现高效的模型扩展。 在专家并行的实现中，通信是关键环节。当一个设备上的令牌需要由另一个设备上的专家处理时，它们之间需要进行数据交换。这个过程依赖于一种高效的**“all-to-all”**通信模式，即每个设备都需要向所有其他设备发送一部分数据（发往对应专家的令牌），并从所有其他设备接收一部分数据（由本地专家处理的令牌）。这种通信模式对网络带宽要求极高，也是 MoE 训练中的主要性能瓶颈之一。\n总而言之，MoE 的应用边界正在从架构层面到系统实现层面不断拓宽，持续的创新使其能够应对更大规模、更复杂的挑战。而这一切成功的背后，都离不开一个核心机制——专家专业化。\n7.0 专家专业化分析 # 专家混合模型成功的核心机制之一，是其能够在训练过程中自发地形成“专家专业化”（Expert Specialization）现象。这意味着，尽管所有专家在初始化时是相同的，但随着训练的进行，它们会逐渐学习并专注于处理不同类型的数据、模式或任务。这种涌现出的分工协作，是 MoE 能够以较低的计算成本实现强大性能的关键。本章将通过理论解释和具体模型案例，分析专家专业化是如何形成的，以及它在实践中表现出的具体模式。\n7.1 专业化的形成机制 # 在一篇名为《Towards Understanding the Mixture-of-Experts Layer in Deep Learning》的研究中，研究者通过一个巧妙的“玩具实验”直观地展示了专家专业化的形成过程。实验设置了一个包含四个明显数据簇的二元分类问题，并使用一个拥有四个专家的 MoE 模型来解决它。\n初始阶段： 在训练开始时，所有专家的参数都是随机初始化的，门控网络（路由器）也将输入数据近似随机地分配给各个专家。此时，专家之间没有明显的区别，它们的处理能力也基本相同。 演化过程： 随着训练的进行，由于初始权重的微小随机差异，某些专家在处理特定数据簇时会表现出微弱的优势。门控网络通过反向传播学习到了这种微弱的信号，并开始倾向于将该数据簇的更多样本路由给这个稍具优势的专家。 正反馈循环： 这种倾向性形成了一个正反馈循环。一个专家处理某一类数据的样本越多，它就越能学习到该类数据的内在模式，从而在该类数据上表现得更好。而它表现得越好，门控网络就越有信心将这类数据路由给它。 最终状态： 经过充分训练后，模型达到一个稳定状态：四个专家与四个数据簇之间几乎形成了一一对应的关系。每个专家都成为了处理特定数据簇的“专家”，而门行网络则演变成一个高效的“聚类器”和“调度员”，负责识别输入数据属于哪个簇，并将其精确地导向对应的专家。 这个实验揭示了，专家专业化并非预先设定，而是在训练数据、非线性激活函数和门控网络的共同作用下，从随机状态中自发涌现出的一种有序结构。\n7.2 案例分析：Mixtral 与 MoE-LLaVA # 在真实世界的大规模 MoE 模型中，专家专业化的模式更加复杂和微妙。\nMixtral 的路由分析: Mistral AI 对其开源模型 Mixtral 8x7B 的路由行为进行了深入分析，得出了一个有趣的结论：Mixtral 的专家们表现出的是显著的**“句法专业化”**而非“语义专业化”。这意味着，专家并非根据输入的主题（如生物、哲学或数学）来进行分工，而是根据输入的句法结构。例如，特定的标点符号、代码中的缩进或某种特定的语法结构（如 JSON 格式的括号），无论出现在什么主题的文本中，都倾向于被路由到固定的几个专家。这表明模型可能学会了将语言结构的基础“解析”工作委托给特定专家，从而释放其他专家的容量以专注于更高层次的语义处理，这是一种高效的资源分配策略。 MoE-LLaVA 的专家负载与偏好: 在多模态模型 MoE-LLaVA 中，研究者也观察到了专家专业化的独特模式。他们发现，在网络的不同深度上，专家的活跃程度（负载）是不同的。在较浅的层，几个专家协同工作；而在较深的层，则可能由某个专家主导计算。更有趣的是，尽管模型同时处理文本和图像两种模态的数据，但分析显示，专家并未对特定模态表现出明显的偏好。文本令牌和图像令牌在专家间的路由分布高度相似。这表明，MoE-LLaVA 的专家学习到的是更抽象、跨模态的通用知识，而不是专门处理单一模态的知识，这恰恰反映了其强大的多模态融合与处理能力。 综上所述，专家专业化是 MoE 实现高效学习的关键现象，但其具体的形成机制和表现模式在不同模型和任务中可能存在差异，这仍然是一个活跃且富有启发性的研究领域。理解了其优势所在，我们同样需要客观审视 MoE 架构所面临的局限性。\n8.0 MoE 架构的局限性与挑战 # 虽然专家混合模型（MoE）通过条件计算带来了模型规模与效率上的革命性突破，但采用该架构也伴随着一系列独特的、系统性的挑战。这些局限性不仅增加了模型训练和部署的复杂性，也对硬件基础设施提出了更高的要求。为研究人员和工程师提供一个全面的风险视角，本章将客观、系统地评估 MoE 的主要局限性。\n训练不稳定与负载不均衡 这是 MoE 架构最核心的挑战之一。由于路由决策是动态的，很容易出现部分专家被过度使用，而其他专家则长期闲置的“负载不均衡”现象。这不仅会导致优化过程困难，训练不稳定，还可能引发“专家崩溃”（expert collapse），即未被充分训练的专家无法学习到任何有效知识，从而浪费了大量的模型参数。虽然通过辅助负载均衡损失等方法可以缓解该问题，但实现完美的均衡仍然非常困难。 通信开销与硬件依赖 在分布式训练中，MoE 依赖一种称为“all-to-all”的通信模式来在不同设备间传递令牌。这种通信模式要求每个设备与所有其他设备进行数据交换，其通信开销非常巨大，常常成为训练过程中的主要性能瓶颈。因此，高效的 MoE 训练严重依赖于具备高速互联能力的硬件，如 Google 的 TPU Pods 或集成了 NVLink/NVSwitch 的 NVIDIA GPU 集群。在常规硬件上，高昂的通信成本可能会完全抵消 MoE 在计算上的稀疏优势。 路由复杂性与梯度碎片化 门控网络做出的 top-k 路由决策本质上是离散的（选择或不选择），这种离散性会破坏梯度在反向传播过程中的平滑流动，被称为“梯度碎片化”。这不仅给优化带来了挑战，还可能引入训练不稳定性。为了解决这个问题，研究者们采用了诸如添加噪声、软性门控或使用辅助损失等技巧，但这无疑增加了模型的整体复杂性。 模型容量的未充分利用 MoE 模型虽然拥有海量的总参数，但对于任何一个输入样本，只有一小部分参数（通常是一到两个专家）被激活。这意味着模型的“有效表示容量”（effective representational capacity）远小于其名义上的总参数量。大量参数在大多数时间里处于“休眠”状态，这引发了关于这种参数扩展方式是否真正高效的讨论。如何动态地、更充分地利用这些闲置参数，是当前的一个重要研究课题。 推理不稳定与延迟可变性 动态路由机制使得 MoE 模型在推理时的延迟（latency）变得难以预测。根据输入内容的不同，令牌可能被路由到不同的设备上，导致每次推理的计算路径和通信模式都可能发生变化。这种延迟的可变性给需要稳定、低延迟响应的实时系统（如在线服务、对话式 AI）的部署带来了巨大挑战。 高显存 (VRAM) 与内存驻留需求 这是一个与稀疏计算优势形成鲜明对比的悖论。尽管在计算时是稀疏的，但在推理部署时，所有专家的参数通常必须同时加载到显存（VRAM）中。这是因为路由决策是实时的，系统无法预知下一个令牌将被分配给哪个专家，因此必须让所有专家都处于“待命”状态。这导致 MoE 模型的显存占用与其总参数量成正比，对硬件的内存容量提出了极高的要求。这正是第 6.2 节中讨论的专家并行（Expert Parallelism）所带来的根本性权衡：EP 通过将专家分布到多个设备上，实现了训练时参数规模的扩展，但这种分布式特性也意味着在推理时，所有专家必须同时驻留在内存中，从而造成了巨大的推理部署瓶颈。 克服上述这些局限性是当前 MoE 研究的前沿方向。接下来，我们将通过分析成功的 MoE 模型案例，来了解这些挑战在实践中是如何被有效管理和权衡的。\n9.0 知名 MoE 模型案例分析 # 理论的探讨和挑战的分析最终都需要在实际的模型中得到验证和体现。专家混合模型架构的真正影响力，体现在它如何赋能那些定义了行业标杆的旗舰级模型。本章将剖析两款具有里程碑意义的 MoE 模型——神秘的 GPT-4 和开源的 Mixtral 8x7B，以揭示 MoE 架构在真实世界中的设计选择、性能优势及其带来的权衡。\n9.1 GPT-4：开启大规模 MoE 时代 # 尽管 OpenAI 从未正式公布 GPT-4 的详细架构，但根据业界广泛流传且可信度较高的信息，GPT-4 被认为是一个大规模的专家混合模型，其设计开启了 MoE 在商业化顶级模型中应用的时代。\n根据业界的推测，GPT-4 可能是一个拥有 16 个专家的 MoE 模型。每个专家自身的参数量巨大（可能约为 111B），再加上共享的注意力参数，使得模型的总参数量达到了约 1.8 万亿。这种前所未有的参数规模是其强大能力的基础。 MoE 架构的核心优势在于，它使 GPT-4 能够在保持顶级性能的同时，将计算成本控制在可接受的范围内。据估计，在每次前向传播（即生成一个 token）中，GPT-4 仅需激活其中的 2 个专家，涉及的计算参数量约为 280B。这一数字远低于一个同等规模的稠密模型所需的计算量（1.8T），从而实现了计算效率与模型能力的解耦。 然而，这种架构也带来了权衡。在推理服务中，由于并非模型的所有部分都被每个请求所利用，如何高效地管理计算资源、最大化硬件利用率成为一个巨大的工程挑战。这也解释了为何 GPT-4 的推理成本相较于前代模型显著增加。 9.2 Mixtral 8x7B：开源 MoE 的标杆 # 如果说 GPT-4 验证了 MoE 在闭源商业模型中的可行性，那么由 Mistral AI 发布的 Mixtral 8x7B 则为开源社区带来了 MoE 架构的标杆之作，极大地推动了该技术的普及和研究。\nMixtral 8x7B 是一个拥有 8 个专家的开源 MoE 模型，其中每个专家的核心参数规模约为 7B。在每个 Transformer 层的解码过程中，其路由器会为每个 token 选择 2 个最合适的专家进行计算，然后将它们的输出进行组合。 该架构的最大优势在于其卓越的性能-效率比。其总参数量约为 47B，但由于稀疏激活，每个令牌的推理仅涉及约 14B 参数。这种设计使其推理速度比 Llama 2 70B 模型快 6 倍，同时在多项基准测试中性能与之相当甚至更优。 对 Mixtral 的分析还揭示了其内部专家已展现出明显的专业化分工。研究发现，其路由器能够智能地为不同的输入选择合适的专家组合来共同回答问题，证明了 MoE 架构中专家协作机制的有效性。 GPT-4 和 Mixtral 8x7B 这两个成功案例，从闭源和开源两个维度共同证明了专家混合模型架构在当前人工智能发展阶段的巨大潜力和核心地位。它们的设计哲学和实践经验，为下一代更强大、更高效的 AI 模型的设计提供了宝贵的指引。\n10.0 总结与未来展望 # 专家混合模型（MoE）架构，以其独特的条件计算范式，为现代人工智能的发展提供了一条在计算资源有限的现实下，通往更强大、更高效模型的关键路径。其核心价值在于，通过稀疏激活一小部分专门化的“专家”，实现了模型参数规模与计算成本之间的解耦，为构建前所未有的超大规模模型打开了大门。\n本文档系统性地梳理了 MoE 架构的全貌。我们从其“分而治之”的核心原理出发，追溯了它从早期理论到被 Switch Transformer 等模型发扬光大的发展历程。我们深入探讨了实施 MoE 所面临的三大核心技术挑战——负载均衡、专家容量和令牌丢弃——并分析了相应的解决方案。此外，我们还对现代 MoE 架构进行了分类，揭示了从稀疏、混合到层级化、多模态的多样化设计，并探讨了 Expert Choice 和结构化路由等高级路由机制的演进。\n展望未来，MoE 架构的研究仍处于高速发展阶段，多个前沿方向预示着其未来的巨大潜力：\n更深入的理论理解: 尽管 MoE 在实践中取得了巨大成功，但其内在的工作机制，特别是专家专业化是如何自发形成的，以及路由决策背后的深层原理，仍有待进一步揭示。更坚实的理论基础将有助于指导更优的模型设计、提升泛化能力并增强模型的可解释性。 更高效的门控与专家设计: 当前的 top-k 路由机制仍有其局限性。未来，探索超越现有范式的创新路由方法，如更具自适应性、上下文感知能力，甚至是可学习的路由结构，将是提升 MoE 性能的关键。同时，专家本身的设计也将更加多样化，例如在注意力和其它网络模块中应用 MoE。 向新领域的应用拓展: 目前，MoE 的成功主要集中在自然语言处理和计算机视觉领域。未来，将 MoE 的思想和架构应用于更广泛的领域，如强化学习、图神经网络、表格数据分析等，具有巨大的潜力。在这些领域，数据同样具有异构性和复杂性，非常适合利用 MoE 的专业化能力来提升模型性能。 总而言之，专家混合模型不仅仅是一种技术架构，更代表了一种设计哲学：在有限的计算预算下，通过智能化、专业化的资源调配，实现智能的最大化。随着研究的不断深入和技术的持续创新，MoE 范式必将继续推动人工智能的边界，引领我们走向一个更加智能、高效且可扩展的未来。\n参考 # MoE notebookLLM 总结\n"},{"id":26,"href":"/www6vAlgo/docs/RL/core/PPO-family/PPO/","title":"(原理)PPO","section":"PPO family","content":"\nPPO # (原理)PPO\n"},{"id":27,"href":"/www6vAlgo/docs/RL/core/PPO-family/PPO1/","title":"(原理)PPO","section":"PPO family","content":" PPO训练中四种模型的合作关系 # PPO训练中各模型的输入与输出 # 基于PPO进行RLHF训练的原理图 # 参考 # 第8部分：RLHF 与 RLAIF\n"},{"id":28,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningBatchsize/","title":"(原理)Batchsize","section":"网络优化","content":"\n最佳实践 # batchsize # batchsize 下限 [1] 别太小的限制在于，batch size太小，会来不及收敛。\n所以在常见的setting（～100 epochs），batch size一般不会低于16。\nbatchsize 上限 [1] batch size别太大的限制在于两个点，\n1）batch size太大，memory容易不够用。这个很显然，就不多说了。\n2）batch size太大，深度学习的优化（training loss降不下去）和泛化（generalization gap很大）都会出问题。\nlearning rate \u0026amp; batch size # 总之，可以证明，learning rate/batch size的比值对深度学习是有指数级的影响[3]，所以非常重要，没事别瞎调。[1]\n这也是为什么大的batch_size往往建议可以相应取大点learning_rate, 因为梯度震荡小，大learning_rate可以加速收敛过程，也可以防止陷入到局部最小值，而小batch_size用小learning_rate迭代，防止错过最优点，一直上下震荡没法收敛（这也是一个小trick）。[2]\n参考 # 怎么选取训练神经网络时的Batch size? Summer Clover 训练神经网络时batchsize扩大一倍的同时需要增加epoch数量吗? 新一 7.1 批大小调整实验 百度邱\n7.1 批大小调整实验\n设置BatchSize\n深度学习中的batch的大小对学习效果有何影响？\n"},{"id":29,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningForwardBackward/","title":"(原理\u0026实战)前向/反向传播","section":"basic","content":"\n前向/反向传播 # (原理\u0026amp;实战)前向/反向传播\n"},{"id":30,"href":"/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/Dropout/","title":"(原理\u0026实战)Dropout","section":"正则化","content":"\nDropout # (原理\u0026amp;实战)Dropout\n"},{"id":31,"href":"/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLModel/","title":"机器学习-模型","section":"机器学习","content":"\n机器学习-模型 # 机器学习-模型\n"},{"id":32,"href":"/www6vAlgo/docs/DeepLearning/Transformer/SelfAttention/","title":"(原理)Self-Attention","section":"Transformer","content":"\nSelf-Attention # (原理)Self-Attention\n"},{"id":33,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Embedding/survey/","title":"(Survey)Embedding","section":"Embedding","content":" 论文 # 论文地址\nOn The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey 通用文本嵌入（GPTE）模型的典型架构和训练方式 # 通用文本embedding的代表模型及参数 # 参考 # Embedding的9点总结-从架构、数据到代表模型\n"},{"id":34,"href":"/www6vAlgo/docs/LLM/Survey/LargeModel/","title":"大模型","section":"Survey","content":" 参考 # 1xx. [译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023） 实战\n1xx. 通向AGI之路：大型语言模型（LLM）技术精要 ***\n1xx. 必看的十二个大模型前沿综述：兼论HALO大模型幻觉检测与缓解方案及Google小模型预测大模型训练不稳定的探索 12个综述\n"},{"id":35,"href":"/www6vAlgo/docs/LLM/Core/Emergent/","title":"(原理)涌现现象","section":"Core","content":" Emergent Abilities # 🔗 文章：Emergent Abilities of Large Language Models (2022.10) (arxiv.org) 🔑关键词和摘要 Keywords: LLMs, Emergent Ability, Scaling abstract 不可预测 不能从小模型的的性能外推 是否能通过继续扩大模型规模来获得更多涌现能力 ⚙️研究设计和结论 定义 通常的涌现现象 大模型的涌现现象 小模型接近随机 大模型突然出现 相变 实验框架 performance vs 1. FLOPs, model parameters Training datasets 叠甲：emergent 与很多因素都有关，本文并不是说到哪个 scale 就会出现 emergent，而是说 emergent 现象普遍存在。 实验1 Few-shot Prompting 测试数据说明: A: 三位数加法，两位数乘法 B: [dɪfərənt], 复原 \u0026ldquo;different,\u0026rdquo; C: 从 e l h l o 复原 hello D: 波斯语问答 E: 针对GPT-3 对抗标的问答 \u0026hellip; 结果 这些 task，以 few-shot 形式展示过以后，都有 emergent 不同模型 emergent scale 不一样 有的 task，只有 540B 的 PaLM emerge了 实验2 增强语言模型能力的 emerge 现象 已知的一些大模型技巧在何种规模下发挥作用？ 大模型技巧 思维链 Chain-of-thought: Let\u0026rsquo;s think step by step. 指令微调 请写一段XXX的描述 草稿本方法： 计算 15+16, 让模型在草稿本上写“5+6=11，进位1” 这些增强语言模型能力的方法都有一定程度的涌现 联想：之前的 prompt tuning，parameter efficient tuning，都是某种随着模型规模扩大的涌现？ 讨论 Emergent 现象的解释 多步能力说 每个子能力达到 90% -\u0026gt; 一无是处 每个子能力达到 95% -\u0026gt; 能完成一些任务了 指标缺陷说 奇怪的现象：交叉熵损失不是 emergent 的，而是在逐步下降 Emergent 的阈值可能会越来越小 更干净的数据，更好的训练技巧，更优秀的模型结构都可以是 Emergent阈值变小 未来方向： 继续扩大 model scale，远未达到上限 一些新结构的 scaling 数据的 scaling 理解 prompt 机制 更前沿的 task，用来指导 emergent 理解 emergence 📚论文贡献 优点 第一次正式提出 emergent 实验 做了充分的实验表明该现象在各种数据集上广泛存在 甚至验证了一些“方法”的涌现 提出了一些解释该现象的观点，并提出质疑 改进点 还是不知道为啥 emerge 实验采用各种不同模型，无法得出哪个计算量级对哪种能力有 emerge 参考 # 清华博士带你思考大语言模型LLM的涌现现象（Emergent） 有脑图\nEmergent Abilities of Large Language Models （https://arxiv.org/abs/2206.07682）\n再谈ChatGPT等大模型的涌现能力：关于涌现能力的定义、测试方法及分析工作总结 "},{"id":36,"href":"/www6vAlgo/docs/LLM/Dense/Llama/","title":"LLaMA","section":"Dense","content":" LLaMA # LLaMA\n"},{"id":37,"href":"/www6vAlgo/docs/LLM/Survey/LLM-Deep-Dive/","title":"LLM Deep Dive","section":"Survey","content":" 大型语言模型深度解析：从预训练到强化学习 # 执行摘要 # 本文档深入剖析了大型语言模型（LLM）的构建、运作原理及其认知特性，其信息完全基于 Andrej Karpathy 的技术讲解。大型语言模型的开发过程主要分为三个核心阶段，其复杂性和计算成本逐级递减，但对最终模型能力的塑造至关重要。\n预训练 (Pre-training)：此阶段是计算最密集、成本最高昂的环节。通过处理海量的互联网文本数据（如 FineWeb 数据集的 15 万亿个词元），模型学习语言的统计规律。其核心任务是“预测下一个词元”，从而将互联网的知识以一种有损压缩的形式编码到其数十亿甚至上万亿的参数中。此阶段的产物是基础模型 (Base Model)，它本质上是一个“互联网文档模拟器”，能够生成统计上类似互联网文本的内容，但本身并非一个可直接交互的助手。\n后训练：监督式微调 (Supervised Fine-Tuning, SFT)：此阶段旨在将基础模型转化为一个助手 (Assistant)。通过在一个由人类标注员根据特定指南创建的高质量对话数据集上继续训练，模型学习模仿专家行为。此时，用户与模型的交互，本质上是在与一个“人类标注员的统计模拟”进行对话。模型的回答反映了其训练数据中人类标注员遵循的“有用、真实、无害”等原则。\n后训练：强化学习 (Reinforcement Learning, RL)：这是提升模型高级能力的关键阶段，尤其是在需要推理和解决问题的领域。\n在可验证领域 (Verifiable Domains)，如数学和编程，模型通过生成大量候选解决方案并根据最终答案的正确性进行“试错”学习。这个过程使其能够发现超越简单模仿的人类专家、更适合其自身认知结构的、新颖有效的“认知策略”，即学会“思考”。 在不可验证领域 (Unverifiable Domains)，如创意写作，通过基于人类反馈的强化学习 (RLHF)，训练一个“奖励模型”来模拟人类偏好，从而指导主模型进行优化。然而，这种方式存在局限性，因为奖励模型本身可能被“欺骗”，限制了其性能提升的上限。 最终，大型语言模型是一个强大的、但并非绝对可靠的工具。它们是随机的词元序列生成器，其能力呈现出“瑞士奶酪”模型——在解决奥林匹克竞赛级别的难题时表现出色，却可能在看似简单的常识性问题上犯错。这是由其基于词元的运作方式、每个词元固定的计算量以及训练数据的特性共同决定的。用户应将其视为强大的助手，用于获取灵感、生成初稿和加速工作流程，但必须始终保持批判性思维，验证其输出，并对最终成果负责。\n-\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n第一阶段：预训练——构建知识基础 # 预训练是构建大型语言模型的第一步，也是计算资源消耗最大的阶段。其目标是从海量文本数据中学习语言的底层结构和世界知识。\n1. 数据收集与处理 # 数据来源：通常始于像 Common Crawl 这样的庞大网络爬取档案，该组织自 2007 年以来已索引了数十亿网页。\n数据处理流程：原始数据需经过多阶段的严格过滤和清洗，以确保数量、质量和多样性。\nURL 过滤：移除包含恶意软件、垃圾邮件、成人内容或极端言论等不良网站的 URL。 文本提取：从原始 HTML 代码中剥离导航栏、广告和脚本，仅保留正文内容。 语言过滤：通过语言分类器筛选出特定语言的文档（例如，FineWeb 数据集保留了 65% 以上为英文的页面）。 去重与 PII 移除：删除重复内容，并识别和过滤掉包含个人身份信息（PII）的页面，如地址、社保号码等。 最终数据集：经过处理后，尽管原始互联网数据量巨大，但最终用于训练的文本数据集大小相对可控。例如，FineWeb 数据集大小约为 44TB，包含约 15 万亿个词元（Tokens），这个体量的数据可以存放在单个现代硬盘上。\n2. 分词 (Tokenization) # 神经网络无法直接处理原始文本，必须先将文本转换为一个由有限符号组成的序列。这个过程被称为分词。\n目标：在“词汇表大小”和“序列长度”之间取得平衡。使用较少的符号（如二进制的 0 和 1）会导致序列过长，而我们希望用更多的符号来换取更短的序列，因为序列长度是神经网络中宝贵且有限的资源。 算法：现代 LLM 普遍采用**字节对编码（Byte Pair Encoding, BPE）**算法。该算法迭代地查找文本中最常见的连续字节对，并将它们合并成一个新的、唯一的符号（词元）。 结果：通过这个过程，词汇表被扩展。例如，GPT-4 使用一个包含 100,277 个唯一词元的词汇表。文本被转换成一维的词元 ID 序列。这些 ID 本身没有数值意义，仅仅是唯一标识符。一个单词、一个词根、一个标点，甚至常见的词组都可能成为一个独立的词元。 3. 神经网络训练 # 训练的核心目标是让神经网络学习词元序列中的统计关系，即预测下一个词元。\n训练过程：\n采样：从数据集中随机抽取一个固定长度的词元窗口（称为“上下文”，Context）。例如，一个长度为 4096 或 8192 的窗口。 输入与输出：将上下文输入到神经网络中。网络的输出是一个概率分布，涵盖了词汇表中所有（例如 100,277 个）词元，表示每个词元作为下一个出现的可能性。 学习与更新：在训练开始时，网络参数是随机初始化的，其预测也是随机的。我们将网络的预测与数据集中的“真实”下一个词元进行比较。通过一个称为反向传播的数学过程，微调网络内部的数百万或数十亿个参数（权重），使得“正确”词元的预测概率被“轻推”得更高一点，而其他所有错误词元的概率则相应降低。 迭代：这个“采样-预测-更新”的过程在整个数据集上重复进行数万亿次，通常以“批处理”的方式并行完成。 损失函数 (Loss)：研究人员通过一个名为“损失”的指标来监控训练进程。损失值越低，代表模型的预测与真实数据越吻合。在训练过程中，理想的情况是看到损失值稳步下降。\n4. 神经网络内部结构与推理 # 架构：现代 LLM 普遍采用Transformer架构。它是一个巨大的数学表达式，由多个层组成，每层都包含注意力机制（Attention）和多层感知机（MLP）等模块。信息（词元序列）从输入端流经这些层，与模型的参数进行复杂的混合运算（乘法、加法等），最终在输出端生成下一个词元的概率。\n参数：这些参数可以被看作是混音台上的旋钮。训练的过程就是在寻找一套“最佳”的旋钮设置，使得模型能够准确地模拟训练数据中的语言模式。\n推理 (Inference)\n：训练完成后，模型的参数被固定下来。当用户与模型（如 ChatGPT）交互时，进行的就是推理过程。\n用户提供一个初始词元序列（前缀）。 模型接收这个序列，计算出下一个最可能的词元概率分布。 系统根据这个概率分布进行采样（像掷一个加权的骰子），选择一个词元作为下一个输出。 新生成的词元被附加到序列末尾，形成新的、更长的上下文。 重复步骤 2-4，逐个词元地生成回应，直到达到预设长度或生成一个特殊的“结束”词元。 随机性：由于采样过程的存在，即使输入完全相同的前缀，每次生成的结果也可能不同。这使得模型的输出具有一定的创造性和多样性。 -\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n第二阶段：后训练——从模拟器到助手 # 预训练产出的基础模型虽然蕴含海量知识，但它只会续写文本，无法理解指令或进行对话。后训练阶段的目标就是将其转化为一个有用的助手。这个阶段的计算成本远低于预训练。\n1. 监督式微调 (Supervised Fine-Tuning, SFT) # 这是将模型转变为助手的核心步骤，其本质是通过示例进行编程。\n核心思想：用一个全新的、精心策划的数据集替换掉预训练时使用的互联网文本数据，然后继续训练基础模型。\nSFT 数据集：\n内容：由成千上万个高质量的“指令-回应”对话对组成。这些对话覆盖了广泛的主题和任务类型。 来源：最初由人类标注员根据公司（如 OpenAI）制定的详细标注指南手动创建。这些指南通常长达数百页，定义了理想助手的行为准则（如“有用、真实、无害”）。 演变：现在，SFT 数据集的创建过程也大量借助其他 LLM 来生成初步回答，再由人类专家进行编辑和优化，这种人机协作的方式极大地提高了数据生产的效率和规模。 对话分词：为了让模型理解对话结构，需要引入特殊的控制词元，如 im_start（回合开始）、user（用户发言）、assistant（助手发言）和 im_end（回合结束），将结构化的对话转换回模型能够处理的一维词元序列。\n效果：经过 SFT 训练后，模型学会了识别并遵循对话格式。当它看到以用户提问结尾的上下文时，就会续写出符合助手“人设”的回答。因此，与一个 SFT 模型对话，本质上是在与一个模仿人类标注员行为的统计模拟器进行交互。\n2. 大型语言模型的心理学与局限性 # LLM 的训练方式和内部结构导致了一系列独特的认知特性和行为偏差。\n幻觉 (Hallucinations) # 原因：模型在训练数据中看到的大部分问答都是以自信的口吻陈述事实。因此，当遇到它不知道答案的问题时，它倾向于模仿这种“自信回答”的风格，而不是承认“我不知道”。它会根据统计上最可能的方式“编造”一个听起来合理的答案。\n缓解措施：\n数据干预：通过程序化地探查模型的知识边界，在 SFT 数据集中主动添加“我不知道”或“我无法找到相关信息”的回答样本。这教会了模型在不确定时可以拒绝回答。 工具使用 (Tool Use)：赋予模型使用外部工具的能力，如网络搜索或代码解释器。当模型判断自身知识不足时，它可以生成一个特殊的指令来调用工具，将获取到的信息（如搜索结果或代码运行输出）插入到其上下文窗口中。 知识存储：参数 vs. 上下文窗口 # 理解 LLM 如何使用知识至关重要：\n参数 (Parameters)：存储在模型权重中的知识，类似于人类的长期记忆或模糊的印象。它是模型在预训练阶段从海量数据中学习到的压缩知识。回忆这些知识可能不完全精确。 上下文窗口 (Context Window)：模型在处理当前任务时接收到的所有词元序列。这相当于模型的工作记忆或短期记忆。位于上下文窗口内的信息对模型是直接可见的，可以被精确地引用和处理。 实践启示：为了获得更高质量的回答，与其依赖模型回忆其参数中的知识，不如将相关信息直接提供在提示（Prompt）中，放入其上下文窗口。例如，要求模型总结一本书的章节时，最好将该章节的全文附在提示中。 计算能力与“思考”过程 # “模型需要词元来思考”：模型处理每个词元时所能执行的计算量是有限的。它无法在生成单个词元的一瞬间完成复杂的、多步骤的推理。\n连锁思考 (Chain of Thought)：因此，为了解决复杂问题，模型必须将推理过程分解成多个中间步骤，并将这些步骤作为词元序列逐步生成。每一步的输出都成为下一步的上下文，从而将复杂的计算分布在多个词元的生成过程中。\n对用户的启示：\n要求模型“一步一步地思考”或“展示你的推理过程”可以显著提高其在复杂问题上的表现。 直接要求模型给出最终答案（尤其是在数学问题中），相当于强迫它在生成一个词元时完成所有计算，这很容易导致错误。 “瑞士奶酪”能力模型：LLM 的能力分布极不均匀。\n强项：知识密集型任务、模式识别、语言转换。 弱项：精确的字符操作（如拼写、颠倒字符串）、精确计数、空间推理和某些看似简单的常识问题。这些弱点通常与其基于词元而非字符的运作方式有关。例如，模型“看”不到单词中的字母，而是看到代表单词或词根的词元。\n-\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n第三阶段：强化学习——超越模仿，学会思考 # 强化学习（RL）是训练 LLM 的前沿阶段，旨在让模型超越对人类专家的简单模仿，通过实践和探索发现更优的问题解决方法。这类似于学生通过做练习题来掌握知识，而不仅仅是背诵课本上的例题。\n1. 可验证领域的强化学习 (如数学与编程) # 在这些领域，答案有明确的对错标准，为自动化训练提供了可能。\n过程：\n探索 (Exploration)：针对一个问题（Prompt），让模型生成成百上千个不同的候选解决方案。 验证 (Verification)：用一个自动化的程序（如检查最终答案是否正确，或运行代码看是否通过测试用例）来评估每个解决方案的优劣。 学习 (Learning)：对那些导向正确答案的、高质量的解决方案（即成功的“思维路径”）进行训练，强化模型在未来采用类似路径的倾向。 涌现能力：学会思考：通过这个过程，模型会自发地学习到对解决问题有益的认知策略。它会发现，在输出最终答案前，进行自我反思（“等等，让我再检查一遍”）、多角度验证、设定变量、回溯修正等行为，能够显著提高正确率。这些复杂的“内心独白”式的推理过程是人类专家在提供标准答案时不会写出来的，只能通过 RL 的试错过程被模型自己发现。\n与 AlphaGo 的类比：这与 AlphaGo 的学习过程类似。AlphaGo 通过自我对弈的强化学习，发现了超越人类顶级棋手理解的下法（如著名的“第 37 手”）。同样，LLM 领域的 RL 也有潜力让模型发现超越人类思维定式的、更高效的推理路径。\n2. 不可验证领域的强化学习 (如创意写作) # 在这些领域，答案没有客观的对错，只有主观的好坏。\n挑战：无法自动化地为模型的输出（如一首诗或一个笑话）打分。\n解决方案：基于人类反馈的强化学习 (RLHF)：\n训练奖励模型：首先，让人类标注员对模型生成的多个回答进行排序（例如，哪个笑话最好笑）。这个任务比直接打分或写出“完美答案”要容易得多。 模拟人类偏好：然后，用这些人类排序数据来训练一个独立的神经网络，即奖励模型 (Reward Model)。这个模型学会了预测对于给定的回答，人类会给出什么样的评价分数。它成为了“人类偏好”的模拟器。 进行强化学习：最后，让主 LLM 在与这个奖励模型的交互中进行强化学习。主 LLM 的目标是生成能够从奖励模型那里获得高分的回答。 RLHF 的优势与局限：\n优势：使得在主观领域应用 RL 成为可能，并且通常能提升模型的表现，因为人类进行“评价”比进行“创作”更容易，数据质量更高。 局限：奖励模型只是对人类偏好的一个有损模拟，它自身存在漏洞。如果强化学习进行得太久，主 LLM 会学会如何“欺骗”奖励模型，生成一些毫无意义但却能获得高分的内容（即对抗性样本）。因此，RLHF 更像是一种有限的“微调”，而不是一种能够无限提升模型能力的“魔法”。\n-\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n总结与未来展望 # 1. 未来能力展望 # 多模态 (Multimodality)：模型将原生支持文本、图像、音频的混合输入和输出，实现更自然的交互。 智能体 (Agents)：模型将能够执行更长期的、多步骤的任务，从简单的问答工具演变为能够自主规划和执行工作的数字助理。 无处不在的集成：AI 能力将更深入地融入操作系统和各种应用中，变得更加“隐形”和无缝。 测试时训练 (Test-Time Training)：模型可能发展出在推理过程中持续学习和适应新信息的能力，打破当前训练和推理阶段的严格分离。 2. 如何获取信息与使用模型 # 保持信息同步：\nLMSYS Chatbot Arena：一个通过匿名用户投票对主流模型进行排名的排行榜。 AI News Newsletter：一份内容详尽的 AI 领域新闻通讯。 X (Twitter)：AI 领域的许多最新进展和讨论都发生在此平台。 在哪里使用模型：\n专有模型：通过其提供商的官方网站访问，如 OpenAI 的 ChatGPT、Google 的 Gemini 等。 开源权重模型：可通过第三方推理服务提供商（如 Together.AI）访问，或在本地设备上运行（需要 LM Studio 等工具）。 基础模型：较少见，但一些平台（如 Hyperbolic）提供与基础模型直接交互的服务。 3. 最终结论：我们究竟在与什么对话？ # 当用户与一个大型语言模型交互时，其本质是一个词元自动补全系统。\n对于SFT 模型（如免费版的 ChatGPT-4o），用户得到的是一个对人类标注员行为的统计模拟。其回答的风格、内容和倾向性，都源于其在包含数百万次人类示范的对话数据集上的训练。 对于强化学习模型（或称为“思考模型”，如付费版的 ChatGPT-4o-mini、DeepSeek-V2），用户得到的则更进一步。它不仅模仿人类，还包含了在解决问题的大量实践中自发涌现的认知策略。这种能力是其通过试错发现的，而非直接从人类那里复制而来。 这些模型是极其强大的工具，能够极大地提高生产力。然而，必须清醒地认识到它们的局限性：它们会产生幻觉，能力存在“盲点”，并且缺乏真正的理解。最佳实践是将其用作一个合作的伙伴——用于激发灵感、起草初稿、分析数据和自动化繁琐任务，但用户必须始终扮演最终的监督者和决策者，对结果进行批判性地审查和验证。\n参考 # 视频地址：Deep Dive into LLMs like ChatGPT from NotebookLLM ​ https://www.youtube.com/watch?v=7xTGNNLPyMI 原文 Andrej Karpathy\n1xx. 深入浅出大模型：预训练、监督微调、强化学习、RLHF\n1xx. Andrej Karpathy：深度解析LLM —— 从预训练到强化学习\n"},{"id":38,"href":"/www6vAlgo/docs/Agent/Deep-Research/Search/websailer/","title":"WebSailor","section":"Search","content":" 论文 # 《WebSailor: Navigating Super-human Reasoning for Web Agent》阿里巴巴集团通义实验室\n主要介绍了一种名为WebSailor的新型网页智能体（Web Agent），其在复杂信息寻求任务中展现了超越人类的推理能力。 以下是对文档的深度阅读总结，涵盖其核心问题、方法、实验及贡献。\n一、研究背景与问题定义 # 1.1 信息寻求的挑战 # 互联网时代的信息爆炸超越了人类认知极限（有限记忆、脆弱注意力、无法并行探索）。 专有智能体系统（如OpenAI的Deep Research）已展现出超越人类的性能，但开源智能体仍存在巨大性能差距。 1.2 任务分级 # 论文将信息寻求任务分为三个级别：\nLevel 1：低不确定性任务（如单次搜索即可解决）。 Level 2：高初始不确定性但解决路径清晰（如标准多跳问答）。 Level 3（本文焦点）：高不确定性且解决路径复杂、无预定义路径（如BrowseComp基准中的任务）。 1.3 性能差距根源 # 现有训练范式集中于Level 1–2任务，缺乏对Level 3复杂推理模式的暴露，导致模型无法发展出多步推理能力。\n二、核心方法：WebSailor框架 # 2.1 训练数据合成（SailorFog-QA） # a) 构建复杂知识图 # 通过随机游走从真实网站中提取互联的知识结构，生成具有涌现式非线性结构的图。 b) 生成高不确定性问题 # 采样多样化拓扑的子图（包含新颖的实体与关系组合）。 信息模糊化处理（如将精确日期转为“2010年代初”，名称部分掩码等），强制智能体进行推理而非简单查找。 c) 优势 # 数据基于真实互联网，贴合实际挑战。 子图拓扑多样性自然产生需复杂推理模式（多步演绎、组合与比较分析）的问题。 高度可扩展（子图数量随图规模非线性增长）。 2.2 推理轨迹重建 # a) 问题 # 开源大型推理模型（LRM，如QwQ、DeepSeek-R1）能生成正确轨迹，但其原生推理输出冗长、风格化，直接用于微调会限制智能体的探索策略泛化能力，且长轨迹易超出上下文窗口限制。\nb) 解决方案 # 用LRM生成完整动作-观察序列（丢弃原生冗长思考）。 用另一指令遵循模型（如Qwen-2.5-72B）为每一步动作重建简洁、目标导向的思考（“短链思维”风格），形成高质量监督信号。 2.3 训练流程优化 # a) 冷启动（RFT） # 必要性：RL奖励稀疏（初始近零反馈），且蒸馏依赖低（仅需2k+高质量样本）。 过滤：保留正确轨迹、长度＜32k token、工具调用＞5次（确保复杂性）。 训练目标：增强决策能力（掩码环境观察的损失计算）。 b) 强化学习（DUPO算法） # 挑战：多轮推理与工具使用导致训练缓慢。 创新：复制采样策略优化（训练前过滤全正确样本，训练中复制同一批次内标准差非零的样本），提速2–3倍。 奖励设计：结合格式验证（0.1权重）和答案验证（0.9权重），避免奖励黑客。 三、实验结果 # 3.1 基准测试 # BrowseComp-en/zh：最具挑战性的网页浏览基准，需复杂策略。 GAIA：需多模态与工具使用（仅用文本子集）。 Xbench-DeepSearch：动态深度搜索基准。 3.2 性能对比 # WebSailor（3B/7B/32B/72B）在所有开源模型与智能体方法中领先，且超越部分结合浏览能力的专有LRM（如Grok-3、Doubao）。 WebSailor-72B在BrowseComp-zh上与Doubao持平，虽仍落后于DeepResearch（SOTA），但显著缩小了开源与专有系统的差距。 向下兼容性：在简单任务（如SimpleQA）上也表现优异。 3.3 关键分析 # 数据复杂性：SailorFog-QA的工具调用分布与BrowseComp-en高度相似（长尾、多＞5次调用），而WebDancer数据集中＞50%仅需2次调用。 RL有效性：RL训练显著提升Pass@1性能（尤其BrowseComp），增强样本效率与稳定性。 冷启动必要性：无冷启动的RL模型工具调用数更低，无法掌握长视距推理，性能差距大。 四、局限与未来工作 # 上下文长度限制（32k token）可能制约更复杂问题的解决。 过度思考倾向：对简单问题也进行多步工具调用（但常为交叉验证，非无意义探索）。 训练效率：同步RL框架低效（仅50步），未来将转向异步训练。 五、结论 # WebSailor通过合成高不确定性数据、重建简洁推理轨迹、以及RFT冷启动与DUPO算法，实现了开源智能体在复杂信息寻求任务上的突破性进展，证明了开源模型可达到接近专有系统的性能。未来将继续探索更复杂任务与高效RL训练，以追求更广泛的“超人类”性能。\n附录与案例 # 文档还提供了：\n工具细节（search、visit）； QA构建流程； 训练超参数； 完整轨迹案例（如BrowseComp-en中关于Joey Hess的查询），展示多步推理与工具调用的实际应用。 总结 # 该研究在数据合成、推理重建和训练优化方面均有显著创新，为开源社区提供了可复现的高性能网页智能体方案，推动了对“超人类推理”能力的探索。\n参考 # 本文由元宝生成\nAgent智能体 | 深入解读阿里开源Web Agent新王者：WebSailor\n"},{"id":39,"href":"/www6vAlgo/docs/RL/core/PPO-family/RewardModel/","title":"(原理|实现)PPO-RewardModel","section":"PPO family","content":"\nPPO-RewardModel # (原理|实现)PPO-RewardModel\n"},{"id":40,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningLoss/","title":"(原理\u0026实战)交叉熵损失","section":"basic","content":"\n交叉熵损失 # (原理\u0026amp;实战)交叉熵损失\n"},{"id":41,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningNorm/","title":"规范化 Norm","section":"网络优化","content":"\nNorm 作用[1] # dnn 的标准组件，稳定和加速训练过程\nBatch Norm[1] # reduce cross batch size mini-batch dimension 一般用于图像，不涉及到padding的问题；\nLayer Norm[1] # reduce cross hidden dim reduce across the feature dimension. 一般用于序列，一个 batch size 内存在 padding；\nRMSNorm: 对 LN 的一种变体，llama 💡 https://spaces.ac.cn/archives/9009 Pre LN: llama Post LN: attention is all you need llama在工程上使用Pre LN\n[pytorch] BN、LN、RMSNorm 及 pre LN vs. post LN 对比，标准化 v *** ​\tnormalization.ipynb\n​\t[pytorch] BN、LN、RMSNorm 及 pre LN vs. post LN 对比，标准化 1xx. Batch Normalization, Layer Normalization and Root Mean Square Layer Normalization: A Comprehensive Guide with Python Implementations\ntodo\n7.5 逐层规范化 百度邱 有代码\nhttps://aistudio.baidu.com/education/lessonvideo/3048901\n"},{"id":42,"href":"/www6vAlgo/docs/DeepLearning/Transformer/ModelGQA/","title":"(原理)GQA","section":"Transformer","content":"\n论文 # GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\nMHA vs. MQA vs. GQA [1] # MHA # 首先是原始的 MHA(Multi-Head Attention)，QKV 三部分有相同数量的头，且一一对应。每次做 Attention，head1 的 QKV 就做好自己运算就可以，输出时各个头加起来就行。\nMQA # 而 MQA 则是，让 Q 仍然保持原来的头数，但 K 和 V 只有一个头，相当于所有的 Q 头共享一组 K 和 V 头，所以叫做 Multi-Query 了。实现改变了会不会影响效果呢？确实会影响但相对它能带来的收益，性能的些微降低是可以接受的。\n能带来多大的收益呢，实验发现一般能提高 30%-40% 的吞吐。\n收益主要就是由降低了 KV cache 带来的。实际上 MQA 运算量和 MHA 是差不多的，可理解为读取一组 KV 头之后，给所有 Q 头用，但因为之前提到的内存和计算的不对称，所以是有利的。\nGQA # 而 GQA 呢，是 MHA 和 MQA 的折衷方案，既不想损失性能太多，又想获得 MQA 带来的推理加速好处。具体思想是，不是所有 Q 头共享一组 KV，而是分组一定头数 Q 共享一组 KV，比如上面图片就是两组 Q 共享一组 KV。\nMQA 和 GQA 形式在推理加速方面，主要是通过两方面来完成：\n降低了从内存中读取的数据量，所以也就减少了计算单元等待时间，提高了计算利用率； KV cache 变小了 head_num 倍，也就是显存中需要保存的 tensor 变小了，空出来空间就可以加大 batch size，从而又能提高利用率。 如果要用 MQA 和 GQA，可以是从头训练的时候就加上，也可以像 GQA 论文里面一样，用已有的开源模型，挑一些头取个 mean 用来初始化 MQA 或 GQA 继续训练一段时间。\nGQA \u0026amp; MQA [2] # 图 4.1 Multi-head attention 拥有 H 个查询、键和值头。Multi-query attention 在所有 查询头之间共享单个键和值头。Grouped-query attention 则在每个查询头组之间共享单 个键和值头，从而在多头和多查询注意力之间进行插值。\nFig. 4.1 Multi-head attention has H query, key, and value heads. Multi-query attention shares single key and value heads across all query heads. Grouped-query attention instead shares single key and value heads for each group of query heads, interpolating between multi-head and multi-query attention.\n参考 # 为什么现在大家都在用 MQA 和 GQA？ ***\nLLM学习系列1：大模型架构要点总结 主流大语言模型的技术原理细节 *** 腾讯 [架构] + 训练 + 微调\n1xx. 理解Attention:从起源到MHA,MQA和GQA *** 1xx. 深度解析Group Query Attention(GQA)为什么能给LLM decoder带来极大推理加速 1xx. 深度学习中的注意力机制：MHA、MQA和GQA\n1xx. 【研1基本功 （真的很简单）Group Query-Attention】大模型训练必备方法——bonus(位置编码讲解) v *** Nomolization[post, pre, sandwich] + Position Encoding[RoPE] + GQA代码 1xx. 一文通透各种注意力：从多头注意力MHA到分组查询注意力GQA、多查询注意力MQA 删除\n手写大模型组件之Group Query Attention，从 MHA，MQA 到 GQA\n"},{"id":43,"href":"/www6vAlgo/docs/LLM/Dense/LlamaFamily/","title":"LLaMA 家族","section":"Dense","content":" LLaMA 家族[1] # 项目 描述 数据集 LLaMa 基座模型 公开可用的数据集(1T token) Stanford Alpaca 结合英文语料通过Self Instruct方式微调LLaMA 7B Self Instruct from davinci-003 API(52K) Vicuna-13B 通过ShareGPT.com的7万条对话数据微调LLaMA(Alpaca基础之上, 多轮对话和长序列, full fine-tune) 用户共享对话(70K sample) BELLE 结合中文语料通过Self Instruct方式微调BLOOMZ-7B或LLaMA Chinese-LLaMA/Chinese-Alpaca 通过中文数据预训练/指令微调LLaMA 姜子牙系列模型Ziya-LLaMA-13B-v1 基于LLaMA-13B的中英文模型 ChatLLaMA(英文版) LLaMA的RLHF版 ColossalChat 通过self-instruct技术指令微调LLaMA且加上RLHF {% asset_img \u0026rsquo;llama2-famaly.jpg\u0026rsquo; %}\n参考 # 家族 # LLaMA的解读与其微调：Alpaca-LoRA/Vicuna/BELLE/中文LLaMA/姜子牙/LLaMA 2 *** 1xx. 我想学大模型，应该从哪个模型开始？LLaMA生态家谱整理和分析 1xx. NLP（九）：LLaMA, Alpaca, ColossalChat 系列模型研究\n1xx. \u0026laquo;千帆增强版 Llama 2-提升大模型对话指令遵循能力\u0026raquo; v\n1xx. 近期大模型动态：LLaMA-2-7B-32K的训练数据组织情况及面向儿童心理健康领域的微调模型推介 llama-2-7b-32k - LLaMA-2的上下文长度为4Ktoken。要将其扩展到32K上下文，该工作分成了三个部分：建模、数据和系统优化。\n实战 # 1xx. 从0到1复现斯坦福羊驼（Stanford Alpaca 7B） GPUs: 8 卡 A800 80GB GPUs\n汉化 # 1xx. 掘力计划 23 期-Linly-Chinese-LLaMA2 中文开源大模型方案分享 v\n"},{"id":44,"href":"/www6vAlgo/docs/LLM/challenge/Hallucination/","title":"(原理)幻觉问题","section":"Challenge","content":" 幻觉[3] # Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge. 大型语言模型中的幻觉通常是指模型生成不忠实、捏造、不一致或无意义的内容。作为一个术语，幻觉在某种程度上被推广到模型犯错的情况。在这里，我想将幻觉问题缩小到模型输出是捏造的， 而不是基于所提供的上下文或世界知识的情况。\nThere are two types of hallucination: 幻觉有两种类型：\nIn-context hallucination: The model output should be consistent with the source content in context. 上下文幻觉：模型输出应与上下文中的源内容一致。 Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so. 外在幻觉：模型输出应以训练前数据集为基础。但是，考虑到预训练数据集的大小，检索和识别每代冲突的成本太高。如果我们将预训练数据语料库视为世界知识的代理，我们基本上会尝试确保模型输出是真实的，并且可以通过外部世界知识进行验证。同样重要的是，当模型不知道某个事实时，它应该这么说。 Anti-Hallucination Methods[3] # RAG → Edits and Attribution # Self-RAG (“Self-reflective retrieval-augmented generation”; Asai et al. 2024) trains a LM end-to-end to learn to reflect on its own generation by outputting both task output and intermittent special reflection tokens. They created a supervision dataset for a critic model and a generator model by prompting GPT-4 and then distilled that into an in-house model to reduce inference cost. Self-RAG（“自反射检索增强一代”;Asai 等人，2024 年）通过输出任务输出和间歇性特殊反射令牌 ，端到端训练 LM 以学习反射自己的生成。他们通过提示 GPT-4 为 critic 模型和生成器模型创建了一个监督数据集，然后将其提炼成内部模型以降低推理成本。\nChain of Actions # Without grounding by external retrieved knowledge, we can design a process for using the model itself to do verification and revision to reduce hallucination. 在没有外部检索知识的基础的情况下，我们可以设计一个流程，使用模型本身进行验证和修改，以减少幻觉。\nDhuliawala et al. (2023) proposed a method named Chain-of-Verification (CoVe) based on a chain of actions to plan and execute verification. [10] Dhuliawala 等人（2023 年） 提出了一种名为验证链 （CoVe） 的方法，该方法基于一系列行动来计划和执行验证。\nRECITE (“Recitation-augmented generation”; Sun et al. 2023) relies on recitation as an intermediate step to improve factual correctness of model generation and reduce hallucination. The motivation is to utilize Transformer memory as an information retrieval mechanism. Within RECITE’s recite-and-answer scheme, the LLM is asked to first recite relevant information and then generate the output. Precisely, we can use few-shot in-context prompting to teach the model to generate recitation and then generate answers conditioned on recitation. Further it can be combined with self-consistency ensemble consuming multiple samples and extended to support multi-hop QA. RECITE （“朗诵增强生成”;Sun 等人，2023 年）依靠背诵作为中间步骤来提高模型生成的事实正确性并减少幻觉。动机是利用 Transformer 内存作为信息检索机制。在 RECITE 的背诵和回答方案中，要求 LLM 首先背诵相关信息，然后生成输出。准确地说，我们可以使用小镜头上下文提示来教模型生成背诵，然后生成以背诵为条件的答案。此外，它可以与使用多个样本的自一致性集成相结合，并扩展以支持多跳 QA。\ntodo\nSurvey # 论文 # 论文地址 A Survey of Hallucination in Large Foundation Models Paper 1xx. 大模型前沿热点最新综述：大模型微调遗忘、Agent智能体、幻觉及RAG检索增强模型推介 大模型微调遗忘 幻觉\n论文 # 论文地址 Siren\u0026rsquo;s Song in the AI Ocean: A Survey on Hallucination in Large Language Models Paper 1xx. 人工智能海洋中的塞壬之歌：大型语言模型LLM中的幻觉研究综述（一） 1xx. 大型语言模型的幻觉研究｜减轻及避免大模型LLM幻觉（二） 1xx. 值得一读的大模型生成幻觉研究综述：大模型幻觉的起因、评估以及减轻策略总结 幻觉 vs 事实性[1] # 幻觉主要是指LLM生成毫无根据或毫无根据的内容，幻觉可以理解为模型倾向于\u0026quot;生成与某些来源相关的无意义或不真实的内容\u0026quot;。这与事实性问题不同，后者强调模型学习、获取和利用事实性知识的能力。\n举例说明两者的区别：\n如果一个LLM在被要求创作\u0026quot;一个关于兔子和狼交朋友的童话故事\u0026quot;时，创作出了一个关于\u0026quot;兔子和狗交朋友\u0026quot;的故事，那么它就表现出了幻觉。不过，这并不一定是事实性错误。 如果生成的内容包含准确的信息，但与提示的具体内容有出入，那就是幻觉，而不是事实性问题。 例如，如果LLM的输出包含了比提示指定更多的细节或不同的元素，但事实仍然正确，这就是幻觉。\n相反，如果LLM避免给出直接答案，而是说\u0026quot;我不知道\u0026quot;，或者给出了一个准确的答案，但遗漏了一些正确的细节，那么这就是事实性问题，而不是幻觉。\n此外，值得注意的是，幻觉有时会产生一些内容，虽然与原始输入内容有偏差，但在事实方面仍然是准确的。\n解决方案[2] # Prompt 工程 *\nFew-shot 外部知识 *\nRAG 后处理 *\n实事检查 * 人工检查 * 提升数据质量\nPretraining的数据质量 SFT的数据质量 模型能力提升 *\n微调 参考 # 再看大模型事实性的界定、错误的起因、评估及前沿缓解方案：Survey on Factuality in LLMS\n降低大模型幻觉的5种方案 v\n减少大模型幻觉，你必须要掌握的 6 个方法！ v\nExtrinsic Hallucinations in LLMs ***\n【译】LLM中的外部幻觉\nWork # 再看大模型幻觉问题如何缓解 ：Chain-of-Verification-一种基于链式验证思想的自我修正工作解读 1xx. 也看缓解大模型幻觉的多阶段RAG框架：加入混合检索、过程理由生成与验证的方案 survey # 1xx. 大模型的幻觉问题调研: LLM Hallucination Survey\n1xx. 网络安全领域微调模型SecGPT：兼看大模型幻觉的度量方式、评估benchmark及RAG增强不同方式 大模型幻觉综述\n1xx. LLM之幻觉（一）：大语言模型幻觉解决方案综述\n"},{"id":45,"href":"/www6vAlgo/docs/LLM/Dense/Llama3-1/","title":"Llama3.1","section":"Dense","content":" Llama3.1 # Llama3.1\n"},{"id":46,"href":"/www6vAlgo/docs/RL/core/compare/","title":"(原理) 综述","section":"Core","content":" 算法 # 算法 核心思想 是否使用评论家 主要创新点 关键优势 目标问题 PPO 将策略更新限制在信任区域内以稳定学习。 是 截断代理目标函数。 稳定、鲁棒。 通用RL对齐(RLHF)。 DPO 通过分类损失直接在偏好对上优化策略。 否 将奖励重参数化为最优策略的函数。 简洁、稳定、无RM/RL循环。 通用RL对齐(RLHF)。 GRPO 使用一组样本的奖励统计量来估计优势。 否 基于组的优势估计。 内存/计算效率高。 资源密集型的推理任务。 DAPO 系统性地应用一套技术来解决大规模RL问题。 否 Clip-Higher、动态采样等的组合。 解决特定的训练病理问题。 规模化训练的可扩展性和稳定性。 Dr. GRPO 从GRPO目标中移除已识别的长度和难度偏差。 否 无偏的损失函数形式。 token效率更高，偏差更小。 GRPO中的长度/难度偏差。 GSPO 在序列级别执行重要性采样和截断。 否 序列级重要性采样。 极高的稳定性，尤其对MoE模型。 token级更新的不稳定性。 GMPO 使用奖励的几何平均值以对异常值保持鲁棒。 否 目标函数中使用几何平均。 对异常奖励值的鲁棒性。 奖励异常值导致的不稳定。 GFPO 在更新前根据行为指标过滤采样的轨迹。 否 对轨迹进行拒绝采样。 生成简洁、高效的回答。 回答长度膨胀问题。 LitePPO 组合归一化和损失聚合的最佳实践。 否 对现有技术的有原则配置。 以最小的复杂性实现高性能。 RL流程中的过度工程化。 演化脉络 # PPO：token-level，价值函数依赖，clip在token上。 GRPO：引入 group 原则，reward group内归一，无需value，但clip/S依然是token-level，variance大。 Dr.GRPO：修正GRPO的长度和方差归一偏置，不再token归一。 DAPO：进一步吸收众多工程技巧（Clip-Higher、Dynamic Sampling等）来缓解大模型RL的瓶颈，token-level范式不变。 GSPO：范式跃迁，off-policy与clip全部sequence-level，variance低，性能和算法纯粹性最佳，最新Qwen3 RL实践基础。\n核心公式对照表 # 参考 # PPO、DPO、GRPO及其变体（Dr. GRPO、DAPO、GSPO、GMPO、GFPO、LitePPO）策略优化算法综述 PO 系列工作解析 (一)：从PPO到GRPO/DAPO/Dr.GRPO再到GSPO的演化\n"},{"id":47,"href":"/www6vAlgo/docs/RL/core/DPO/","title":"(原理|实现)DPO","section":"Core","content":"\nDPO # (原理|实现)DPO\n"},{"id":48,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningGradOpt/","title":"(原理)梯度优化","section":"网络优化","content":"\n梯度优化 # Gradient accumulation # Gradient checkpointing [10] # 显存占用优化算法\nmemory usage 与 computation time 之间的 tradeoff ； gradient checkpointing\nIn deep neural networks, backpropagation requires storing intermediate activations for computing gradients during the backward pass.\n但是当层数变多时，存储所有的中间层的激活值（intermediate activations）非常地占用显存；\ngradient checkpointing\n选择性地重新计算（recompute）一部分的 intermediate activations 在反向传播过程中来缓解显存的压力；\nGradient Clipping (梯度裁剪) # 目的[21] # 梯度爆炸问题的常见应对方式为“梯度裁剪”，也就是通过“clip”方式来防止迭代中梯度值过大。\n两种常见形式[20] # 梯度范数裁剪（Gradient Norm Clipping）: 这种方法涉及计算所有参数梯度的范数（例如L2范数），如果这个范数超过了设定的阈值，就将梯度缩放到这个阈值以内。在PyTorch中，这可以通过 torch.nn.utils.clip_grad_norm_ 函数实现。 梯度值裁剪（Gradient Value Clipping）: 这种方法对每个参数的梯度值进行独立裁剪，确保它们不会超过一个设定的最大值或最小值。在PyTorch中，这可以通过 torch.nn.utils.clip_grad_value_ 函数实现。 参考 # overview # Performance and Scalability: How To Fit a Bigger Model and Train It Faster ***\ngradient accumulation # 1xx. [LLMs 实践] 11 gradient accumulation 显存优化 trick v\n​\tgradient_accumulation.ipynb\n​\t[ LLMs 实践] 11 gradient accumulation 显存优化 trick 1xx. Pytorch入门（7）—— 梯度累加（Gradient Accumulation）\n1xx. 聊聊梯度累加(Gradient Accumulation)\n1xx. What is Gradient Accumulation in Deep Learning?\n1xx. Performing gradient accumulation with Accelerate\n​\t使用Accelerate进行梯度累积\ngradient checkpointing # [LLMs 实践] 13 gradient checkpointing 显存优化 trick v ​\tgradient_checkpointing.ipynb\n​\t[LLMs 实践] 13 gradient checkpointing 显存优化 trick ​\tFitting larger networks into memory. *** 看动图\n​\tBackprop and systolic arrays.\nGradient Clipping # 梯度裁剪（Gradient Clipping） ​\thttps://github.com/pytorch/pytorch/blob/main/torch/nn/utils/clip_grad.py\n深度炼丹之梯度裁剪 1xx. 【深度学习】第6.2节 梯度裁剪\n1xx. 【Pytorch】梯度裁剪——torch.nn.utils.clip_grad_norm_的原理及计算过程\n1xx. PyTorch使用Tricks：梯度裁剪-防止梯度爆炸或梯度消失 ！！\n"},{"id":49,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningOverfitting/","title":"(原理)过拟合","section":"basic","content":"\n(原理)过拟合 # (原理)过拟合\n"},{"id":50,"href":"/www6vAlgo/docs/DeepLearning/Transformer/TransformerCode/","title":"(实战)Transformer","section":"Transformer","content":"\n参考 # 1xx. transformer.ipynb git Transformer代码实现\n1xx. Transformer transformer.py git\n1xx. [译] Transformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019） V, github Transformers from scratch\n1xx. 从零实现Transformer的简易版与强大版：从300多行到3000多行\n1xx. Transformer源码详解（Pytorch版本）\n"},{"id":51,"href":"/www6vAlgo/docs/LLM/challenge/ImpossibleTriangle/","title":"(原理)不可能三角","section":"Challenge","content":" 不可能三角[1] # 不可能三角 # 预训练模型之所以是划时代的进展，是它具备了中等尺寸（一张卡即可精调）和全任务SOTA的精调效果 而最近两年预训练模型都在往大尺寸发展，也就是具备了少样本效果，但他们的少样本效果依旧比不过中等模型的精调 弥补方法 # 优化size 对于减少模型尺寸，一条典型的故事线就是蒸馏。但其中仍存在两个问题：一是学生模型很难达到原始模型的效果，二是原始的大尺寸模型的推理效率太低 优化few-shot 对于提升少样本表现，数据增强是一个好办法，比如用无监督数据做自监督训练、或者基于其他模型生成一些伪样本，但这类方法依旧受限于现有标注样本的多样性，泛化性能提升有限 fine-tuning 对于提升精调表现和效率（其实也偏少样本），最近一个比较火的故事是prompt，但这种方式对prompt的设计非常敏感，同时效果也很难超过目前的有监督SOTA 其他 不可能三角 # 分布式系统 # CAP理论 C 一致性 A 可用性 P 分区 分布式存储 # RUM猜想 Read-overhead Update-overhead Memory-overhead 范式 # pretrain, finetune 范式[3] # 第三阶段范式\npretrain, prompt, predict 范式[3] # 第四阶段范式\n总结 # 根据不可能三角形， pretrain, finetune 范式[3] 向pretrain, prompt, predict 范式[3]的迁移是受大模型大小的影响\n参考 # 不可能三角 # 预训练模型的下一步？突破Impossible Triangle Impossible Triangle: What’s Next for Pre-trained Language Models? 微软朱晨光：预训练模型下一步怎么走？突破PLM的「不可能三角」 Go to Page self "},{"id":52,"href":"/www6vAlgo/docs/RL/framework/veRLConfig/","title":"(原理\u0026实战)veRL Config","section":"Framework","content":"\nveRL Config # (原理\u0026amp;实战)veRL Config\n"},{"id":53,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Embedding/Embedding/","title":"(原理)Embedding","section":"Embedding","content":" example [1] # 降维: t-SNE K-Means 聚类 文本搜索 相似度搜索 Embedding 价值 [2] # 降维 将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。 捕捉语义信息 Embedding不仅仅是降维，更重要的是，它能够捕捉到数据的语义信息。 泛化能力 由于Embedding能够捕捉到数据的一些内在规律，因此对于这些未见过的数据，Embedding仍然能够给出合理的表示 应用 [2] # 语义表示和语义相似度 词语关系和类比推理 上下文理解 文本分类和情感分析 机器翻译和生成模型 天梯榜 # mteb/leaderboard\nexample[3] # m3e模型 bge模型 参考 # embedding git\n《AI 大模型应用开发实战营》 03-大模型开发基础：Embedding\n一文通透Text Embedding模型：从text2vec、openai-ada-002到m3e、bge\n1xx. 如何选取RAG中的embedding模型 v ***\nhuggingface embedding模型排行榜\nSentence Bert Demo Repo git\n1xx. 引入任务Instruction指令的句子向量化方案：Instructor的实现思路及训练数据集构造方案\nRepo git\n1xx. 也看利用大模型进行RAG文本嵌入训练数据生成：兼看面向NLP任务的开源指令微调数据集 《Improving Text Embeddings with Large Language Models》\n1xx. 如何提高LLMs的文本表征(Text Embedding)能力?\n《Improving Text Embeddings with Large Language Models》\n1xx. 文本转向量教程s2——认识文本转向量方法（sbert本质和推理加速） V\n"},{"id":54,"href":"/www6vAlgo/docs/LLM/Core/Eval/","title":"测评 *","section":"Core","content":" 基础指标[1] # 分类任务\nAccuracy Precision Recall F1-score AUC-ROC 曲线 生成任务\nBLEU ROUGE METEOR 人工评估 回归任务\nMSE MAE R2 PPL 困惑度\n测评集 # MMLU C-EVAL Framework # OpenCompass 参考 # AI大模型面试题：5.模型微调怎么评估效果 1xx. 一些讨论：三张关于大模型微调方案的脑图及几点llama2等行业落地的问题思考 1xx. https://github.com/CLUEbenchmark/SuperCLUE-Llama2-Chinese\n1xx. 如何让自己的大模型榜单评分更高：也谈榜单评测评分的一些常见方案和典型案例 CEval # 1xx. 大模型落地的一些前沿观点：兼看知识图谱增强大模型问答的几个方案及CEVAL榜单评测启发 二、CEVAL榜单评测中能够得到一些启示\n1. C-Eval 数据集评测简明教程\n1xx. 大模型B端落地“牛刀杀鸡”的奇怪感觉：兼看CEVAl通用评测到金融、医疗两大垂域评测的转变 CEVAl\nFramework # https://opencompass.org.cn/home\n"},{"id":55,"href":"/www6vAlgo/docs/DeepLearning/Transformer/TrainTokenizer/","title":"Tokenizer","section":"Transformer","content":"\ntokenizer 分词 # 单词分词法 单字分词法 子词分词法 BPE [GPT系列], WordPiece 参考 # 1xx. 大模型词表扩充必备工具SentencePiece 1xx. NLP（二）：浅谈分词 1xx. https://www.bilibili.com/video/BV1vN411p7t2/ 1xx. 开源大模型如何更好地适应中文场景：LLAMA扩充词表、BLOOM裁剪词表基本原理与开源实现\n"},{"id":56,"href":"/www6vAlgo/docs/Agent/Deep-Research/Search/Kimi-Researcher/","title":"Kimi-Researcher","section":"Search","content":" 论文 # Kimi-Researcher - End-to-End RL Training for Emerging Agentic Capabilities\n参考 # 强化学习智能体新模板：深入解析Kimi‑Researcher\n1xx. Kimi-Researcher：端到端强化学习驱动的自主智能体\n1xx. up: 有个视频 "},{"id":57,"href":"/www6vAlgo/docs/RL/core/unified/","title":"unified paradigm","section":"Core","content":" RL unified paradigm # RL unified paradigm # DPO # PPO # GRPO # 参考 # DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n"},{"id":58,"href":"/www6vAlgo/docs/LLM/Reasoning/kimi/1.5/","title":"Kimi1.5","section":"kimi","content":" 📌 0. 背景 # Kimi-K1.5 与 DeepSeek-R1 几乎同步发布（2025-01-20），二者推理能力均达到 OpenAI o1 水平。 Kimi-K1.5 是多模态模型；DeepSeek-R1 仅文本。 相比 DeepSeek-R1，Kimi-K1.5 技术报告披露了更多可落地的算法细节，尤其在 RL 数据构建、评估、采样方面极具参考价值。 🧱 1. 整体架构 # 采用标准三阶段流程：\nPre-training → SFT → Reinforcement Learning（RL）\n（图源：木尧｜知乎）\n🔤 2. 预训练（Pre-training） # 分三阶段：\n阶段一：视觉-语言预训练 # 先训纯语言模型（LLM），再逐步加入多模态数据； Vision tower 独立训练，初期不更新 LLM 参数； 后期图文交织数据从 0% → 30%，逐步放开 LLM 更新。 阶段二：冷却（Cooling）阶段 # 用精选 + 合成数据（QA 对）巩固推理/知识能力； 合成方式：用专有模型生成 → 拒绝采样保质量。 阶段三：长上下文激活 # 目标：支持 131,072 token 上下文； 关键技术： 过采样 long-context 数据：40% 全注意力 + 60% partial attention； 渐进训练：4k → 32k → 128k； RoPE base 频率设为 1,000,000（更大上下文需更高频率）。 → 得到基础模型 kimi-k1.5-base。\n✍️ 3. SFT 训练 # 3.1 常规 SFT # 数据构建： 非推理任务：人工种子集 → 模型生成多回复 → 人工排序+优化； 推理任务（数学/代码）：规则+奖励模型验证 + 拒绝采样（更高效准确）。 数据分布（~1M）： 类型 数量 一般问答 500k 编码 200k 数学/科学 200k 创意写作 5k 长上下文任务 20k 图文任务（图表/OCR/视觉推理等） 1M 训练细节： Epoch 1：seq_len=32k, lr 2e-5 → 2e-6 Epoch 2：seq_len=128k, lr 1e-5 → 1e-6 packing 多样本训练（提升 GPU 利用率） 🔍 3.2 Long-CoT SFT（重点①） # 目标：让模型学会人类式深度思考： 规划 → 评估 → 反思 → 探索 数据构造：对高质量问题，用 Prompt Engineering 生成含完整思考链的长答案； 本质仍是 SFT，差异仅在 answer 长度与结构。 🎯 4. 强化学习（RL）——核心亮点 # 4.1 RL 数据集构建原则（三大关键） # 维度 具体做法 多样性 多领域数据 + 公司自建标签系统（核心资产未公开） 难度平衡 高温采样 10 次 → 计算通过率定难度；动态更新（每次用最新 checkpoint）；课程学习：由易→难 可精确评估 移除易猜题型（多选/判断/证明题）；移除易 hack prompt：不走 CoT 也能高概率答对 → 剔除 ✅ 核心原则：“答案必须可精确评估”——RL 的“宪法”。\n4.2 问题定义 # 将推理建模为搜索空间优化问题：\n固定答案题：规则判断 → reward ∈ {0, 1} 开放问答：用奖励模型打分 4.3 策略优化 # 无 Value Model：与 DeepSeek-R1 的 GRPO 一致 → 保留错误路径梯度，对提升推理能力有帮助； 长度惩罚：防过度思考，奖励随推理长度衰减： 4.4 采样策略 # 课程采样：按难度递进； 优先采样：按 1 − 成功率 比例采样 → 专攻短板问题。 🔥 4.5 Long2Short（重点②） # 实现 长链推理 → 短链推理 的高效迁移：\n方法 说明 权重融合 Long/Short checkpoint 直接加权平均（无需训练，可直接工程复用） 最短拒绝采样 生成多条 → 选最短且正确者 长短 DPO 构建 pair：短且正确（+） vs 长/错误（−） 长度惩罚 RL RL 阶段显式抑制冗长输出 4.6 其他亮点细节 # 4.6.1 代码 RL：自动生成测试用例 # 用 CYaRon + Kimi-k1.5 生成 → 用 10 个正确提交验证 → 通过率 ≥70% 为有效； 1000 题中 → 614 无特殊评测 → 323 题最终入训。 4.6.2 数学 RL：双奖励模型 # 类型 优势 Classic RM 输入：问题+标准答案+模型作答 → 输出标量 Chain-of-Thought RM 训练时看 CoT，效果更好（用 800k CoT+label 数据训练） 4.6.3 视觉 RL 数据三类 # 类别 作用 真实世界数据 科学题、地点猜测、图表分析 → 提升现实推理 合成视觉数据 程序生成图像 → 训练空间/几何推理 文本渲染数据 文档→图像（截图/照片）→ 提升文字密集图理解，保证跨模态一致性 4.6.4 RL 框架优化 # 训练/推理分离框架； Rollout 采样优化 → 提升数据利用率： 📊 5. 实验结论 # 5.1 主要结果 # 多项 benchmark 达 o1 水平，首个追平 o1 的多模态模型： Long2Short 后，Short 模型性能几乎无损： 5.2 自我进化：CoT 长度 ↔ 能力正相关 # 模型自发生成更长 CoT； CoT 越长，性能越好；模型越大，提升斜率越陡： 5.3 课程学习显著有效 # 固定难度训练（蓝线）快速饱和； 课程学习（橙线）持续提升： 5.4 负样本梯度有用 # 对比 ReFT（仅用正样本）→ 含负样本策略表现更优： ✅ 总结：Kimi-K1.5 的三大工程可复用亮点 # RL 数据精筛三原则（多样+动态难度+可评估）——可直接用于自建 RL pipeline； Long2Short 迁移技术（尤其权重融合）——低成本部署轻量高能模型； 拒绝采样 + 优先采样 + 课程学习 组合拳 —— 提升训练效率与收敛质量。 作者：chaofa｜全网同名\n原文链接：https://yuanchaofa.com/post/kimi-k1.5-paper-reading-notes.html\n如需导出为 PDF/PPT 或提取某部分细节（如代码测试生成流程），我可进一步整理。\n参考 # 以下是对 《深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的》 的结构化摘要，保留原文关键图片与核心要点，便于快速把握 Kimi-K1.5 的技术亮点与工程实践细节。\n【论文解读】Kimi-k1.5：无需复杂搜索，Long Context + RL就能实现复杂推理\n"},{"id":59,"href":"/www6vAlgo/docs/LLM/Survey/LLM-Compare/","title":"LLM架构演进","section":"Survey","content":" 效率与能力的平衡：LLM架构演进与关键设计权衡深度解析 # 1. 执行摘要 (Executive Summary) # 现代大语言模型（Large Language Model, LLM）的架构演进，本质上是各大研究机构在 计算效率 与 模型能力 的双重约束下所做出的一系列战略性“赌注”。本报告深度剖析了这一趋势，并指出两条主导性策略正浮出水面，以应对规模化扩展的困境：一是通过稀疏专家混合 (Mixture-of-Experts, MoE) 架构实现计算高效的容量扩展；二是通过多头潜在注意力 (Multi-Head Latent Attention, MLA) 等创新机制，实现内存高效的上下文处理。这些设计选择不仅揭示了技术路线的分歧，更反映了不同组织在性能、训练稳定性与推理成本之间的核心权衡。\n-\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n2. 核心架构对比矩阵 # 本节通过一个结构化的对比矩阵，系统性地梳理和评估视频中讨论的各个LLM架构的关键特征、创新点及其带来的具体影响，为读者提供一个清晰的横向比较视图。下表提炼了各个模型在架构设计上的核心取舍，揭示了行业在效率与能力权衡中的多样化探索。\n模型架构 核心创新与特点 设计权衡与影响 视频论据引用 DeepSeek v3 多头潜在注意力 (MLA)；拥有共享专家的MoE架构 MLA：通过引入一个压缩状态来存储键值缓存 (KV Cache)，以少量额外计算为代价，实现了内存占用的急剧下降，显著降低了长上下文推理的硬件门槛。\nMoE：以更高的训练复杂性换取巨大的模型容量和显著降低的推理激活参数量。 \u0026gt; DeepSeek的MLA与MHA相比，KV缓存参数量从86万降至3.4万，实现了超过20倍的压缩 [视频未提供时间戳]。\n\u0026gt; 模型总参数为6710亿，但推理时激活参数仅为370亿 [视频未提供时间戳]。 Olmo 2 残差连接内部的后置归一化 (Post-Norm) 布局；QK Norm 追求极致的训练稳定性。相比传统的前置归一化 (Pre-Norm)，该设计旨在减少训练过程中的梯度尖峰，实现更平滑、更可控的优化过程，降低训练失败风险。 \u0026gt; 视频中的图表显示，与Pre-Norm相比，Olmo 2采用的Post-Norm布局使得梯度L2范数曲线更平滑，显著减少了训练不稳定的尖峰 [视频未提供时间戳]。 Gemma 3 滑动窗口注意力；“Pre-Norm + Post-Norm”双重归一化 + QK Norm 这种混合局部/全局注意力的策略是一种务实的妥协，使模型能处理长上下文，同时避免了纯全局注意力带来的内存成本二次方增长，从而降低了部署的硬件要求。 \u0026gt; Gemma 3采用5:1的局部/全局注意力层比例，在长上下文场景下，其内存成本增长远低于纯全局注意力模型 [视频未提供时间戳]。 Mistral Small 3.1 浅层宽架构 相对于Gemma 3的深层窄架构，Mistral选择了更少但更宽的Transformer块。这种设计优先考虑 推理吞吐量，因为序贯计算的层数更少，从而降低了交互式应用的延迟。 \u0026gt; 视频作者推测，其更少的Transformer块（40 vs Gemma 3的62）可能是其推理速度更快的原因，这是一种在推理延迟和服务成本上的权衡 [视频未提供时间戳]。 Llama 4 (Maverick) 独特的MoE配置，激活极少数量的大型专家 采用一种独特的MoE配置，激活专家数极少（一个共享专家，一个常规专家），但专家本身规模巨大。此设计路径与DeepSeek的“多数小型专家”形成鲜明对比，对路由效率和专家特化程度提出了不同挑战。 \u0026gt; Llama 4 (400B) 每次仅激活两个专家（一个共享，一个常规），激活参数为700亿，但其每个专家的中间层维度（8100）远大于DeepSeek（2000）[视频未提供时间戳]。 Qwen 3 (Dense \u0026amp; Sparse) 稠密版：深层窄架构\n稀疏版：无共享专家 深层窄架构：与Llama 3相比，以更慢的推理速度换取了显著更低的内存占用，优化了部署成本。\n无共享专家：简化了MoE的训练设计，但可能牺牲了由共享专家学习通用知识所带来的整体学习效率。 \u0026gt; 作者的实现数据显示，Qwen 3（更深）内存占用约1.5GB，推理速度24 tokens/sec；而Llama 3（更宽）内存占用约3GB，速度为42 tokens/sec [视频未提供时间戳]。 GPT-OSS MoE架构；恒定宽度的前馈网络 (FFN) 采用非传统的恒定宽度FFN（中间层维度等于输入维度），而非主流的“先扩张后压缩”的倒沙漏形态。这是一种探索参数效率的新尝试，但可能限制了模型在FFN层中的信息处理能力。 \u0026gt; 传统的FFN中间层维度通常是输入的4倍，而GPT-OSS保持不变，与Qwen 3的“沙漏”形态（收缩）和主流的“倒沙漏”形态（扩张）均不同 [视频未提供时间戳]。 Kimmy 2 大量细粒度专家；首层为稠密块 将“多数小型专家”的趋势推向极致，旨在实现更精细化的知识分工。首层采用稠密块是一种工程上的稳定策略，用于防止专家在训练初期过早“崩溃”。 \u0026gt; 拥有1万亿总参数，但激活参数仅320亿，比规模更小的DeepSeek v3（370亿）更高效。其专家数量远超其他模型 [视频未提供时间戳]。 该矩阵揭示了三种相互竞争的设计哲学：1) 通过新颖注意力机制追求超高效率，以DeepSeek的MLA为代表；2) 通过创新的归一化策略追求极致的训练稳定性，由Olmo 2和Gemma 3引领；3) 在吞吐量与内存占用之间进行优化，体现在Qwen、Llama和Mistral的深/浅层架构选择中。这些相互竞争的哲学体现为三个核心的架构争议点，我们将在下文中进行深入剖析。\n-\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n3. 关键架构争议点分析 # 理解当前LLM架构设计的核心争议点，对于评估不同模型的技术路线、预测未来发展趋势以及在实际应用中做出明智的技术选型至关重要。这些争议本质上是在 模型性能、推理成本 和 训练稳定性 这个“铁三角”之间寻求最优解的不同路径。\n3.1 稠密模型 (Dense) vs. 稀疏模型 (Sparse/MoE) # 定义与对比 稠密模型 (Dense Model) 指在每次前向传播计算中，模型的所有参数都会被激活和使用，是传统Transformer架构的标准形态 [视频未提供时间戳]。相比之下，稀疏模型 (Sparse Model) 以专家混合 (Mixture-of-Experts, MoE) 为代表，其拥有海量的总参数，但在处理每个输入时，仅通过一个路由器 (router) 激活一小部分参数（即“专家”），从而在保持巨大模型容量的同时实现计算上的稀疏性 [视频未提供时间戳]。\n稀疏模型的优势 MoE架构的核心价值在于 “以更低的推理成本获得更大的模型容量”。通过将知识分布式地存储在大量专家中，模型可以获得极高的知识容量，但在推理时只需调用一小部分专家，从而保持了较低的计算负载。\n例如，DeepSeek v3拥有6710亿总参数，但在推理时仅激活370亿参数；Llama 4拥有4000亿总参数，激活700亿参数。这使得巨大模型在实际部署中成为可能 [视频未提供时间戳]。 此外，部分MoE架构引入了共享专家 (shared expert) 的概念。该专家每次都被激活，负责学习所有任务都需要的通用知识（如语法、标点符号），从而避免了每个独立专家重复学习这些基础知识，释放其容量以专注于更特定的领域，提升了整体学习效率 [视频未提供时间戳]。\n设计演进与挑战 MoE架构的设计趋势正从早期的 “少数大型专家” 演变为 “多数小型化、细粒度的专家”，这种设计被认为能实现更专业的知识分工 [视频未提供时间戳]。然而，稀疏模型也带来了显著的工程挑战：其训练过程更为复杂，需要精心设计路由器、引入辅助损失项来平衡专家负载，并时刻面临“专家崩溃”等训练不稳定的风险 [视频未提供时间戳]。\n稠密模型的战略价值 尽管稀疏模型优势显著，稠密架构仍保有其战略价值，它提供了简化的训练流程和更低的操作复杂性。Qwen 3同时发布稠密和稀疏版本的策略便是一个力证。对于那些将上市时间和训练稳定性置于首要位置的项目而言，避免MoE路由和负载均衡等复杂问题，选择一个更易于训练和优化的稠密架构，仍然是一个极具吸引力的工程选择 [视频未提供时间戳]。\n3.2 归一化层布局策略：Pre-Norm vs. Post-Norm # 背景阐述 归一化层，如 Layer Normalization 或更现代的 RMS Norm，在Transformer架构中对稳定训练至关重要。其布局策略主要有两种：Post-Norm (后置归一化)，原始Transformer的选择，位于残差连接之后；以及 Pre-Norm (前置归一化)，自GPT-2以来的主流选择，位于多头注意力和前馈网络之前，以提升深度网络的训练稳定性 [视频未提供时间戳]。\n新范式分析 近期模型对这一传统布局发起了挑战，探索更优的稳定性与性能平衡点。\nOlmo 2的“内部Post-Norm”：Olmo 2创新地将Post-Norm布局移至 残差连接的内部。 Gemma 3的“双重归一化”：Gemma 3采取了更为极致的策略，同时使用了 “Pre-Norm + Post-Norm”，并在注意力计算中额外加入了 QK Norm（对Query和Key向量进行归一化）。这种多重保障的设计哲学，反映了其对 追求极致训练稳定性 的不懈努力，即使会带来微小的计算开销 [视频未提供时间戳]。 总结权衡 归一化层布局的争议核心在于 训练稳定性与最终模型性能 之间的权衡。Olmo 2和Gemma 3等模型的创新表明，业界并未满足于现有的Pre-Norm范式，而是正在积极探索能够同时实现稳定训练和高性能的更优解决方案。\n3.3 架构形态之争：深层窄架构 vs. 浅层宽架构 # 识别模式 视频中反复出现一个关键的设计权衡：模型的 深度（Transformer块的数量）与 宽度（前馈网络中间层维度、注意力头数）之间的选择。这一选择直接影响了模型的多个关键特性。\n案例对比分析\nQwen 3 vs. Llama 3：Qwen 3采用了更深、更窄的设计，而Llama 3则更浅、更宽。 Mistral 3.1 vs. Gemma 3：这对组合也呈现了类似的模式。Mistral架构更浅、更宽，而Gemma 3则更深、更窄。这再次体现了模型设计者在 推理延迟和服务成本 与 模型参数效率 之间的不同取舍 [视频未提供时间戳]。 结论提炼 深/窄与浅/宽的选择没有绝对的优劣，而是服务于特定业务目标的工程决策。浅层宽架构优先考虑低延迟的用户体验，因此更受交互式应用的青睐；而深层窄架构则优化了参数效率和内存成本，使其更适合批处理任务或在资源受限的硬件上进行部署。 这一权衡直接决定了模型的 推理速度、内存占用和参数效率。\n参考 # NotebookLLM 视频 转 报告 开源大语言模型架构全景图：11种主流LLM深度对比 https://www.youtube.com/watch?v=rNlULI-zGcw "},{"id":60,"href":"/www6vAlgo/docs/RL/core/RLVR/","title":"RLVR","section":"Core","content":" RLVR # 可验证奖励强化学习 (RLVR) 技术白皮书 # 1.0 引言：重塑大模型能力边界的新范式 # 大型语言模型（LLM）的“后训练”（Post-training）阶段，是将其从一个原始的、未经雕琢的预训练模型，塑造为对用户真正有用、安全且遵循指令的智能工具的关键环节。在这一战略性阶段，各种技术范式不断涌现，旨在精准地调优模型的行为与能力。其中，可验证奖励强化学习 (Reinforcement Learning with Verifiable Rewards, RLVR) 作为一种新兴且强大的训练范式，正日益受到业界的广泛关注。\n本白皮书旨在全面解析 RLVR 的核心原理、技术演进脉络，及其在提升 LLM 数学推理、代码生成等核心能力方面的具体应用与未来潜力。我们希望通过深度的分析与洞察，为人工智能领域的专业人士、研究人员及开发者提供一个清晰、完整的技术蓝图。\n为了充分理解 RLVR 的创新之处，我们有必要首先回顾其技术根源。接下来，我们将从强化学习在语言模型领域的历史演进开始，追溯从“从人类反馈中学习的强化学习”（RLHF）到 RLVR 的范式变革之路。\n2.0 从 RLHF 到 RLVR：一种范式的演进 # 技术的进步往往遵循着清晰的演进脉络，新的范式总是在继承与革新中诞生。可验证奖励强化学习 (RLVR) 并非凭空出现，它是对早期强化学习范式，特别是“从人类反馈中学习的强化学习”（Reinforcement Learning from Human Feedback, RLHF）的直接回应与发展。本章节将追溯这一发展路径，以揭示 RLVR 诞生的必然性及其旨在解决的核心问题。\n2.1 奠基石：从人类反馈中学习的强化学习 (RLHF) # RLHF 的核心思想是利用人类的偏好数据来训练一个“奖励模型”（Reward Model），这个模型能够评估模型生成内容的优劣，并以此为信号，通过强化学习算法来指导和优化大型语言模型的策略。简而言之，它教会模型“什么是更好的回答”。\n以 **InstructGPT** 和 **ChatGPT** 的巨大成功为例，RLHF 发挥了至关重要的催化剂作用。它不仅开启了流畅、自然的聊天交互模式时代，更重要的是，它为后续更复杂的强化学习应用奠定了坚实的基础架构，并点燃了整个行业对 RL 与语言模型结合的浓厚兴趣。\n从本质上看，RLHF 是一种将复杂、模糊甚至难以言传的人类价值观与偏好注入到模型中的有效方式。然而，这一过程的复杂性也为其带来了与生俱来的挑战。\n2.2 RLHF 的挑战与局限性 # RLHF 在实践中面临着多重挑战，这些挑战共同推动了业界对更高效、更直接的训练范式的探索：\n数据采集的复杂性: 收集高质量、大规模、一致性强的人类偏好数据，是一项成本高昂且耗时巨大的工程。 过程的“混乱”与跨学科性: RLHF 的实施远非一个纯粹的算法问题。它涉及心理学、社会学以及难以量化的基础性问题，整个流程“混乱且跨学科”（messy and interdisciplinary），难以实现标准化和规模化复制。 主观性与不一致性: 完全依赖人类标注员的主观偏好，不可避免地会引入个人或群体的偏见，为奖励模型的稳定性和可靠性带来了不确定性。 与其将这些挑战视为有待“解决”的问题，不如将其看作是 tackling 基础性、主观性对齐任务时固有的复杂性。RLHF 试图解决的是“何为良好价值观”这类根本性难题，这决定了其在学术上具有更长远的研究前景。\n2.3 RLVR 的兴起：更直接、更高效的路径 # RLVR 的出现，并非作为 RLHF 的替代品，而是一种务实的、以行业为中心的优先级转变。它巧妙地绕开了 RLHF 试图解决的“混乱”的基础性问题，转而专注于那些能够带来可衡量性能提升的客观任务。其核心变革在于，用一个客观的、可被程序自动验证的“计分函数”（Scoring Function）或“验证器”（Verifier），取代了基于人类偏好的、主观的奖励模型。\n这一转变具有重大的战略意义。RLVR 的目标极为明确和简单——“让模型的分数变得更高”（make the scores better）。这一目标与行业的核心需求高度契合，因为它能够直接、可量化地提升模型在特定任务上的性能，尤其是在那些具有明确“正确答案”（ground truth）的领域，如数学问题求解、代码生成与调试、以及事实性知识遵循等。\n下表清晰地对比了 RLHF 与 RLVR 之间的核心区别：\n特性 强化学习来自人类反馈 (RLHF) 可验证奖励强化学习 (RLVR) 奖励来源 基于人类偏好数据的奖励模型 (主观) 基于任务结果的确定性计分函数 (客观) 核心目标 对齐人类价值观、风格和偏好 提升在可验证任务上的性能分数 应用领域 风格塑造、安全性、通用对话能力 数学推理、代码生成、事实遵循 主要挑战 数据收集成本高、过程复杂、主观性强 仅适用于有明确验证标准（“ground truth”）的领域 总而言之，RLVR 不仅仅是一次技术上的简化，更代表了后训练理念的一次重要转变——从解决模糊的基础性对齐问题，转向追求可度量的任务性能。这一转变，为我们深入剖析 RLVR 的技术核心奠定了基础。\n3.0 RLVR 的核心原理与技术框架 # 在理解了 RLVR 的演进背景后，本章节将深入其技术内部，详细剖析它在现代 LLM 后训练流程中的具体定位、关键组成部分以及核心运作机制。我们的目标是为读者构建一个清晰、准确的 RLVR 技术蓝图。\n3.1 RLVR 在现代后训练流程中的定位 # 在现代 LLM 的后训练实践中（如 Tulu 3 模型所展示的），通常包含一个多阶段、目标各有侧重的流程。RLVR 在其中扮演着一个独特且关键的角色。我们可以将这个流程分解为三个递进的阶段：\n监督微调 (Supervised Fine-Tuning, SFT): 这是后训练的基石。SFT 的主要作用是**“教授格式和响应形态”**，通过高质量的“指令-回答”对，让模型学会如何遵循指令、以特定的结构（如聊天格式）生成内容。它为模型打下了行为基础。 偏好微调 (Preference Tuning, 如 DPO): 这是 RLHF 理念的现代化、泛化实现。它通过对比数据（例如，“回答A”优于“回答B”）来教会模型**“更像什么，更不像什么”**。这一阶段主要用于优化模型的风格、语调，并能间接提升推理路径的合理性，使其更符合人类偏好。 强化微调 (Reinforcement Fine-Tuning, 即 RLVR): 这是三个阶段中**“最开放”（most open-ended）**的环节。我们订阅“能力激发观”（elicitation view）来理解其作用。这一观点至关重要，因为它将 RLVR 的角色重新定义为：它并非在教授模型新知识，而是创建一个强大的激励信号，以放大那些在预训练后已存在但概率较低的潜在能力。 通过奖励那些成功解决任务的输出，RLVR 极大地提升了模型生成正确答案的概率，从而在数学、代码等硬核能力上实现突破。 3.2 RLVR 机制的关键组成部分 # 若将经典的强化学习框架（智能体-环境）与 LLM 的应用进行类比，会发现一个显著的变化。在 LLM 的 RLVR 实践中，“环境”（Environment）的概念被极大地简化了。模型（智能体）实际上是与一系列独立的提示词（状态）和一个奖励函数进行交互，而非在一个连续的、有状态记忆的环境中探索。\nRLVR 的核心技术替换体现在以下方面： 用“验证器”（Verifier）取代“奖励模型”（Reward Model）。\n与需要大量数据训练的奖励模型不同，验证器通常是一个简单的、确定性的函数。其工作原理非常直接：\n如果模型生成的答案是正确的，则给予一个正向奖励（例如 +1）。 如果答案是错误的，则不给予奖励（即奖励为 0）。 这种非黑即白的奖励机制，使得训练信号非常清晰，直指最终的任务目标。\n3.3 验证方法与应用领域 # RLVR 的强大之处在于其验证方法的多样性与可扩展性，能够应用于任何可以定义明确验证规则的领域。以下是一些典型的验证方法及其应用场景：\n数学问题验证 通过 math_verify 等开源库或内部开发的工具，解析模型生成的最终答案，并检查其是否与标准答案相符。这对于提升模型在 GSM8K 等数学基准测试上的表现至关重要。 代码执行验证 通过在隔离的**代码沙箱（Code Sandboxes）**中运行模型生成的代码，并检查其是否能成功通过预设的单元测试（Unit Tests）。这是提升模型编程和软件工程能力的关键手段。 事实性知识验证 使用一个能力更强的“语言模型作为裁判”（LM as a Judge）来判断模型回答的事实准确性。虽然这是一种相对“较软”的验证形式，但它将可验证性的边界从绝对正确扩展到了事实一致性领域。 指令遵循验证 对于特定的格式要求，可以设计简单的检查函数。例如，对于“请用全小写字母回答 IPv6 的作用”这一指令，验证器只需检查返回的文本中是否不含大写字母即可。 在处理更复杂的多领域问题时，单一的 +1/0 奖励可能不足。此时，可以引入**“奖励塑造”（Reward Shaping）**的概念，设计更精细的奖励机制来权衡答案的不同方面（如步骤的正确性、最终结果的准确性等），从而引导模型产生更理想的行为。\n总而言之，RLVR 的核心框架简洁而强大，其可扩展性使其能够持续赋能 LLM 在各类可精确评估的任务上取得进步。接下来，我们将探讨支撑这一框架的具体算法。\n4.0 关键算法与实施考量 # 一个成功的理论框架，离不开高效算法的支撑以及对实践中各种细微差别的深刻理解。本章节将聚焦于当前支撑 RLVR 的核心算法，特别是 GRPO，并深入探讨在实际部署过程中必须面对的挑战与权衡。\n4.1 算法演进：从 PPO 到 GRPO # 在语言模型的强化学习应用中，一个显著的行业趋势是，越来越多的方法正在减少对传统“价值函数”（Value Functions）的依赖。在此背景下，由 DeepSeek 提出并成功应用的 GRPO (Group Relative Policy Optimization) 算法脱颖而出，它作为经典 PPO 算法的变体，展现出优异的性能。GRPO 的关键特征在于：\n优势函数 (Advantage) 的计算: 这是 GRPO 的核心创新。一个生成补全（completion）的“优势”，是根据其奖励与同一提示词下其他所有补全的平均奖励进行比较得出的。这带来了一个深刻的后果：如果在处理单个提示的批次（batch）中，所有生成的答案都正确（奖励均为+1）或都错误（奖励均为0），那么每个答案的优势都将为零。这意味着模型将不会从该样本中进行任何学习更新。 这一机制是 GRPO 效率的关键所在：它自动将学习过程聚焦于模型产生对错混杂的“边界”案例上——这正是模型最需要学习的地方。 KL 惩罚项的位置: 在 GRPO 中，用于防止模型策略偏离过远的 KL 散度惩罚项被直接置于损失函数中。这与许多传统 RLHF 实践中将其作为奖励的一部分有所不同，是一个影响训练动态的重要实现细节。 4.2 算法细节的权衡：长度偏见与归一化 # GRPO 的成功也催生了一系列变体算法，如 DAPO 和 Dr-GRPO，它们试图在细微之处进行优化。然而，虽然这些算法变体在学术上很有趣，但一个来自实践的关键教训是：实现质量和数据分布的性质，其影响力往往远超算法上的细微调整。 与其将新论文视为保证有效的银弹，不如将其看作是学习 RL 机制的宝贵课程。\n这些算法调整试图解决的一个核心问题是**“长度偏见”（Length Bias）**。这种偏见源于一个事实：一条长而正确的推理链是一系列 token 概率相乘的产物，这使得其整体序列概率天生较低。在按 token 计算的策略梯度更新中，这条低概率但高价值的序列可能比较短、价值较低的序列获得不成比例的小更新权重。不同的归一化策略正是为了抵消这种效应，确保有价值的长篇推理得到应有的奖励。\n4.3 训练过程中的挑战：方差与可复现性 # 与 SFT 和 DPO 相比，RLVR 训练过程表现出显著更高的方差。这一挑战给研发和部署带来了实际的困难：\n对随机种子的敏感性: 使用不同的随机种子启动训练，最终得到的模型性能可能会有天壤之别。 收益的不可预测性: 有时 RLVR 阶段能带来巨大的性能提升（例如，在基准测试上+2个百分点），而有时其收益却微乎其微，其中的原因尚不完全清楚。 行业实践: 这种高方差迫使工业界采取一种务实的、近乎“暴力”的方法：并行启动多个具有不同随机种子的 RL 训练任务。通过密切监控早期的学习曲线，表现不佳的运行会被主动“扼杀”，以便将计算资源重新导向少数展现出良好早期轨迹的任务。 资源投入的权衡: 实验表明，“让模型烹饪更久”（let it cook longer），即延长训练时间，有时确实能获得更好的模型。但这与宝贵的 GPU 资源的投资回报率（ROI）之间存在着一种难以预测的权衡。 尽管存在这些挑战，RLVR 凭借其巨大的潜力，已经展现出不可替代的应用价值。下一章，我们将具体分析其应用案例与所取得的丰硕成果。\n5.0 应用案例、实验成果与最佳实践 # 理论和算法的价值最终需要通过实际应用来检验。本章节将展示 RLVR 在不同场景下的具体应用，分析其带来的显著性能提升，并总结从大量实验中提炼出的关键洞见与最佳实践。\n5.1 核心应用：提升数学与编程等关键能力 # RLVR 最直接、最主流的应用场景，是在标准的后训练流程（SFT 和 DPO）之后，增加一个专门的 RLVR 阶段。这一阶段的目标非常明确：集中火力提升模型在数学和编程等可被精确评估的硬核能力上的分数。\n在实践中，研究人员还探索出一种被称为**“迭代式 RLVR”（Iterative RLVR）**的有效技巧。其操作方式是，在一个已经过少量 RLVR 训练的模型检查点（checkpoint）上，重新开始新一轮的 RLVR 训练。实验表明，这种迭代优化的方法有时能够进一步压榨出模型的性能潜力，实现分数的持续攀升。\n一个令人鼓舞的观察是，RLVR 的训练曲线通常表现得相当稳定，其形态与标准的强化学习实践非常相似，这大大降低了其在工程上实施和调试的门槛。\n5.2 范式拓展：直接在基础模型上进行大规模 RL # 这种方法与 5.1 节中寻求的增量改进形成鲜明对比。其目标不仅仅是提升一个已具能力的模型的分数，而是从根本上**“激发”**出基础模型中如“思维链”（Chain-of-Thought）等复杂的推理行为，使其经历一次更为剧烈的策略转变，之后再通过偏好微调等手段进行风格或安全性的对齐。\n这种更具雄心的应用范式是：直接在预训练完成的基础模型（Base Model）上进行大规模的 RLVR 训练，DeepSeek 的 Q* 模型便是这一范式的典型代表。\n5.3 性能分析：显著的增益与宝贵的特性 # 综合评估 RLVR 带来的性能影响，其成果是显著但多变的。我们可以将其核心价值总结为以下两个方面：\n性能提升 (Performance Enhancement): 在数学和代码等关键基准测试上，RLVR 能够带来显著的分数提升，例如相比 DPO 后的模型，有时能高出 2 个百分点。 通过算法优化（如从 PPO 切换到 GRPO）和迭代式训练等技巧，可以获得更大的性能飞跃。 能力保持 (Capability Retention): 一个关键且令人惊喜的发现是，RLVR 在专门提升目标能力（如数学）的同时，通常不会导致模型在其他通用能力上出现明显下降。例如，衡量通用对话能力的 AlpacaEval 分数能够保持稳定。 这证明了现代 LLM 具有巨大的学习容量，可以在不产生“灾难性遗忘”的情况下，高效地学习和整合新技能。 实践证明，RLVR 是一种高效且相对安全的模型能力增强技术。它的成功应用不仅提升了现有模型的性能上限，也为我们指明了通往更强大人工智能的未来方向。\n6.0 未来方向与展望 # 作为一种新兴的、以结果为导向的后训练方法，可验证奖励强化学习 (RLVR) 已经取得了令人瞩目的成功。尽管对于强化学习的纯粹主义者而言，RLVR 的核心原则——奖励智能体正确的产出——听起来可能有些基础，但其真正的颠覆性在于它在现代 LLM 后训练栈中的易用性、可扩展性以及惊人的有效性。当前的研究和应用很可能仅仅是冰山一角。本章节将探讨 RLVR 未来可能演进的几个重要方向，及其可能对整个大型语言模型发展格局产生的深远影响。\n6.1 扩展可验证性的边界 # RLVR 的一个核心发展方向，是将其应用范围从数学、代码等“硬”可验证领域，逐步扩展到事实性问答、遵循复杂指令等“半”可验证领域。我们可以构想一个**“可验证性谱系”（Spectrum of Verifiability）**：在谱系的一端，是具有绝对正确答案的任务，它们是 RLVR 的主场；在另一端，是完全主观的任务，这依然是偏好微调的优势领域。未来的后训练流程将不再是单一技术的线性组合，而是会根据任务的可验证程度，灵活地结合多种技术，以达到最优效果。\n6.2 与推理时计算的协同进化 # 当前，RLVR 是实现**“推理时计算扩展”（Inference-time Scaling）最主流的训练手段，即通过训练让模型生成更长、更复杂的推理链来提升准确率。然而，这种关系可能会在未来发生演变。随着并行计算等其他推理时优化技术的成熟，或许我们不再需要通过 RL 无限度地增加模型的单次生成长度。未来的趋势可能是在训练成本**（通过 RL 教会模型长推理链）与推理效率（通过更高效的推理时技术获得答案）之间寻求一个更优的平衡点。\n6.3 终极愿景：RL 是否会成为 LLM 训练的核心？ # 最后，我们不妨提出一个大胆的设想：RLVR 或更广义的强化学习训练，未来是否可能成为 LLM 开发的焦点？\n一个引人深思的案例是，DeepSeek V3 论文中后训练仅占总计算资源的 0.2%，而其后续 Q* 模型的 RL 训练则耗时数周。这表明，后训练，特别是 RL 阶段的计算占比正在以前所未有的速度急剧增加。我们可以描绘一个未来的图景：LLM 的开发过程，可能演变为在一个包含多种验证器和 RL 环境的复杂系统中，通过大规模、多领域的强化学习训练，来持续迭代和提升一个核心智能体的通用能力。如果这一天到来，它将从根本上改变当前以预训练为主导的计算资源分配格局。\nRLVR 的出现，不仅仅是一次技术工具的进步。它更可能预示着我们关于如何构建和发展大型语言模型这一核心理念的深刻变革。这条通往更强通用智能的道路，值得整个 AI 社区持续关注、探索和投入。\nPPT # RLVR\n参考 # Notebook 视频转报告\nbili\n"},{"id":61,"href":"/www6vAlgo/docs/Agent/Deep-Research/Survey/TongyiDeepresearchNT/","title":"Tongyi DeepResearch","section":"Survey","content":" notebookLLM # "},{"id":62,"href":"/www6vAlgo/docs/Agent/Deep-Research/Search/websailerNT/","title":"WebSailor","section":"Search","content":" notebookLLM # "},{"id":63,"href":"/www6vAlgo/docs/DeepLearning/basic/Pytorch/","title":"(实战)PyTorch","section":"basic","content":"\nPyTorch 实战 # (实战)PyTorch\n"},{"id":64,"href":"/www6vAlgo/docs/LLM/Survey/LeaderBoard/","title":"大模型 排行榜","section":"Survey","content":" 大模型 # 排行榜 # HuggingFaceH 大模型排行榜\nLLM Collection\n中国排行榜 # 中国大模型 通用 39 金融 25 司法 8 法律 6 医学 13 医疗 24 教育 13 科研 17 工业 23 政务 12 运维 7 "},{"id":65,"href":"/www6vAlgo/docs/Vision/%E4%BA%BA%E5%83%8F%E7%94%9F%E5%9B%BE/gptMultimodalIDCreate/","title":"人像生图","section":"人像生图","content":"\nInstantID # InstantID\nPhotoMaker # PhotoMaker\n总结 # 【InstantID : ipAdaptor +controlnet, image Contoll的思路】\n【photomaker: image Edit 的思路】\n"},{"id":66,"href":"/www6vAlgo/docs/LLM/Reasoning/Survey/ReasoningTTS/","title":"(Survey) Test-Time Scaling","section":"Survey","content":"\nTest-Time Scaling # (Survey) Test-Time Scaling\n"},{"id":67,"href":"/www6vAlgo/docs/Vision/Controllable/gptDiffusionDreamBooth/","title":"(原理|实战)DreamBooth","section":"Controllable","content":"\nApproach[1] # Our method takes as input a few images (typically 3-5 images suffice, based on our experiments) of a subject (e.g., a specific dog) and the corresponding class name (e.g. \u0026ldquo;dog\u0026rdquo;), and returns a fine-tuned/\u0026ldquo;personalized\u0026rsquo;\u0026rsquo; text-to-image model that encodes a unique identifier that refers to the subject. Then, at inference, we can implant the unique identifier in different sentences to synthesize the subjects in difference contexts. 我们的方法将主题（例如，特定的狗）和相应的类名称（例如“狗”）的一些图像（根据我们的实验，通常 3-5 个图像就足够了）作为输入，并返回一个微调/ “个性化”文本到图像模型，编码指向主题的唯一标识符。然后，在推理时，我们可以将唯一标识符植入不同的句子中，以合成不同上下文中的主题。\nGiven ~3-5 images of a subject we fine tune a text-to-image diffusion in** two steps**:(a) fine tuning the low-resolution text-to-image model with the input images paired with a text prompt containing a unique identifier and the name of the class the subject belongs to (e.g., \u0026ldquo;A photo of a [T] dog”), in parallel, we apply a class-specific prior preservation loss, which leverages the semantic prior that the model has on the class and encourages it to generate diverse instances belong to the subject\u0026rsquo;s class by injecting the class name in the text prompt (e.g., \u0026ldquo;A photo of a dog”). (b) fine-tuning the super resolution components with pairs of low-resolution and high-resolution images taken from our input images set, which enables us to maintain high-fidelity to small details of the subject. 给定约 3-5 个主题的图像，我们分两步微调文本到图像的扩散：(a) 使用与包含唯一的文本提示配对的输入图像微调低分辨率文本到图像模型标识符和主题所属类的名称（例如，“[T]狗的照片”），同时，我们应用特定于类的先验保存损失，它利用模型在类并鼓励它通过在文本提示中注入类名（例如“狗的照片”）来生成属于主题类的不同实例。 (b) 使用从输入图像集中获取的低分辨率和高分辨率图像对来微调超分辨率组件，这使我们能够保持对象小细节的高保真度。\nDreambooth实战 # Dreambooth in Diffusers [10] # ### download pic from huggingface_hub import snapshot_download local_dir = \u0026#34;./dog\u0026#34; snapshot_download( \u0026#34;diffusers/dog-example\u0026#34;, local_dir=local_dir, repo_type=\u0026#34;dataset\u0026#34;, ignore_patterns=\u0026#34;.gitattributes\u0026#34;, ) ### training ### 在modelscope上运行有问题，连不上huggingface export MODEL_NAME=\u0026#34;CompVis/stable-diffusion-v1-4\u0026#34; export INSTANCE_DIR=\u0026#34;dog\u0026#34; export OUTPUT_DIR=\u0026#34;path-to-save-model\u0026#34; accelerate launch train_dreambooth.py \\\\ --pretrained_model_name_or_path=$MODEL_NAME \\\\ --instance_data_dir=$INSTANCE_DIR \\\\ --output_dir=$OUTPUT_DIR \\\\ --instance_prompt=\u0026#34;a photo of sks dog\u0026#34; \\\\ --resolution=512 \\\\ --train_batch_size=1 \\\\ --gradient_accumulation_steps=1 \\\\ --learning_rate=5e-6 \\\\ --lr_scheduler=\u0026#34;constant\u0026#34; \\\\ --lr_warmup_steps=0 \\\\ --max_train_steps=400 \\\\ --push_to_hub # examples/dreambooth/train_dreambooth.py ### 把prior loss加到instance loss上 if args.with_prior_preservation: # Add the prior loss to the instance loss. loss = loss + args.prior_loss_weight * prior_loss dreambooth in Diffusers examples[11] # if args.with_prior_preservation: # Chunk the noise and noise_pred into two parts and compute the loss on each part separately. noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0) target, target_prior = torch.chunk(target, 2, dim=0) # Compute instance loss loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\u0026#34;none\u0026#34;).mean([1, 2, 3]).mean() # Compute prior loss prior_loss = F.mse_loss(noise_pred_prior.float(), target_prior.float(), reduction=\u0026#34;mean\u0026#34;) # Add the prior loss to the instance loss. loss = loss + args.prior_loss_weight * prior_loss else: loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\u0026#34;mean\u0026#34;) accelerator.backward(loss) 参考 # Dreambooth 原理 # DreamBooth Repo 1xx. DreamBooth 论文精读+通俗理解【论文精读】Dreambooth：diffusion生成模型微调方法 V\nDreambooth 实战 # 使用 Diffusers 通过 Dreambooth 技术来训练 Stable Diffusion\nDreambooth Repo\ndreambooth Diffusers examples on Colab 运行过 Initial setup Settings for teaching your new concept Teach the model the new concept (fine-tuning with Dreambooth) Run the code with your newly trained model\n1xx. + Unit 3: Stable Diffusion\n1xx. Dreambooth V 【只训练unet】\n"},{"id":68,"href":"/www6vAlgo/docs/Vision/editing/gptDiffusionEditPix2pix/","title":"pix2pix-zero","section":"Editing","content":"\n论文 # 论文地址\nZero-shot Image-to-Image Translation\n开源地址\npix2pix-zero git\nProject page\nProject page\n官网有个介绍视频 看过不错\nMethod[1] # 上图展示了pix2pix-zero方法的概述，这是一个将图片从猫变成狗的图像到图像的翻译例子。首先，使用规范化的DDIM反转来得到一个反转的噪声映射，这是由BLIP图像字幕（caption）网络和CLIP文本嵌入模型自动生成的文本嵌入引导的。然后，使用原始文本嵌入去噪以获得交叉注意力图，作为输入图像结构的参考（顶部行）。接下来，使用编辑后的文本嵌入去噪，通过损失函数确保这些交叉注意力图与参考交叉注意力图相匹配（第二行）。这确保了编辑图像的结构与原始图像相比不会发生剧烈变化。没有交叉注意力引导的去噪示例显示在第三行，导致结构上的大偏差。此可视化强调了在编辑过程中保持图像原始结构的交叉注意力的重要性。\nDiscovering edit directions [2] # 参考 # pix2pix-zero：零样本图像到图像转换 【深度学习】【风格迁移】Zero-shot Image-to-Image Translation 实战\nZero-shot Image-to-Image Translation\n"},{"id":69,"href":"/www6vAlgo/docs/Vision/editing/gptDiffusionEditInstruct/","title":"InstructPix2Pix","section":"Editing","content":"\n论文 # 论文地址 InstructPix2Pix: Learning to Follow Image Editing Instructions 开源地址 Repo git Project page Project page Method # 这章就讲两件事：1）如何生成数据集（章节3.1）；2）如何基于上一步生成的训练数据，训练一个图像编辑扩散模型（章节3.2）；\n参考 # InstructPix2Pix：用指令给图像做修改\n实战 # 1xx. InstructPix2Pix diffusers 1xx. InstructPix2Pix V Code Repo git\n"},{"id":70,"href":"/www6vAlgo/docs/Vision/editing/gptDiffusionEditPrompt/","title":"Prompt-to-Prompt","section":"Editing","content":"\n论文 # 论文地址 PROMPT-TO-PROMPT IMAGE EDITING WITH CROSS-ATTENTION CONTROL 开源地址 Prompt-to-Prompt git Project page Project page 示意[1] # 应用[1] # **应用方面：**Prompt-to-Prompt 这个方法是原理上的创新，应用方面只适用于“已经生成了一张大致满意的图，我们想对它进行部分修改”。但是对于“手头有一张来历不明的图，我们想对它进行修改”这个任务就很麻烦了，因为很难去倒推这张图对应的prompt是啥。\n所以后续有一项工作叫 InstructPix2Pix，作用是“一张来历不明的图，只要说‘把猫改成狗’，模型就能把画面里的猫改成狗，其他不变。”非常好用，听说已经集成在 Stable Diffusion WebUI 里可以直接用了。\nMethod[1] # 上半部分，原版cross-attention，下半部分，本文的cross-attention\n参考 # Prompt-to-prompt：让生成的图像保持一致 diffusion model(十四)： prompt-to-prompt 深度剖析\n"},{"id":71,"href":"/www6vAlgo/docs/LLM/Reasoning/Survey/ReasoningPostTraining/","title":"(Survey)Reasoning LLM Post-Training","section":"Survey","content":"\nReasoning LLM Post-Training # "},{"id":72,"href":"/www6vAlgo/docs/RL/core/GRPO-family/GRPO/","title":"(原理)GRPO","section":"GRPO Family","content":"\nGRPO # GRPO\n"},{"id":73,"href":"/www6vAlgo/docs/Vision/Survey/gptMultimodalSeries/","title":"多模态 系列","section":"Survey","content":"\n目录 # Stage1: 模块独立[2] # {% asset_img \u0026rsquo;\u0026rsquo; %}\nmodel # CLIP ViLT ALBEF Stage2: 模块共享[2] # model # VLMO BLIP BLIP2 BEiTv3 Stage3: 范式统一[2] # model # Unified-IO Uni-Perceiver PaLi 总结 [1] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n参考 # Overview # 多模态大模型 CLIP, BLIP, BLIP2, LLaVA, miniGPT4, InstructBLIP 系列解读 ***\n[Transformer 101系列] 多模态的大一统之路 ***\n1xx. 多模态论文串讲 *** 多模态论文串讲：ALBEF \u0026amp; VLMo \u0026amp; BLIP \u0026amp; CoCa \u0026amp; Beit V3\n1xx. 图生文多模态大模型开源项目回顾：兼看20240307大模型进展早报\n1xx. 图文多模态大模型综述\n1xx. Multimodality and Large Multimodal Models (LMMs) 多模态和多模态大模型 (LMM)[译] CLIP Flamingo\n1xx. 写在多模态征服一切之前（未来数据和模型应该是什么样的？）\n"},{"id":74,"href":"/www6vAlgo/docs/Vision/Vision-Encoder/gptMultimodalVQVAE/","title":"VQ-VAE","section":"Vision Encoder","content":"\n参考 # 关于 VQ-VAE 直观理解\n[论文简析]VQ-VAE:Neural discrete representation learning[1711.00937] v 看评论中的置顶 有代码\n如何搭建VQ-VAE模型（Pytorch代码） v 看视频介绍 https://github.com/KevinOfCathay/DDPM-demo\n"},{"id":75,"href":"/www6vAlgo/docs/Vision/Vision-Encoder/gptMultimodalDINO/","title":"DINO","section":"Vision Encoder","content":"\nDINO # https://github.com/facebookresearch/dino\n参考 # 重塑自监督学习: DINO 网络如何颠覆视觉特征表示的常规方法 有动图\nDINOv2 # 论文 # 论文地址 DINOv2: Learning Robust Visual Features without Supervision\n开源地址 https://github.com/facebookresearch/dinov2\nProject page Project page Project page\n参考 # DINOv2：无需微调，填补 SAM 的空白，支持多个下游任务\n全网最详细的 DINOv2 论文解读来啦！\n深度学习算法应用实战 | DINOv2 图像相似度实战\n视觉大模型DINOv2:自我监督学习的新领域\nDINOv2：无需微调，填补SAM空白\n实战 # DINOv2 on mmpretrain\n"},{"id":76,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/post-training/DeepseekDistill/","title":"(实战)Deepseek 蒸馏","section":"Post-training","content":"\n(实战)Deepseek 蒸馏\n"},{"id":77,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/post-training/DeepseekSFT/","title":"(实战)Deepseek R1 SFT","section":"Post-training","content":"\nDeepSeek R1 SFT # (实战)DeepSeek R1 SFT\n"},{"id":78,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/DeepSeekExplain2/","title":"解读 DeepSeek[邱锡鹏]","section":"R1","content":"\n这基本上就是R1的技术路线。我简单列一些关于DeepSeek R1的思考和启发：\n1、R1/R1-zero的技术路线和社区对o1复现的差异\n此前社区对o1的复现基本都会涉及到蒸馏和搜索。 R1-Zero没有SFT，没有过程监督，没有搜索，也能训练出类似o1的效果。学术界之前也有很多实验，但在较小的模型上都没有成功。说明只有基模型足够强，Scaling RL才能取得比较好的效果。 虽然R1强调MCTS没有效果，但是简单的majority vote能大幅提升R1的效果，说明搜索仍然是重要的Scale的范式。 R1的成功还依赖DeepSeek强大的系统效率和RL调教能力。 2、策略初始化\nR1-zero是一个比较好的尝试，但是R1还是经过了先SFT（大概几干条）后再进行RL。 未来后训练的重心会逐步倾向于RL，但是少量训练用于SFT可能还是必须的。 3、奖励模型\nR1的奖励设计跟普通的后训练没特别大的区别（Qwen2，Tulu3），有ground truth用ground truth做EM，否则用RM。 RM的（训练数据量，模型大小，OOD问题，选代周期）的相关问题在整个训练的流程中还是比较关键。可能使用当前开源的比较强大的RM可以达到比较好的效果，也有可能基于内部的数据重新进行了偏好标注。 奖励设计（例如RPM的技巧）可能会在基于少量样本的强化学习微调上仍然起到显著作用。 4、PRM和MCIS\nDS给了两个PRM和MCTS的“不成功尝试”。但PRM部分说的比较笼统，并且DS的PRM只评估Correctness（与OAI的Lets verify step by step一致）。 R1给的是一个简单而且可规模化的可行解，这样做不一定是最优的。基于R1的Test-time search也继续优化它的效果。 PRM总归是一种比较稠密的监督信号，按照传统R1的理论，对OR进行shaping可以使训练更稳定或收敛得更快。 PRM不应该是一个被完全放弃的东西，可以让模型收敛得更快速或更稳定（Scaling曲线的斜率更大）。 5、写作能力提升\no1相比4o在写作等任务上的提升非常小，但R1的创作经常会令人眼前一亮，可能主要是强基模型在Scale RL后涌现的能力，也有人猜测是因为R1的安全对齐做的比较少，没有太约束模型的创作能力。 6、过度优化问题\nR1经常会使用一些高端词汇，典型的如量子纠缠和熵增熵减（会用在各个领域）。猜测是某种形式的reward hacking导致的。 R1在一些通用领域没有ground truth的任务上的推理效果还并不理想，强化学习的训练并不能保证泛化。 7、Test-Time Scaling\no1出来后大家讨论比较多的是Test-Time Scaling，但重要的还是Training-Time Scaling，包括数据和Training Step。蒸馏见效快，但上限不高，重要的还是高质量致据的缺失，蒸馏数据无法提供训练Scaling。RL是其中的关键，因为它可以保障有足够的数据和足够的训练步骤。 8、Agentic展望\nR1是目前唯一同时具有强推理能力和联网搜索的产品，效果很好，可以调研一些复杂的信息并进行回答。强推理模型最终的落脚点大概率是Agent，怎么用强推理模型帮助Agent更好更鲁棒是一个比较重要的问题。 参考 # DeepSeek最强专业拆解来了，清交复教授超硬核解读\n"},{"id":79,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/DeepSeekExplain1/","title":"解读 DeepSeek[刘知远]","section":"R1","content":"\n硬核解读 DeepSeek：大模型强化学习技术原理与大模型技术发展研判\n"},{"id":80,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/ReasoningLLM/","title":"(原理)Reasoning LLM","section":"R1","content":"\nReasoning LLM # (原理)Reasoning LLM\n"},{"id":81,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/DeepSeekR1/","title":"(原理) DeepSeek R1","section":"R1","content":"\n(原理) DeepSeek R1 # (原理)DeepSeek R1\n"},{"id":82,"href":"/www6vAlgo/docs/LLM/Reasoning/Overthinking/survey/","title":"(Survey)Overthinking","section":"Overthinking","content":" 论文 # Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models\nhttps://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs\n参考 # NICE 有个分享\n全景解读高效推理：LLM 也会「想太多」？\nhttps://zhuanlan.zhihu.com/p/1888902868641244612 https://www.cnblogs.com/Orzjh/p/18957909 ***\nhttps://blog.csdn.net/qq_27590277/article/details/146432960 ***\n"},{"id":83,"href":"/www6vAlgo/docs/LLM/Reasoning/Qwen/qwen-next/","title":"(原理)Qwen3-Next","section":"Qwen","content":" 模型结构 # 混合架构：Gated DeltaNet + Gated Attention 线性注意力打破了标准注意力的二次复杂度，在处理长上下文时有着更高的效率。我们发现，单纯使用线性注意力或标准注意力均存在局限：前者在长序列建模上效率高但召回能力弱，后者计算开销大、推理不友好。通过系统实验，我们发现 Gated DeltaNet [1] 相比常用的滑动窗口注意力（Sliding Window Attention）和 Mamba2 有更强的上下文学习（in-context learning）能力，并在 3:1 的混合比例（即 75% 层使用 Gated DeltaNet，25% 层保留标准注意力）下能一致超过超越单一架构，实现性能与效率的双重优化。 在保留的标准注意力中，我们进一步引入多项增强设计：\n（1）沿用我们先前工作 [2] 中的输出门控机制，缓解注意力中的低秩问题。\n（2）将单个注意力头维度从 128 扩展至 256。\n（3）仅对注意力头前 25% 的位置维度添加旋转位置编码，提高长度外推效果。\n极致稀疏 MoE：仅激活 3.7% 参数 Qwen3-Next 采用了高稀疏度的 Mixture-of-Experts (MoE) 架构，总参数量达80B，每次推理仅激活约 3B 参数。我们实验表明，在使用全局负载均衡 [4] 后，当激活专家固定时，持续增加专家总参数可带来训练 loss 的稳定下降。相比Qwen3 MoE的128个总专家和8个路由专家，Qwen3-Next我们扩展到了512总专家，10路由专家与1共享专家的组合，在不牺牲效果的前提下最大化资源利用率。\n训练稳定性友好设计 我们发现， 注意力输出门控机制能消除注意力池 [5] 与极大激活 [6] 等现象，保证模型各部分的数值稳定。在Qwen3中我们采用了QK-Norm，我们发现部分层的 norm weight 值会出现异常高的现象。为了缓解这一现象，进一步提高模型的稳定性，我们在Qwen3-Next中采用了 Zero-Centered RMSNorm [7]，并在此基础上，对 norm weight 施加 weight decay，以避免权重无界增长。我们还在初始化时归一化了 MoE router 的参数 [8]，确保每个 expert 在训练早期都能被无偏地选中，减小初始化对实验结果的扰动。\nMulti-Token Prediction Qwen3-Next 引入原生 Multi-Token Prediction (MTP) 机制 [3][9][10]，既得到了 Speculative Decoding 接受率较高的 MTP 模块，又提升了主干本身的综合性能。Qwen3-Next 还特别优化了 MTP 多步推理性能，通过训练推理一致的多步训练，进一步提高了实用场景下的 Speculative Decoding 接受率。\n参考文献 # Gated Delta Networks: Improving Mamba2 with Delta Rule\nGated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free\nDeepSeek-V3 Technical Report\nDemons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models\nEfficient Streaming Language Models with Attention Sinks\nMassive Activations in Large Language Models\nGemma: Open Models Based on Gemini Research and Technology\nApproximating Two-Layer Feedforward Networks for Efficient Transformers\nBetter \u0026amp; faster large language models via multi-token prediction\nProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training\n参考 # Qwen3-Next：迈向更极致的训练推理性价比\nhttps://qwen3-next.com/\nQwen3-Next：混合注意力 + 超稀疏 MoE + MTP = SOTA 推理速度\n"},{"id":84,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/V3/DeepSeek/","title":"DeepSeek V3","section":"V3","content":" 论文 # DeepSeek-V3 Technical Report\nhttps://github.com/deepseek-ai/DeepSeek-V3\n参考 # 【LLM技术报告】DeepSeek-V3技术报告（全文） 翻译\nDeepSeek-V3 关键点解读：架构篇\n【论文解读】DeepSeek-V3技术报告\n"},{"id":85,"href":"/www6vAlgo/docs/LLM/Dense/BERT/","title":"(原理)BERT","section":"Dense","content":"\nBERT # (原理)BERT\n"},{"id":86,"href":"/www6vAlgo/docs/Vision/Diffusion/gptDiffusionunCLIP/","title":"(原理)unCLIP","section":"Diffusion","content":"\nunCLIP # (原理)unCLIP\n"},{"id":87,"href":"/www6vAlgo/docs/Vision/Controllable/gptDiffusionReferenceNet/","title":"(原理|实战)ReferenceNet","section":"Controllable","content":"\nReferenceNet # (原理|实战)ReferenceNet\n"},{"id":88,"href":"/www6vAlgo/docs/Vision/Controllable/gptDiffusionIPAdapter/","title":"(原理|实战)IP-Adapter","section":"Controllable","content":"\n论文[IP-Adapter] # 论文地址 IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models 开源地址 IP-Adapter git enable a pretrained text-to-image diffusion model to generate images with image prompt 有很多notebook的demo Project page Project page IP-Adapter[10] # IP-Adapter is an image prompt adapter that can be plugged into diffusion models to enable image prompting without any changes to the underlying model. Furthermore, this adapter can be reused with other models finetuned from the same base model and it can be combined with other adapters like ControlNet. The key idea behind IP-Adapter is the decoupled cross-attention mechanism which adds a separate cross-attention layer just for image features instead of using the same cross-attention layer for both text and image features. This allows the model to learn more image-specific features.\nIP-Adapter 是一种图像提示适配器，可以插入扩散模型以启用图像提示，而无需对底层模型进行任何更改。此外，该适配器可以与从同一基本模型微调的其他模型重复使用，并且可以与其他适配器（如 ControlNet）结合使用。IP-Adapter背后的关键思想是解耦的交叉注意力机制，该机制仅为图像特征添加了一个单独的交叉注意力层，而不是对文本和图像特征使用相同的交叉注意力层。这使模型能够学习更多特定于图像的特征。\nMethod[2] # 解耦注意力机制[1]\nIPAdapter提出的解耦注意力机制decoupled cross attention是指将图像提示（image prompt）也看作是text，按照上面的公式原理计算image prompt和查询特征 的cross attention，最后将text prompt计算的cross attention和image prompt计算的cross attention相加。\n参考 # IPAdapter原理和代码解析 【腾讯】IP-Adapter论文解读，拿捏图生图，人脸更不在话下。 v 实战 # IP-Adapter diffusers 1xx. ip-adaptor on colab with diffusers 运行过没异常\n1xx. IPAdapter 多个图像风格混合到一张图像上 comfyUI工作流 v 有代码\n"},{"id":89,"href":"/www6vAlgo/docs/Vision/Controllable/gptDiffusionT2IAdapter/","title":"(原理|实战)T2I-Adapter","section":"Controllable","content":"\n论文[T2I-Adapter] # 论文地址 T2I-Adapter 开源地址 T2I-Adapter git T2I-Adapter[10] # T2I-Adapter is a lightweight adapter for controlling and providing more accurate structure guidance for text-to-image models. It works by learning an alignment between the internal knowledge of the text-to-image model and an external control signal, such as edge detection or depth estimation.\nT2I-Adapter 是一种轻量级适配器，用于控制文本到图像模型并提供更准确的结构指导。它的工作原理是学习文本到图像模型的内部知识与外部控制信号（如边缘检测或深度估计）之间的对齐。\nThe T2I-Adapter design is simple, the condition is passed to four feature extraction blocks and three downsample blocks. This makes it fast and easy to train different adapters for different conditions which can be plugged into the text-to-image model. T2I-Adapter is similar to ControlNet except it is smaller (~77M parameters) and faster because it only runs once during the diffusion process. The downside is that performance may be slightly worse than ControlNet.\nT2I-Adapter的设计很简单，条件被传递给四个特征提取模块和三个下采样模块。这使得针对不同条件训练不同的适配器变得快速而容易，这些适配器可以插入到文本到图像模型中。T2I-Adapter 类似于 ControlNet，不同之处在于它更小（~77M 参数）和更快，因为它在扩散过程中只运行一次。缺点是性能可能比 ControlNet 稍差。\nMotivation[1] # {% asset_img \u0026rsquo;\u0026rsquo; %}\nMethod[1] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n参考 # 1.【北大-腾讯最新工作】T2I-Adapter 更加可控的文本生成图像 V\n1xx. T2I-Adapter：挖掘更多SD模型的控制能力\n1xx. Efficient Controllable Generation for SDXL with T2I-Adapters\n实践 # T2I-Adapter hugggingface "},{"id":90,"href":"/www6vAlgo/docs/Vision/Controllable/gptDiffusionControlNet/","title":"(原理|实战)ControlNet","section":"Controllable","content":"\n论文[ControlNet] # 论文地址 Adding Conditional Control to Text-to-Image Diffusion Models 开源地址 ControlNet git ControlNet[10] # ControlNet is a type of model for controlling image diffusion models by conditioning the model with an additional input image. There are many types of conditioning inputs (canny edge, user sketching, human pose, depth, and more) you can use to control a diffusion model. This is hugely useful because it affords you greater control over image generation, making it easier to generate specific images without experimenting with different text prompts or denoising values as much.\nControlNet是一种模型，用于通过使用额外的输入图像调节模型来控制图像扩散模型。您可以使用**多种类型的调节输入（精巧的边缘、用户草图、人体姿势、深度等）**来控制扩散模型。这非常有用，因为它为您提供了对图像生成的更大控制，从而可以更轻松地生成特定图像，而无需尝试使用不同的文本提示或对值进行过多的去噪。\nMethod [2][3] # ControlNet 采用了一种类似微调的方法，如下图，在原模型的基础上，增加一个可训练副本，可训练副本的输入是原输入x加上条件c，然后把两个模型的输出相加，可训练副本的输入输出都经过零卷积(zero convolution)处理，用于在刚开始训练时保持模型的稳定性。\n{% asset_img \u0026rsquo;\u0026rsquo; %}\n具体的针对 Stable Diffusion 的 ControlNet 结构如下图，只复制了 UNet 的 Encoder blocks 和 Middle block (结构+权重)，控制条件图c先经过几层卷积，再与原UNet的输入zt相加作为输入，ControlNet 每个 block 的输出再 Add 到原 UNet 的 Decoder block 输入，实际实现中，ControlNet 的输出还可以乘上一个scale，用于控制影响程度。注意这里 ControlNet 同样输入了和原 UNet 一样的 Prompt\u0026amp;Time\n{% asset_img \u0026rsquo;\u0026rsquo; %}\n完整的 Diffusion + ControlNet 流程如下：\n{% asset_img \u0026rsquo;\u0026rsquo; %}\nConditional Inputs[2] # {% asset_img \u0026rsquo;\u0026rsquo; %}\nComparison with T2I-Adapter[2] # {% asset_img \u0026rsquo;\u0026rsquo; %}\nT2I-Adapter vs. Controlnet # Complexity 更高【Conditional Input可以叠加到一起】\nFlexibility 更高 【每种输入有一种Adaptor，多种Adaptor可以配合使用】\n参考 # 【AI论文精读】【图像生成】全网最详细controlnet论文逐段精读 V 【2023 ControlNet】斯坦福最新的可控文本生成图像扩散模型 V ControlNet 算法原理与代码解释 *** 优化，代码 1xx. 深入浅出完整解析ControlNet核心基础知识 ***\n不得不读 | 深入浅出ControlNet，一种基于生成扩散模型Stable Diffusion、可控生成的AIGC绘画生成算法！\n实践 # ControlNet Hugggingface control net V "},{"id":91,"href":"/www6vAlgo/docs/Vision/Diffusion/gptDiffusionGuidance/","title":"(原理)Guidance","section":"Diffusion","content":"\nGuidance # Guidance\n"},{"id":92,"href":"/www6vAlgo/docs/Vision/Diffusion/gptDiffusionXL/","title":"(原理)SD XL","section":"Diffusion","content":"\nSDXL # SDXL\n"},{"id":93,"href":"/www6vAlgo/docs/Vision/editing/gptDiffusionImageEditWork/","title":"(Work|实战)Image Editing","section":"Editing","content":"\n总结 # Prompt-to-Prompt\ntrain-free，察觉到了attention map的妙用\npix2pix-zero\n察觉到了attention map的妙用\nInstructPix2Pix\ntrainable，training数据基于Prompt-to-Prompt\nMGIE\n基于LMM\n"},{"id":94,"href":"/www6vAlgo/docs/Vision/Vision/gptVisionTask/","title":"CV 任务","section":"Vision","content":"\n分类 [1] # image-level # image recognition\n(Retrieval)image-text retrieval\nCaption(image captioning)\nVQA(visual question answering)\nregion-level # Object Detection object detection\nDETR -\u0026gt; DINO -\u0026gt; Grounding DINO dense caption\nphrase grounding\npixel-level # Segmentation generic segmetation referring segmetation 其他 # 对比\n[CNN 更深的网络] [transformer 没有局限] CV任务\n分类（Classification） 检测（Detection） 分割（Segmentation） 跟踪（Tracking） 行为识别（Action Recognition） 参考 # [CVPR Tutorial Talk] Towards General Vision Understanding Interface pdf "},{"id":95,"href":"/www6vAlgo/docs/Vision/gptDiffusionDiT/","title":"(原理|实战)DiT","section":"Vision","content":"\n论文 # 论文地址\nScalable Diffusion Models with Transformers\n开源地址 Scalable Diffusion Models with Transformers git\nDiT Repo git\nArch # code[10] # DiT\ndit git DiTBlock\ndit_block git 参考 # 1xx. AI大讲堂：文生视频谁能敌？专业拆解【DiT模型】 V\nDiT原文: https://arxiv.org/abs/2212.09748\nCode: https://github.com/facebookresearch/DiT\nHuggingface: https://huggingface.co/spaces/wpeebles/DiT\n1xx. 14步手搓sora!Diffusion Transformer, DiT工作原理 V\n实战 # 【Sora重要技术】复现DiT（Diffusion Transformer）模型 V ***\ndits Repo "},{"id":96,"href":"/www6vAlgo/docs/Vision/Controllable/gptDiffusionFineTuning/","title":"(work|实战) fine-tuning","section":"Controllable","content":"\n对比总结[1] # 训练方法 方法 局限性 Text Inversion 使用提供的一组图片训练一个新单词的Embedding ，并将其与词汇表中的已有单词关联起来，这个新单词即为这组图片概念的指代。 训练过程只对应 Embedding，扩散模型没有新知识输入，所以也无法产生新的内容。 Full FineTune 最朴素的方式，使用图片+ 标注的数据集，进行迭代训练，数据集标注可以选择BLIP来生成。训练直接对原模型的所有权重进行调整。 容易过拟合，导致生成图片的多样性不够，结果难以控制。模型体积大，不便于传播。 Dreambooth 提供代表某个新概念（instance） 对应的一组图像，并使用罕见字符（identifier） 进行概念Mapping，训练过程充分考虑原有相关主题（class）生成，避免过拟合。训练直接对原模型的所有权重进行调整。 训练过程只针对新概念 （instance），多样性差。如果需要多概念生成，需要多次训练。模型体积大，不便于传播。 LoRA（w Dreambooth） 冻结预训练模型参数，在每个Transformer块插入可训练层，不需要完整调整 UNet 模型的全部参数。训练结果只保留新增的网络层，模型体积小。 训练效果不如Dreambooth 对比总结[2] # 对比总结[3] # 参考 # Stable Diffusion 微调及推理优化 【论文串读】Stable Diffusion模型微调方法串读 V Stable Diffusion——四种模型 LoRA（包括LyCORIS）、Embeddings、Dreambooth、Hypernetwork 实战 # 1xx. Text Inversion V 【这个比较详细】 1xx. lora Dreambooth V 【冻结不训练unet，只训练lora】 【为unet模型添加注意力层，注意力层是要训练的参数】 【大部分代码和Dreambooth差不多】\n实战 # 1xx. + concept customization dreambooth lora + textual_inversion diffusers\n1xx. 手把手教你微调Stable Diffusion * lora on DreamBooth\n"},{"id":97,"href":"/www6vAlgo/docs/Vision/editing/gptDiffusionImageEdit/","title":"(综述)Image Editing","section":"Editing","content":"\n目录 # 论文 # 论文地址 《Diffusion Model-Based Image Editing: A Survey》\n开源地址 Repo git\n图像编辑[1] # 大类 # 从图片编辑的任务方面可以被分为3个大类 语义编辑semantic editing 风格编辑stylistic editing 结构编辑structural editing APPROACHES # TRAINING-BASED APPROACHES\nInstructPix2Pix TESTING-TIME FINETUNING APPROACHES\nTRAINING AND FINETUNING FREE APPROACHES\n论文[2] # 论文地址 《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》 复旦、南洋理工\n开源地址 A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models\nSurvey[2] # 参考 # survey # 《Diffusion Model-Based Image Editing: A Survey》 论文阅读：Diffusion Model-Based Image Editing: A Survey 基于扩散模型的图像编辑：首篇综述\n《A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models》 300多篇相关研究，复旦、南洋理工最新多模态图像编辑综述论文\n1xx. 《LLMs Meet Multimodal Generation and Editing: A Survey》 * Image Generation， Image Editing Repo\n"},{"id":98,"href":"/www6vAlgo/docs/Vision/Multimodal-Agent/gptAgentMultimodalApp/","title":"Agent - UI-assistants","section":"Multimodal Agent","content":"\n参考 # App Agent # 1xx. AppAgent源码分析\u0026amp;思考 https://github.com/mnotgod96/AppAgent https://icoz69.github.io/\n1xx. 【LLM-agent】MOBILE-AGENT: 具有视觉感知能力的自治多模移动设备agent https://github.com/X-PLUG/MobileAgent\n1xx. https://github.com/OpenAdaptAI/OpenAdapt\n"},{"id":99,"href":"/www6vAlgo/docs/Vision/Diffusion/gptMultimodalDiffusion/","title":"(原理)Stable Diffusion","section":"Diffusion","content":"\nStable Diffusion 原理 # (原理)Stable Diffusion\n"},{"id":100,"href":"/www6vAlgo/docs/Vision/Data/gptDatasetMulitmodal/","title":"(survey)多模态  数据集","section":"Data","content":"\n目录 # Survey[0] # Pre-training Adaptation Pre-training数据集 # LAION[1] LAION\nwukong[1] [论文]中文多模态数据集WuKong \u0026amp; FILIP \u0026amp; LiT-tuning Wukong：一亿规模的中文跨模态预训练基准\nMMDialog 百万量级的多模态对话数据集来了，153万张图片4000多主题\nOBELISC[2]\nShareGPT4V[3] opensource\nSFT数据集 # LAMM MultiIntruct 参考 # survey # 多模态模型大常用数据集及处理策略：兼看Chatlaw法律问答中的知识图谱融合思路 《A Survey of Multimodal Large Language Model from A Data-centric Perspective》 预训练数据集 # 多模态数据集收集\n[论文阅读] 开源的多模态文档数据集，OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents\n从网页文档里得到的数据集\n超越同级7B模型！ 中国团队开源大规模高质量图文数据集ShareGPT4V，大幅提升多模态性能 ShareGPT4V git\n1xx. 多模态预训练数据集\n1xx. OpenDataLab\nSFT数据集 # 1xx. 【LMM 015】LAMM：多模态指令微调数据集，框架和基准 1xx. [NeurIPS2023] LAMM：多模态指令微调数据集、框架、评测基准\n1xx. Talk | ACL'23 杰出论文，MultiIntruct：通过多模态指令集微调提升VLM的零样本学习 1xx. 【ACL2023】MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning\n"},{"id":101,"href":"/www6vAlgo/docs/Vision/Survey/gptMultimodalSurvey/","title":"(Survey)多模态","section":"Survey","content":"\n目录 # 论文 # 论文地址 《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》 .Sep 2023 - microsoft\n开源地址 Computer Vision in the Wild (CVinW)\noverview [0] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n视觉理解 [1] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n{% asset_img \u0026rsquo;\u0026rsquo; %}\n视觉生成 [1] # Human Alignments in Visual Generation [10] # 四种alignment的方式\nspatial controllable T2I generation # 结合位置分布的文字描述（比较麻烦的用户交互，不仅需要文字，而且需要位置），常用于对位置要求比较高的创意设计（海报等）\n直接讲原来clip那种image-level的text description升级为基于区域的text description\nreco gligen 将box描述变为spatial condition\ncontrolnet 无需fintinue，直接变为inference-guide\nuniversal guidance for diffusion model text-based editing # 给一张图和对应的修改文字，输出要求的图，常用于ps等产品\ndiffusion process manipulations\npromot2promot text instruction editing\nInstructPix2Pix Editing with external pre-trained models\ntext promots following # 直接给文字描述，生成对应的图，这个是目前常见文生图产品的交互方式，常用于c端或者b端用户图像内容生成。但其对更细节的控制存在一定的难度 Inference-time manipulation StructureDiffusion Attend-and-Excite Model tuning to follow text prompt ddpo concept customization # 给一张图，提取图片中的关键内容，做各种风格（背景/动作）变换，更用于不那么精细的广义产品，比2的运用范围更加广义\nConcept Customization\nTextual Inversion [DreamBooth] Multi-concept customization\nCustom Diffusion Customization without test-time finetuning\nSuTI {% asset_img \u0026rsquo;\u0026rsquo; %}\n{% asset_img \u0026rsquo;\u0026rsquo; %}\nText-to-Image Generation 技术流派（4类） # Generative adversarial networks (GAN) Variational autoencoder (VAE) Discrete image token prediction Diffusion model 统一的视觉模型[2] # 端到端的方式训练LLM[2] # 多模态 Agent[3] # 参考 # 翻译 # 《Multimodal Foundation Models:From Specialists to General-Purpose Assistants》\nAGI之MFM：《Multimodal Foundation Models: From Specialists to General-Purpose Assistants多模态基础模型：从专家到通用助 翻译\nAGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之视觉理解、视觉生成\nAGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之统一的视觉模型、加持LLMs的大型多模态模型\nAGI之MFM：《多模态基础模型：从专家到通用助手》翻译与解读之与LLM协同工作的多模态智能体、结论和研究趋势\n解读 # 大模型系列04 -文本图像生成 1xx. Multimodal Foundation Models: From Specialists to General-Purpose Assistants\n1xx. 对应第二章节 《Alignments in Text-to-Image Generation》 [CVPR2023 Tutorial Talk] Alignments in Text-to-Image Generation V\n1xx. 对应第三章节 《From Specialist to Generalist: Towards General Vision Understanding Interface》 [CVPR Tutorial Talk] Towards General Vision Understanding Interface\n"},{"id":102,"href":"/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalBlip/","title":"(图生文)BLIP-2, Flamingo","section":"端到端训练LLM(LMM)","content":"\n目录 # BLIP-2 # Overview [1] # 用一个Qformer来提取图像特征（等同与Flamingo的perceiver resampler），然后用cross- attention进行多模态交互，此时视觉编码器和LLM都会被冻结，只训练Qformer，而在下游任务微调时，可以再解锁视觉编码器，让它跟Qformer一起训练\n两阶段的训练策略 [1] # BLIP-2设计了两阶段的训练策略，以使视觉编码器能学会提取更关键的信息。\n第一阶段：使用多种预训练任务，如Image-Text Contrastive Learning(ITC)，Image-grounded Text Generation(ITG)，Image-Text Matching(ITM)让Qformer学会如何从视觉编码器中抽取文本相关的特征。 第二阶段，将Qformer插入到LLMs中，用language modeling进行训练。 架构[3] # 两个阶段训练 阶段一 获得高质量的 图文对齐向量表征 通过ITC ITM ITG 三个损失函数获得了很好的图片文本 对齐向量表征能力，仅训练Qformer中很少的参数 【ITM: image-text 是否是匹配的 | image 和text 都能相互看到】 【ITG: image生成text | image 能全看到, text只能逐个的看】 【ITC: image和text的对比学习, 对比学习分类分错了的 送入ITM 负样本 | image和 text 之间是不能看到的】 阶段二 通过向量表征进行文字生成 code [2] # Flamingo[1] # 架构 # 它在Frozen模型的基础上做进一步的改进，不同点主要有两个：一是使用了更大的LLMs，二是冻结视觉编码器，引入perceiver resampler和XAttn-Dense两个适配单元作为可训练的模块。\nperceiver resampler： 类似DETR，通过设计多个Perceiver Resampler来生成64个固定长度的tokens，主要作用在于可以从图像中提取固定长度的特征向量，能够解决图像甚至多帧视频的feature map不一致的问题。【图像和文本对齐】 XAttn-Dense：在每一层LLM上都会增加corss- attention以入到LLM中与视觉向量进行交互，融合多模态信息。【融合】 参考 # blip2 # 基于LLMs的多模态大模型（Flamingo, BLIP-2，KOSMOS-1，ScienceQA）\nblip2 git blip2_instructed_generation git 运行过\n强推！科大讯飞和中科院终于把多模态大模型讲明白了，CLIP、blip、blip2三种模型原理一口气学完 V ***\n1xx. AI论文精读之多模态大模型BLIP-2 V\n1xx. MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践 *\n1xx. BLIP2：下一代多模态模型的雏形\nFlamingo # 1xx. [论文速览]Flamingo: a Visual Language Model for Few-Shot Learning[2204.14198] V 1xx. DeepMind出手！多模态小样本打败精调 1xx. Otter on OpenFlamingo git 1xx. open_flamingo git\n"},{"id":103,"href":"/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalInstructTuning/","title":"(综述)多模态InstructTuning","section":"端到端训练LLM(LMM)","content":"\n目录 # Datasets for Visual Instruction Tuning[1] # Single-turn # MiniGPT-4 MiniGPT-4 [37] curates an image description dataset that contains 3439 image-text pairs for instruction fine-tuning. MiniGPT-4 randomly selects 5000 images from the Conceptual Caption dataset [38], [39] and prompts its pre-trained VLM model to generate detailed descriptions for each image. The generated descriptions are then** refined and filtered** both manually and by using ChatGPT, resulting in 3439 highquality image-text pairs.\nMultiInstruct MultiInstruct [43] build a comprehensive instruction dataset that covers 62 diverse multimodal tasks from 10 broad categories, such VQA, Image-text matching, grounded generation, and so on. These tasks include 34 existing tasks derived from 21 public dataset and 28 new tasks extended from them. Each task is equipped with 5 instruction templates to prompt the model to perform the specific task.\nMulti-turn # LLaVA LLaVA-Instruct-158k [9] contains 158 image-text instruction data, including 58k conversation data asking about the visual content of the image,23k description data, and 77k complex reasoning data where the question may involve multi-step reasoning process. VLIT Data Construction Strategy[2] # Annotation Adaption # MiniGPT-4 Self-Instruct # LLaVA High-Quality VLIT Data[2] # Correctness # Diversity # Complexity # Method [1][2] # Method Training Paradigm[2] Vision Encoder Language Encoder Inst[2] Tuning Data MiniGPT-4 FA → VLIT EvaCLIP ViT Vicuna AA CC3M, CC12M, SBU, LAION 400M, MiniGPT-3.5K MiniGPT-v2 EVA LLaMA2-chat AA+SI LAION, CC3M, SBU, GRIT-20M, COCO caption, Text Captions, RefCOCO, RefCOCO+, RefCOCOg, GQA, VQA-v2, OCR-VQA, OKVQA, AOK-VQA, Flickr30k Dataset, Unnatural Instruction Dataset LLaVa FA → VLIT CLIP ViT Vicuna SI CC3M Concept-balanced 595K, LLaVA-Instruct-158K LLaVA-1.5 FA → VLIT CLIP ViT Vicuna LLaVA, ShareGPT, VQAv2, GQA, OKVQA, OCRVQA, A-OKVQA, TextCaps, RefCOCO, VG MultiInstruct VLIT OFA OFA AA VQAv2, Visual7w, GQA, OK-VQA, Visual Genome, MSCOCO, RefCOCO, COCO-Text, TDIUC, IQA, VAW, MOCHEG, WikiHow Otter CLIP ViT MPT SI MIMIC-IT LAMM VLIT CLIP ViT-L/14 Vicuna SI Language-Assisted Multi-Modal Instruction-Tuning Dataset Qwen-VL FA → VLIT(Multi-Task Tuning) ViT Qwen-7B LAION-en\u0026amp;zh, DataComp, Coyo, CC12M\u0026amp;3M, SBU, COCO, In-house Data, GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA, TextVQA, ChartQA, AI2D, GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg, SynthDoG-en\u0026amp;zh, Common Crawl pdf\u0026amp;HTML CogVLM FA → VLIT EVA2-CLIP-E Vicuna-7Bv-1.5 VQAv2, TextVQA StableLLaVA FA → VLIT CLIP-ViT-L/14 LLaMA AA Synthesized Image-Dialogue Dataset Annotation Adaption-\u0026gt; SI # Self-Instruct -\u0026gt; AA # 参考 # 《Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey》 *** 第4 5章 南洋大学\n《Vision-Language Instruction Tuning: A Review and Analysis》 *** 第2 3 4 5章 腾讯\n《Instruction Tuning for Large Language Models: A Survey》 第5章\n"},{"id":104,"href":"/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalMinigpt4/","title":"(原理|实战)MiniGPT4","section":"端到端训练LLM(LMM)","content":"\n目录 # INTRODUCTION[1] # MiniGPT-4 增加了一个投影层，将编码的视觉特征与 Vicuna 语言模型对齐，并冻结了所有其他视觉和语言组件\nMETHOD[1] # 图 1\nMiniGPT-4 的目标是将来自预训练视觉编码器的视觉信息与先进的大型语言模型（LLM）对齐（Alignment）。具体来说，\n使用 Vicuna作为语言解码器，该解码器基于 LLaMA构建，可以执行各种复杂的语言任务。 视觉感知方：采用与 BLIP-2 相同的视觉编码器，ViT Backbone及其预先训练好的 Q-Former。 语言和视觉模型都是开源的。我们的目标是利用线性投影层弥合视觉编码器与 LLM 之间的差距，图 1 显示了模型概览。 FIRST PRETRAINING STAGE # 第一阶段：在大量对齐的图像-文本对上对模型进行预训练，以获取视觉语言知识。\nTraditional alignment method [2]\nInput: Image Output: Caption Training Objective: Maximize the likelihood of GT captions Training Dataset 组合数据集 [postprocessed by BLIP] Conceptual Caption SBU LAION CURATING A HIGH-QUALITY ALIGNMENT DATASET FOR VISION-LANGUAGE DOMAIN. # Create a dataset with detailed, human-perfered descriptions[2][1] model generates descriptions 在初始阶段，我们使用从第一个预训练阶段得到的模型来生成输入图像的描述。 polishing and filtering by chatgpt 上述自动生成的图片说明包含噪音或不连贯的描述，例如单词或句子重复，句子支离破碎或内容不相关。为了解决这些问题，我们采用了ChatGPT，通过以下提示对描述进行修补 further polishing and filtering by rules \u0026amp; human 完成后处理阶段后，我们会手动验证每张图片说明的正确性，以保证其高质量。 SECOND-STAGE FINETUNING # 第二阶段：使用一个较小但高质量的图像-文本数据集对预训练模型进行微调，并设计了对话模板，以提高生成的可靠性和可用性。 【blip2能识别图像，但是对话能力比较弱，不能说出图像中的细节。在pre-train阶段获取视觉语言知识， 在fine-tuning 阶段获取对话能力】 [2]\n参考 # 原理 # 【LMM 009】MiniGPT-4：使用 Vicuna 增强视觉语言理解能力的多模态大模型 *** MiniGPT-4、表格推理、代码生成、生成式推理-来自斯坦福、北大、阿卜杜拉、达摩院的四位论文一作思辨大模型 V 1xx. miniGPT4：多模态图文理解训练 V 1xx. MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践 1xx. 使用大型语言模型为MiniGPT-4构建视觉语言理解能力 V 实战 # 1xx. 大杀器，多模态大模型MiniGPT-4入坑指南\n"},{"id":105,"href":"/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalLlava/","title":"(原理|实战) LLaVa 演化","section":"端到端训练LLM(LMM)","content":"\nLLaVa 演化 # (原理|实战) LLaVa 演化 LLaVa 实战 # (实战) LLaVa "},{"id":106,"href":"/www6vAlgo/docs/Vision/Multimodal-Agent/gptAgentWeb/","title":"(原理)Web Agent","section":"Multimodal Agent","content":"\nweb scenarios [1] # 在网络场景中，代表用户执行特定任务被称为Web导航问题[390]。代理程序解释用户指令，将其分解为多个基本操作，并与计算机进行交互。这通常涉及到填写表单、在线购物和发送电子邮件等网络任务。代理程序需要具备理解复杂网络场景中的指令的能力，适应变化（如嘈杂的文本和动态HTML网页），并推广成功的操作[391]。通过这种方式，代理程序可以在处理未知任务时实现可访问性和自动化[435]，最终使人类免于与计算机用户界面的重复交互。\n通过强化学习训练的代理程序可以有效地模仿人类行为，使用预定义的操作，如键入、搜索、导航到下一页等。它们在基本任务（如在线购物[392]和搜索引擎检索[90]）中表现良好，这些任务已经得到广泛探索。然而，没有语言模型能力的代理程序可能难以适应现实世界互联网中更真实和复杂的场景。在动态、内容丰富的网页上，如在线论坛或在线业务管理[391]，代理程序常常面临性能方面的挑战。\n为了实现代理程序与更真实的网页之间的成功交互，一些研究人员[393；394]开始利用语言模型的强大HTML读取和理解能力。通过设计提示，他们试图使代理程序理解整个HTML源代码，并预测更合理的下一步操作。Mind2Web[389]结合了为HTML进行微调的多个语言模型，使它们能够在现实世界的场景中总结冗长的HTML代码[388]并提取有价值的信息。此外，WebGum[390]通过使用包含HTML截屏的多模态语料库，赋予代理程序视觉感知能力。它同时进行了语言模型和视觉编码器的微调，加深了代理程序对网页的全面理解。\nPerforming specific tasks on behalf of users in a web scenario is known as the web navigation problem [390]. Agents interpret user instructions, break them down into multiple basic operations, and interact with computers. This often includes web tasks such as filling out forms, online shopping, and sending emails. Agents need to possess the ability to understand instructions within complex web scenarios, adapt to changes (such as noisy text and dynamic HTML web pages), and generalize successful operations [391]. In this way, agents can achieve accessibility and automation when dealing with unseen tasks in the future [435], ultimately freeing humans from repeated interactions with computer UIs.\nAgents trained through reinforcement learning can effectively mimic human behavior using predefined actions like typing, searching, navigating to the next page, etc. They perform well in basic tasks such as online shopping [392] and search engine retrieval [90], which have been widely explored. However, agents without LLM capabilities may struggle to adapt to the more realistic and complex scenarios in the real-world Internet. In dynamic, content-rich web pages such as online forums or online business management [391], agents often face challenges in performance.\nIn order to enable successful interactions between agents and more realistic web pages, some researchers [393; 394] have started to leverage the powerful HTML reading and understanding abilities of LLMs. By designing prompts, they attempt to make agents understand the entire HTML source code and predict more reasonable next action steps. Mind2Web [389] combines multiple LLMs fine-tuned for HTML, allowing them to summarize verbose HTML code [388] in real-world scenarios and extract valuable information. Furthermore, WebGum [390] empowers agents with visual perception abilities by employing a multimodal corpus containing HTML screenshots. It simultaneously fine-tunes the LLM and a visual encoder, deepening the agent’s comprehensive understanding of web pages.\npapers [2] # In web scenarios\n[2023/10] OpenAgents: An Open Platform for Language Agents in the Wild. XLang Lab (The University of Hong Kong) arXiv. [paper] [project page] [code] [demo] *** [2023/07] WebArena: A Realistic Web Environment for Building Autonomous Agents. Shuyan Zhou (CMU) et al. arXiv. [paper] [code] * [2023/07] A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. Izzeddin Gur (DeepMind) et al. arXiv. [paper] [2023/06] SYNAPSE: Leveraging Few-Shot Exemplars for Human-Level Computer Control. Longtao Zheng (Nanyang Technological University) et al. arXiv. [paper] [code] * [2023/06] Mind2Web: Towards a Generalist Agent for the Web. Xiang Deng (The Ohio State University) et al. arXiv. [paper] [code] *** [2023/05] Multimodal Web Navigation with Instruction-Finetuned Foundation Models. Hiroki Furuta (The University of Tokyo) et al. arXiv. [paper] [2023/03] Language Models can Solve Computer Tasks. Geunwoo Kim (University of California) et al. arXiv. [paper] [code] [2022/07] WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. Shunyu Yao (Princeton University) et al. arXiv. [paper] [code] * [2021/12] WebGPT: Browser-assisted question-answering with human feedback. Reiichiro Nakano (OpenAI) et al. arXiv. [paper] [2023/05] Agents: An Open-source Framework for Autonomous Language Agents. Wangchunshu Zhou (AIWaves) et al. arXiv. [paper] [code] *** 参考 # 《The Rise and Potential of Large Language Model Based Agents: A Survey》、 The Rise and Potential of Large Language Model Based Agents: A Survey git 1xx. Multimodal Web Navigation with Instruction-Finetuned Foundation Models 1xx. Google DeepMind｜具备规划长程上下文理解和程序合成能力的真实世界WebAgent 1xx. LLMs-Agent 论文: WebAgent, 2023, Izzeddin Gur et al., Google DeepMind. 1xx. OpenAgents web agent # 1xx. WebVoyager：借助强大多模态模型，开创全新的网络智能体 [译]\n"},{"id":107,"href":"/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalPretrain/","title":"(原理)多模态预训练 概述","section":"端到端训练LLM(LMM)","content":"\n目录 # Overview # {% asset_img \u0026rsquo;\u0026rsquo; %}\n多模态预训练 # 数据集 # 大规模无标注 内容杂 噪音多 架构Transformer # 基于transformer encoder-理解任务 单流 - vl-bert UNITER 双流 - ViLBERT， CLIP（双流结构，对比学习）\n基于transformer decoder-生成任务 DALL-E （VQVAE+GPT, Text-to-Image Generation） 现在都用 → SD 扩散模型\n基于encoder+decoder-理解+生成 文本的decoder\nencoder + decoder 串行, 交叉注意力 encoder + decoder 并行 模型 - 自监督学习 # 模态内掩码学习 文本 语音 视觉自身token级别mask\n模态间掩码学习 不同模态信息的相互预测 mask视觉， 输出对应文本\n模态间匹配学习 匹配与否的分类问题 - 正负样本(二分类) 对比学习 - 模态间的图文匹配对\n下游任务 # 多模态下游任务-模型微调 # 模型微调\np+ finetune（全参数） p+ prompt-tuning p+ adaptor-tuning p+ lora 多模态下游任务\n理解： text/audio/visual 内容生成 生成： 跨模态 检索/问答/推理 更大更强的多模态预训练模型 # 强大的语言模型 更大的视觉模型 更大规模的预训练数据 更多模态形式的数据 参考 # 中科院刘静：多模态预训练的进展回顾与展望 V "},{"id":108,"href":"/www6vAlgo/docs/Vision/Multimodal-Agent/gptAgentMultimodal/","title":"(原理)Agent 多模态","section":"Multimodal Agent","content":"\n目录 # 论文 # 论文地址 《Large Multimodal Agents: A Survey》\n开源地址 Repo git\nSurvey # 类型 I：无长期记忆的闭源 LLMs 作为规划器。 # Visual ChatGPT ***\nMM-REACT ***\nViperGPT\nHuggingGPT ***\nChameleon ***\nLLaVA-Interactive ***\nSeeAct\nGPT-Driver\nMobile-Agent\n类型 II：无长期记忆的微调 LLMs 作为规划器。 # LLaVA-Plus\nGPT4Tools\n类型 IV：具有本地长期记忆的规划器。 # JARV IS-1\nAppAgent\nDLAH\n多模态 Agent[1] # 核心组件\n感知组件关注处理多模态信息 规划器负责推理和制定计划 行动组件执行计划 记忆组件则涉及长期和短期记忆 四种类型\n无长期记忆的闭源 LLMs 作为规划器 无长期记忆的微调 LLMs 作为规划器 具有间接长期记忆的规划器 具有本地长期记忆的规划器 多智能体协作\n讨论了 LMAs 如何通过协作框架共同实现共同目标。 多模态 Agent[10] # 范式 # {% asset_img \u0026rsquo;\u0026rsquo; %}\nMM-ReAct\nHuggingGPT[21, 22]\nChameleon\nVisual ChatGPT [20]\nworks # {% asset_img \u0026rsquo;\u0026rsquo; %}\n参考 # 综述 # 2024年大型多模态智能体(Large Multimodal Agents)综述：组件, 分类，协作，评估，应用，展望 *** 1xx. 智体AI在多模态交互领域的综述（上） 1xx. 智体AI在多模态交互领域的综述（下）\nxxx # 多模态 Agents：用大模型语言模型串联多模态专家 V 多模态Agent # 1xx. {% post_link \u0026lsquo;gptMultimodal\u0026rsquo; %} self 1xx. {% post_link \u0026lsquo;gptMultimodalSurvey\u0026rsquo; %} self\nxxx # 《Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models》 Visual ChatGPT git\nLLMs的自动化工具系统（HuggingGPT、AutoGPT、WebGPT、WebCPM）\nHuggingGPT git hugginggpt in langchain git langchain-huggingGPT git\n1xx. Visual Programming——实现通用人工智能的另一种方式 2022 best paper\n"},{"id":109,"href":"/www6vAlgo/docs/Vision/Survey/gptMultimodal/","title":"(综述)多模态","section":"Survey","content":"\n目录 # 论文[Foundational Models Defining] # 论文地址 《Foundational Models Defining a New Era in Vision: A Survey and Outlook》大学 基础模型分类 [1] # 分类 {% asset_img \u0026rsquo;\u0026rsquo; %} 分类 {% asset_img \u0026rsquo;\u0026rsquo; %} textually prompted models # contrastive CLIP 双塔 generative Flamingo hybrid BLIP conversational GPT-4， miniGPT4, LLaVa 传统上，视觉语言模型主要用于需要同时理解视觉和文本模态的任务。然而，随着CLIP展示出的卓越性能，基于语言监督的模型在显著上升，并成为主流方法。在本节中，我们专注于探索依赖语言作为主要监督来源的方法。这些以文本为提示的模型可以广泛分为三种主要类型：对比、生成和混合方法。\nvisually prompted models # Foundational SAM heterogeneous models # 架构 [1] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n论文[MM-LLMs] # 论文地址 《MM-LLMs: Recent Advances in MultiModal Large Language Models》 腾讯\n开源地址 mm-llms 腾讯\n解析 解析\n论文[MLLM] # 论文地址 A Survey on Multimodal Large Language Models A Survey on Multimodal Large Language Models 中国科学技术大学 腾讯\n开源地址 Repo\nArch [3.2] # {% asset_img \u0026rsquo;\u0026rsquo; %}\n类型[3.1] # 本文将最近具有代表性的MLLM分为4种主要类型： 多模态指令调整（MIT） 多模态上下文学习（M-ICL） 多模态思想链（M-CoT） LLM辅助视觉推理（LAVR）【类似agent】 参考 # survey # 《Foundational Models Defining a New Era in Vision: A Survey and Outlook》 视觉大模型的全面解析 基础模型定义视觉的新时代：综述和展望 万字长文带你全面解读视觉大模型\nxxx\n《A Survey on Multimodal Large Language Models》 v1 v2版本 3.1 MLLM首篇综述 | 一文全览多模态大模型的前世、今生和未来 v1版本 3.2 多模态大语言模型全面综述：架构，训练，数据，评估，扩展，应用，挑战，机遇 v2版本\n"},{"id":110,"href":"/www6vAlgo/docs/Vision/Controllable/gptDiffusionControllableWork/","title":"(Work|实战)Controllable","section":"Controllable","content":" 实战 # ControlNet + t2i_adapter diffusers Custom Diffusion diffusers 总结[metaso] # 功能定位 性能与效率 应用场景 总结 ControlNet 主要用于对图像生成过程中的特定部分进行控制 较大且可能需要较多计算资源 适用于需要对图像特定区域进行精细控制的场景 【ControlNet 结构控制， image2image】 T2I-Adapter 专注于将文本提示转换为图像 较小且更高效，适合资源受限的环境 适用于需要从文本描述生成图像的场景 【T2I-Adapter 多种控制, text2image】 IP-Adapter 用于分析图像提示并提取特征，再将其用于图像生成 在图像质量和对齐方面表现优异 适用于需要结合图像和文本提示进行复杂图像生成的场景 【IP-Adapter 风格特征控制, text2image|image2image 】 "},{"id":111,"href":"/www6vAlgo/docs/Vision/Vision-Encoder/gptMultimodalCLIP/","title":"(原理)CLIP","section":"Vision Encoder","content":" CLIP 在训练过程中做了哪些事情？[Elmo][1] # 在训练过程中，CLIP（Contrastive Language-Image Pre-training）模型主要进行了以下几个步骤：\n数据预处理 : CLIP 使用了一个大规模的数据集，包含 4 亿个 “图像 - 文本” 对，这些数据需要进行清洗和预处理，以便于模型学习。 特征提取 : 通过 Text Encoder 和 Image Encoder 分别对文本和图像进行特征提取。Text Encoder 通常是基于 Transformer 的模型，而 Image Encoder 可以是基于 CNN（卷积神经网络）或者 VIT（Vision Transformer）的模型。 对比学习 : CLIP 采用对比学习的策略，通过对比正确的图像 - 文本对与错误的图像 - 文本对，使得模型能够学习到正确对的特征表示与其他对的区分。具体来说，CLIP 通过 InfoNCE 损失函数来最大化正确对的相似度，同时最小化错误对的相似度。 特征空间对齐 : 通过对比学习，CLIP 将图像和文本的特征映射到一个共享的多模态特征空间中，使得图像特征和文本特征可以直接进行相似度比较。 参数优化 : 通过反向传播和梯度下降等方法，不断调整模型参数，以最小化损失函数，从而优化模型性能。 Zero-shot 推理能力的培养 : 在训练过程中，CLIP 学习了如何通过文本提示（prompts）来进行 zero-shot 的图像分类，即在没有直接观测到的类别标签下，通过文本描述来识别图像内容。 模型评估与调整 : 通过在验证集上的评估，调整模型结构和超参数，以提高模型的泛化能力和性能。 通过这些训练步骤，CLIP 能够学习到一个强大的多模态特征表示，使其能够在多种视觉任务上进行 zero-shot 或 few-shot 的推理。\nCLIP Zero-shot推理[1] # 步骤 # 首先，我们创建一个标签全集，如图中（2）所示，并得到每一个标签的特征向量 然后，我们取一张图片，如图中（3）所示，过Image Encoder后得到该图片的特征向量 最后，计算图片向量和文字向量间的相似度，取相似度最高的那条label即可。 局限 # 当你喂给CLIP一张图时，不管这张图片它是否有见过，CLIP都不会生成一个全新的标签，而是去全集标签中找一个最相似的给你。\n参考 # 关于多模态经典之作CLIP，还有哪些细节是你不知道的 代码 1xx. 神器CLIP：连接文本和图像，打造可迁移的视觉模型 ***\n1xx. 【CLIP系列Paper解读】CLIP: Learning Transferable Visual Models From Natural Language Supervision ***\n1xx. CLIP 模型解读\n1xx. 详解CLIP (一) | 打通文本-图像预训练实现ImageNet的zero-shot分类，比肩全监督训练的ResNet50/101\n1xx. openai多模态大模型：clip详解及实战\n1xx. CLIP：多模态领域革命者\n"},{"id":112,"href":"/www6vAlgo/docs/Vision/Connector/","title":"(原理)Connector","section":"Vision","content":" Input Projector[11] Connector[10] # Input Projector输入投影器 Cross-attention Flamingo, Owl, Qwen-VL Q-Former BLIP2, InstructBLIP, MiniGPT-4, MiniGPT-5 MLP[10] CogVLM , LLaVa1.5 Linear Project LLaVa, PaLI-x, MiniGPT-v2 Perceiver resampler[10] Flamingo CNN [10] DocOwl 1.5[H-Reducer?] MLP[1] # 定义 # 多层感知机（MLP，Multi-Layer Perceptron）属于前馈神经网络（Feedforward Neural Network）的一种。在模型 训练过程中，需要通过反向传播算法计算梯度，将误差从输出层反向传播回输入层，用于更新网络参数。它包含至少三层节点：一个输入层，一个或多个隐藏层，以及一个输出层。每一层的节点都全连接到下一层的每个节点。MLP 模型通常用于解决分类和回归问题。\nPyTorch代码 [2] # class MLP(nn.Module): # 继承自 nn.Module，这是所有 PyTorch 模型的基类。 def __init__(self, vocab_size, context_length, embedding_size, hidden_size, rng): # 接受五个参数：vocab_size（词汇表大小）、context_length（上下文长度）、 ## embedding_size（嵌入层大小）、hidden_size（隐藏层大小）和 rng（随机数生成器）。 # 调用 super().__init__() 初始化父类 nn.Module。 super().__init__() # 定义一个嵌入层 self.wte，使用 nn.Embedding 将输入的 token 索引转换为嵌入向量。 ## vocab_size 是词汇表的大小，embedding_size 是嵌入向量的维度。 self.wte = nn.Embedding(vocab_size, embedding_size) # 使用 nn.Sequential 定义一个多层感知机（MLP）： # self.mlp = nn.Sequential( nn.Linear(context_length * embedding_size, hidden_size), # 第一层全连接层，将输入的上下文嵌入向量映射到隐藏层。 nn.Tanh(), # # Tanh 激活函数。 nn.Linear(hidden_size, vocab_size) # 第二层线性层，将隐藏层的输出映射到词汇表大小的输出。 ) self.reinit(rng) # 调用 reinit 函数，使用自定义的随机数生成器 rng 初始化权重。 @torch.no_grad() def reinit(self, rng): # 定义 reinit 函数，并使用 @torch.no_grad() 装饰器，表示在这个函数中不需要计算梯度。 def reinit_tensor_randn(w, mu, sigma): # 以正态分布 N(mu, sigma) 初始化张量 w 的权重。 winit = torch.tensor(rng.randn(w.numel(), mu=mu, sigma=sigma)) w.copy_(winit.view_as(w)) def reinit_tensor_rand(w, a, b): # 以均匀分布 U(a, b) 初始化张量 w 的权重。 winit = torch.tensor(rng.rand(w.numel(), a=a, b=b)) w.copy_(winit.view_as(w)) # Let\u0026#39;s match the PyTorch default initialization: # 以均值为0、标准差为1的正态分布初始化嵌入层 self.wte 的权重。 reinit_tensor_randn(self.wte.weight, mu=0, sigma=1.0) scale = (self.mlp[0].in_features)**-0.5 # 算第一层全连接层的缩放系数 scale，其值为输入特征数量的负平方根。 # 以均匀分布 U(-scale, scale) 初始化第一层全连接的权重和偏置。 reinit_tensor_rand(self.mlp[0].weight, -scale, scale) reinit_tensor_rand(self.mlp[0].bias, -scale, scale) # 对于第二层全连接层的处理同上 scale = (self.mlp[2].in_features)**-0.5 reinit_tensor_rand(self.mlp[2].weight, -scale, scale) reinit_tensor_rand(self.mlp[2].bias, -scale, scale) def forward(self, idx, targets=None): # 与 MLPRaw 类的 forward 函数基本相同，但更简洁。 B, T = idx.size() emb = self.wte(idx) # (B, T, embedding_size) emb = emb.view(B, -1) # (B, T * embedding_size) logits = self.mlp(emb) loss = None if targets is not None: loss = F.cross_entropy(logits, targets) return logits, loss 参考 # LLM101n 硬核代码解读：超详解读numpy实现多层感知机MLP LLM101n 硬核代码解读：手把手教你用PyTorch实现多层感知机MLP Self\n《谢春宇_多模态大模型：开放世界理解》\n+ MM-LLMs: Recent Advances in MultiModal Large Language Models\n"},{"id":113,"href":"/www6vAlgo/docs/RL/core/GRPO-family/DAPO/","title":"(原理)DAPO","section":"GRPO Family","content":" NotebookLLM # "},{"id":114,"href":"/www6vAlgo/docs/Vision/seg/gptMultimodalSAM/","title":"(原理|实战) SAM","section":"Segmentation","content":" 论文[1] # 论文地址 Segment Anything Model论文\n开源地址 Segment Anything Model模型源码 Git\n官网 Segment Anything Model官网\nSegment Anything Model官网demo网页端\n模型[2] # image encoder # prompt encoder # mask decoder # 在prompt embeddings中插入一个可学习的token，用于docoder的输出。 （1）prompt toekns+output tokens进行self attn, （2）用得到的token和image embedding进行 cross attn（token作为Q） （3）point-wise MLP 更新token （4）用image embedding和（3）的token进行cross atten（image embedding作为Q） 重复上述步骤2次，再将attn再通过残差进行连接，最终输出masks和iou scores。\nSAM应用[3] # 图像分割 目标检测 图像修复( image inpainting) 模型微调 参考 # 【模型解读】【代码复现】Segment Anything Model(SAM) * 【论文解读】MetaAi SAM(Segment Anything) 分割一切 *** 最下面有应用 Segment Anything(sam)项目整理汇总 *** 1xx. 【Segment Anything 模型深度解构】GPT时代，干翻计算机视觉第一步！\n"},{"id":115,"href":"/www6vAlgo/docs/Vision/Vision-Encoder/gptMultimodalVit/","title":"(原理|实战)ViT, ViLT","section":"Vision Encoder","content":" 论文 # 论文地址 AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n开源地址 vision_transformer git\nmodel # 代码[1] # 参考 # ViT # 【Sora重要技术】复现ViT（Vision Transformer）模型 V mnist-vit Repo git 1xx. VIT (Vision Transformer) 模型论文+代码(源码)从零详细解读，看不懂来打我 V\n1xx. 详解VIT（Vision Transformer)模型原理, 代码级讲解 VIT Repo git ***\n1xx. ViT｜ Vision Transformer ｜理论 + 代码 V PPT\nViLT # 1xx. ViLT：最简单的多模态Transformer 1xx. ViLT git 1xx. ViLT 论文精读【论文精读】 ViLT 论文精读【论文精读】 V 1xx. 多模态ViLT模型下游任务微调原理及代码\n"},{"id":116,"href":"/www6vAlgo/docs/Vision/Vision-Encoder/gptMultimodalCLIPPractice/","title":"(实战)CLIP","section":"Vision Encoder","content":" CLIP Training [9] # # image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter # extract feature representations of each modality # ------------------------------------------------- # 1、图像/文字数据过image/text encoder，提取单模态特征 # 每张图片对应一个基本特征I_i # 每张文字对应一个基本特征T_i # ------------------------------------------------- I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # ------------------------------------------------- # 2. 图像/文字的基本特征过多模态Embedding，提取多模态特征 # 同时对这两个多模态特征做Layer Norm # ------------------------------------------------- I_e = l2_normalize(np.dot(I_f, W_i), axis=1) # [n, d_i] * [d_i, d_e] = [n, d_e] T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # [n, d_t] * [d_t, d_e] = [n, d_e] # ------------------------------------------------- # 3、计算图片-文字向量的余弦相似度 # ------------------------------------------------- logits = np.dot(I_e, T_e.T) * np.exp(t) # [n, n] # ------------------------------------------------- # 4、计算Loss # ------------------------------------------------- labels = np.arange(n) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2 CLIP分为按行计算Loss和按列计算Loss 按行计算Loss，在每一行范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一张图片，我们都希望找到和它最相似的文字。 按列计算Loss，在每一列的范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一段文字，我们都希望找到和它最相似的图片。 最后将这两个Loss相加取平均，代表我们在模型优化过程中考虑了“图片-\u0026gt;文字”和“文字-\u0026gt;图片”的双向关系。 Simple Demo[10] # 【基于clip on resnet, 数据集为mnist中的\u0026lt;数字文本，数字图片\u0026gt;对】\nopen_clip[11] # Training CLIP python -m training.main \\ --save-frequency 1 \\ --zeroshot-frequency 1 \\ --report-to tensorboard \\ --train-data=\u0026#34;/path/to/train_data.csv\u0026#34; \\ # 训练数据 --val-data=\u0026#34;/path/to/validation_data.csv\u0026#34; \\ # 验证数据 --csv-img-key filepath \\ --csv-caption-key title \\ --imagenet-val=/path/to/imagenet/root/val/ \\ --warmup 10000 \\ # --batch-size=128 \\ # --lr=1e-3 \\ # --wd=0.1 \\ --epochs=30 \\ # --workers=8 \\ --model RN50 # 模型 Chinese-CLIP # 方法[20] # 我们的核心方法在于把训练分为两阶段（如上图所示），第一阶段和LiT是一致的，冻结图像塔，让文本塔表示接近图像塔表示。当训练继续但下游精度不能再产生显著提升，即下游零样本检索的精度，我们就把训练切换到第二阶段，即解除图像塔的参数冻结，继续用contrastive tuning预训练，同样直到下游精度没有显著提升。后者的意义在于让图像塔能拟合中文世界的图像数据的分布，学习中文世界的知识。更多实验参数欢迎查看论文的附录部分。\ndemo[21] # 代码都看过\n# 图片库特征抽取代码 python3 extract_embeddings.py # 图片特征在faiss向量数据库建立索引 python3 build_index.py # 可视化应用界面 streamlit run app.py 参考 # 实战 # 关于多模态经典之作CLIP，还有哪些细节是你不知道的\n【多模态】复现OpenAI的CLIP模型 V mnist-clip Repo git\nopen_clip Repo git Interacting with open_clip\nChinese-CLIP # 中文CLIP模型卷土重来，这次加量不加价！ 论文\nAIGC之图片生成——基于clip内容检索 clip_retrieval git\ndemos Repo readme有解释\n1xx. 【已重新开源】CLIP的中文副本？说不定有惊喜呢\n1xx. Chinese-CLIP Repo git\n1xx. 中文CLIP文到图搜索应用 demo\nxxx # 1xx. langchain 中有CLIP的实现\n1xx. GitHub - jina-ai/clip-as-service: Scalable embedding, reasoning, ranking for images and sentences with CLIP git\n"},{"id":117,"href":"/www6vAlgo/docs/Vision/Diffusion/gptMultimodalDiffusionPractice/","title":"(实战)Stable Diffusion","section":"Diffusion","content":" API-based # diffusion-models-class [官方课程] # Unit 1: An Introduction to Diffusion Models Unit 2: Fine-Tuning, Guidance and Conditioning Unit 3: Stable Diffusion Unit 4: Going Further with Diffusion Models diffusers 重点pipeline [10] # controlnet 【controllable】 dreambooth 【fine tuning】 instruct_pix2pix 【image edit】 UI-based # stable-diffusion-webui # stable-diffusion-webui-colab[11] 没试过，colab要充值\nstable-diffusion-webui on 阿里serverless [12]\ncomfyui # ComfyUI on 阿里serverless[13]\n参考 # API-based # Repo diffusers git UI-based # 可白嫖且很香—轻轻松松在colab上部署Stable Diffusion大模型！ V stable-diffusion-webui-colab Repo git Install the WebUI Colab to Google Drive git 运行这3个脚本\n超详细云端部署Stable Diffusion教程！ V 【用FC的应用模版部署】【3个月免费的serverless+NAS】\n函数计算 ComfyUI 使用文档\n用 ComfyUI 自制“粘土滤镜 【用FC的应用模版部署】【3个月免费的serverless+NAS】\n"},{"id":118,"href":"/www6vAlgo/docs/Vision/Controllable/gptDiffusionControllable/","title":"(综述)Controllable","section":"Controllable","content":" 论文 # 论文地址 A Survey of Multimodal Controllable Diffusion Models Overview # Formulation # Semantic Control # text-to-image[79, 82–84]\nT2I-Adapter[84]\nSpatial Control # Layout- or segmentation-guided approaches[83, 87–91]\nSketch- or edge-guided approaches[22, 84, 92–96] ControlNet[22] T2I-Adapter[84]\nDepth-guided approaches[22, 84, 94–97] ControlNet[22] T2I-Adapter[84]\nSkeleton-guided approaches[22, 27, 84, 93, 95, 96] ControlNet[22] T2I-Adapter[84]\nID Control # Paint by example[104] edit\nStyle Control # Textual Inversion[111], Dreambooth[112], or LoRA[113]\nControllability Trade-Off # Methodologies # Guidance # Controlnet classifier-guidance Condition # GLIDE classifier-free guidance (CFG) Attention-Based Modification【cross-attention】 # Prompt-to-prompt Zero-shot image-to-image translation "},{"id":119,"href":"/www6vAlgo/docs/Vision/editing/gptDiffusionEditMGIE/","title":"MGIE","section":"Editing","content":" 论文 # 论文地址 MGIE 2024 Apple 开源地址 Repo git\nRepo git Project page Project page Method[1] # 参考 # 【多模态MLLMs+图像编辑】MGIE：苹果开源基于指令和大语言模型的图片编辑神器（24.02.03开源） 1xx. 罕见！苹果开源图片编辑神器MGIE，要上iPhone?\n"},{"id":120,"href":"/www6vAlgo/docs/LLM/Core/token-sampling/temperature/","title":"Token Sampling Methods","section":"Token Sampling","content":" Token Sampling Methods # Token Sampling Methods\n"}]