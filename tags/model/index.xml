<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Model on LLM 算法</title>
    <link>https://www6v.github.io/www6vAlgo/tags/model/</link>
    <description>Recent content in Model on LLM 算法</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 04 Apr 2024 22:03:52 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAlgo/tags/model/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(原理)Self-Attention</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/SelfAttention/</link>
      <pubDate>Sun, 19 Nov 2023 14:19:46 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/SelfAttention/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;self-attention&#34;&gt;&#xA;  Self-Attention&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#self-attention&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Self-Attention-142bfe21108480f6a0c0cdbf9262a7e3?pvs=4&#34;&gt;(原理)Self-Attention&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)GQA</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/ModelGQA/</link>
      <pubDate>Sun, 19 Nov 2023 14:29:40 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/ModelGQA/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.13245&#34;&gt;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;mha-vs-mqa-vs-gqa-1&#34;&gt;&#xA;  &lt;strong&gt;MHA vs. MQA vs.&lt;/strong&gt; GQA [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mha-vs-mqa-vs-gqa-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;mha&#34;&gt;&#xA;  &lt;strong&gt;MHA&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mha&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;首先是原始的 &lt;strong&gt;MHA(Multi-Head Attention)&lt;/strong&gt;，QKV 三部分有相同数量的头，且一一对应。每次做 Attention，head1 的 QKV 就做好自己运算就可以，输出时各个头加起来就行。&lt;/p&gt;&#xA;&lt;h3 id=&#34;mqa&#34;&gt;&#xA;  &lt;strong&gt;MQA&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mqa&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;而 &lt;strong&gt;MQA&lt;/strong&gt; 则是，让 &lt;strong&gt;Q 仍然保持原来的头数&lt;/strong&gt;，但 &lt;strong&gt;K 和 V 只有一个头&lt;/strong&gt;，相当于所有的 Q 头共享一组 K 和 V 头，所以叫做 Multi-Query 了。实现改变了会不会影响效果呢？确实会影响但相对它能带来的收益，性能的些微降低是可以接受的。&lt;/p&gt;&#xA;&lt;p&gt;能带来多大的收益呢，实验发现一般能提高 30%-40% 的吞吐。&lt;/p&gt;&#xA;&lt;p&gt;收益主要就是由降低了 KV cache 带来的。实际上 MQA 运算量和 MHA 是差不多的，可理解为&lt;strong&gt;读取一组 KV 头&lt;/strong&gt;之后，&lt;strong&gt;给所有 Q 头用&lt;/strong&gt;，但因为之前提到的内存和计算的不对称，所以是有利的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;gqa&#34;&gt;&#xA;  GQA&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gqa&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;而 &lt;strong&gt;GQA&lt;/strong&gt; 呢，是 MHA 和 MQA 的折衷方案，既不想损失性能太多，又想获得 MQA 带来的推理加速好处。具体思想是，不是所有 Q 头共享一组 KV，而是&lt;strong&gt;分组一定头数 Q 共享一组 KV&lt;/strong&gt;，比如上面图片就是两组 Q 共享一组 KV。&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)GRPO</title>
      <link>https://www6v.github.io/www6vAlgo/docs/RL/core/GRPO-family/GRPO/</link>
      <pubDate>Thu, 04 Apr 2024 22:03:52 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/RL/core/GRPO-family/GRPO/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;grpo&#34;&gt;&#xA;  GRPO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#grpo&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/ebd/194bfe21108480129939e44fd7ddacfd&#34;&gt;GRPO&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Reasoning LLM</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/R1/ReasoningLLM/</link>
      <pubDate>Sun, 18 Feb 2024 09:01:44 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/R1/ReasoningLLM/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;reasoning-llm&#34;&gt;&#xA;  Reasoning LLM&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reasoning-llm&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Visual-Guide-to-Reasoning-LLMs-Exploring-Test-Time-Compute-Techniques-and-DeepSeek-R1-194bfe2110848099a5c3e0585f332d2a?pvs=4&#34;&gt;(原理)Reasoning LLM&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)BERT</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Dense/BERT/</link>
      <pubDate>Fri, 12 Jan 2024 22:16:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Dense/BERT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;bert&#34;&gt;&#xA;  BERT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#bert&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/BERT-16abfe211084808fb8a3d0769c37c3db?pvs=4&#34;&gt;(原理)BERT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
