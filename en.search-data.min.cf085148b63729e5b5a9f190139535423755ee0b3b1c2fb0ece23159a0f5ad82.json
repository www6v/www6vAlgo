[{"id":0,"href":"/www6vAlgo/docs/RL/framework/verl/","title":"HybridFlow[veRL]","section":"Framework","content":" 论文 # HybridFlow: A Flexible and Efficient RLHF Framework\n解读 # HybridFlow[veRL]\n"},{"id":1,"href":"/www6vAlgo/docs/RL/Agentic-RL/survey/survey/","title":"(Survey)Agentic RL","section":"Survey","content":" 论文 # The Landscape of Agentic Reinforcement Learning for LLMs: A Survey git 解读 # (Survey)Agentic RL\n参考 # The Landscape of Agentic Reinforcement Learning for LLMs: A Survey\nThe Landscape of Agentic Reinforcement Learning for LLMs: A Survey\n"},{"id":2,"href":"/www6vAlgo/docs/RL/Agentic-RL/Tool/OTC/","title":"OTC","section":"Tool","content":" 参考 # 2025年大模型agent rl训练多轮planning技术TORL,ToolRL, RAGEN,OTC,SkyRL-v0, GiGPO,Tool-N1 ,ARTIST, ZeroTIR, GRPO\n"},{"id":3,"href":"/www6vAlgo/docs/RL/Agentic-RL/Tool/ReTool/","title":"ReTool","section":"Tool","content":" 论文 # ReTool: Reinforcement Learning for Strategic Tool Use in LLMs\nhttps://retool-rl.github.io/\n引言 # 大型语言模型（LLM）通过思维链提示和强化学习等技术，在推理任务中取得了显著进展。然而，这些基于文本的方法在需要精确数值计算或符号操作的任务中常常力不从心。ReTool 通过引入一个强化学习框架来解决这一限制，该框架教导 LLM 如何战略性地将代码解释器整合到其推理过程中。\n图1：传统基于文本的强化学习训练（上）与 ReTool 采用交错代码执行的方法（下）的比较。ReTool 允许在生成过程中与代码沙盒进行动态交互。\n这项工作表明，LLM 不仅能学习如何使用计算工具，还能通过基于结果的反馈学习何时以及为何调用它们。这代表着与仅仅模仿预定义工具使用模式的监督微调方法相比，取得了重大进步。\n方法论 # ReTool 采用两阶段训练框架，结合监督微调和强化学习，以开发战略性工具使用能力。\n冷启动监督微调 # 第一阶段通过细致的数据整理和监督训练，建立基础的工具使用能力：\n数据构建过程：\n从 Open-Thoughts 等来源初步收集高质量数学推理数据 （ \\( D_{init} \\) ） 使用人类专家和 DeepSeek-R1 进行双重验证以确保质量 使用结构化提示将基于文本的推理自动转换为代码集成推理（ \\( D_{CI} \\) ） 对代码集成数据集进行额外的格式和答案验证 转换过程使用系统化的提示模板，该模板识别文本推理中的计算步骤，并将其替换为可执行的代码片段及其输出。这创建了演示何时以及如何调用代码解释器的训练数据。\n带有交错代码执行的强化学习 # 核心创新在于 ReTool 的强化学习训练过程，该过程动态集成了实时代码执行：\n训练算法： 采用简单基于准确性的奖励函数的近端策略优化（PPO）：\n\\[\rR(a, \\hat{a}) = \\begin{cases} 1 \u0026 \\text{if predicted answer } \\hat{a} \\text{ equals ground truth } a \\\\ -1 \u0026 \\text{otherwise} \\end{cases}\r\\] 展开过程： 与传统模型生成纯文本的强化学习不同，ReTool 的展开过程包括：\n模型生成自然语言推理 当代码调用被触发（通过 \u0026lt;code\u0026gt; 标签）时，生成暂停 代码被提取并在外部沙盒环境中执行 执行结果（成功或错误）通过 \u0026lt;interpreter\u0026gt; 标签反馈给模型 模型使用解释器反馈继续生成 过程重复直到得到最终答案或达到最大长度 技术优化：\n解释器反馈token在损失计算中被屏蔽，以保证训练稳定性 KV缓存重用最小化了展开过程中的内存开销 异步代码沙盒支持并行执行和更快的训练 结果与分析 # ReTool 在具有挑战性的数学推理基准测试中，相较于基线方法展现出显著改进。\n性能指标 # AIME 基准测试结果：\nAIME 2024：67.0% 准确率（对比基于文本的强化学习基线为 40.0%） AIME 2025：49.3% 准确率（对比基于文本的强化学习基线为 36.7%） 训练效率：在 400 步内实现了卓越性能，而基线则需要 1000+ 步 图2：训练曲线显示了 ReTool 在 AIME 基准测试中相较于基于文本的强化学习方法，具有快速的改进和卓越的最终性能。\n高级骨干模型结果： 以 DeepSeek-R1-Distill-Qwen-32B 作为基础模型，ReTool 实现了更高的性能：\nAIME 2024：72.5% 准确率 AIME 2025：54.3% 准确率 涌现认知行为 # 分析揭示了在强化学习训练过程中出现的几个显著的涌现特性：\n图3：ReTool 训练过程中行为变化的详细分析，展示了响应长度、代码使用模式和执行成功率的演变。\n关键涌现行为：\n令牌效率： 平均响应长度减少 40%（从 10k 减少到 6k 令牌） 代码采用： 代码使用率增加到近 98% 的响应 战略时机： 模型学会了在推理过程的早期调用代码 自我纠正： 自主错误检测和代码修订能力 多样化工具使用： 扩展了基本计算之外的功能，包括验证、优化和分析 自我纠正能力 # 最重要的发现之一是模型能够自主纠正代码执行错误：\n图4：涌现的自我纠正行为示例，模型识别并修复了其生成的代码中的 NameError。\n当遇到 NameError: name 'greedy' is not defined 等错误时，模型通过反思错误并生成修正后的代码，展示了元认知意识，而无需为此行为进行明确的训练。\n工具使用演变 # 这项研究提供了关于代码使用模式在训练期间如何演变的见解：\n图5：词云分析显示了 ReTool 训练前后代码目的的演变，展示了工具使用策略多样性的增加。\n分析表明，虽然“计算”仍然是主要目的，但强化学习训练导致了更多样化的代码应用，包括几何分析、概率计算和解决方案验证。\n比较分析 # ReTool 的方法比传统的基于文本的推理具有明显的优势：\n图6：在复杂数学问题上，基于文本的推理（左）与代码集成推理（右）的并排比较，突出了精度和效率的提升。\n比较表明，代码集成消除了容易出错的冗长手动计算，使模型能够专注于更高级别的战略推理，同时将精确计算卸载到解释器。\n意义和影响 # ReTool 代表了混合神经-符号人工智能系统的重大进步，它证明了大型语言模型可以通过强化学习而不是单纯的模仿来学习战略性工具使用。这项工作的贡献包括：\n技术贡献：\n首次成功将强化学习应用于大规模战略性代码解释器使用 展示了无需明确训练即可涌现的自我纠正能力 高效的训练框架，以更少的步骤实现了卓越的性能 更广泛的影响：\n弥合了神经网络模式识别与符号计算之间的鸿沟 为跨不同领域更通用化的工具使用奠定了基础 通过明确的计算步骤增强了可解释性 展示了自主发现问题解决策略的潜力 这项研究确立了 ReTool 作为开发更强大、更通用的人工智能系统的一种引人注目的方法，这些系统能够有效地结合神经网络和符号计算范式的优势。\n参考 # 通过工具增强 LLM Agent 能力：veRL+ReTool 的完整实践指南 https://www.alphaxiv.org/zh/overview/2504.11536v2 "},{"id":4,"href":"/www6vAlgo/docs/RL/Deep-Research/Survey/Survey/","title":"Survey","section":"Survey","content":" 论文 # Reinforcement Learning Foundations for Deep Research Systems: A Survey\nMethod # 方法 优化目标 数据形式 关键短板 SFT 模仿单步 (q, a) 对 暴露偏差、无法纠错 DPO 偏好排序 (q, a⁺, a⁻) 无状态、信用分配短视 RL 最大化回报 (q, τ, r) 需可验证奖励+探索策略 RL METHODS FOR AGENTIC RESEARCH # TRAINING REGIME AND OPTIMIZATION STRUCTURE # REWARD DESIGN AND CREDIT ASSIGNMENT # 结果奖励（Outcome-only）\n步骤奖励（Step-level）\nframework # 参考 # 2篇最新论文，把Deep Research讲透了~\n"},{"id":5,"href":"/www6vAlgo/docs/RL/Agentic-RL/Tool/ToolRL/","title":"ToolRL","section":"Tool","content":" 论文 # ToolRL: Reward is All Tool Learning Needs\n为了确定最佳奖励策略，探索了四个关键维度的各种奖励配置：(1) 奖励类型（奖励哪些方面），(2) 奖励尺度（奖励多少），(3) 奖励粒度（奖励信号的详细程度），以及 (4) 奖励动态（奖励如何随时间演变）。通过大量的实验确定了最符合主体工具使用情况的奖励设计，并揭示了奖励对于调用工具的 LLM 而言“有用”的原因。论文得出的核心见解总结如下：\n推理轨迹越长并不一定越好，而且过长的奖励可能会降低性能。 动态奖励尺度有助于模型从简单行为平稳过渡到复杂行为。 细粒度的奖励分解可实现更稳定、更有效的学习。 论文得出的核心见解总结如下：\n推理轨迹越长并不一定越好，而且过长的奖励可能会降低性能。 动态奖励尺度有助于模型从简单行为平稳过渡到复杂行为。 细粒度的奖励分解可实现更稳定、更有效的学习。 Format Reward Correctness Reward\n参考 # 2025年大模型agent rl训练多轮planning技术TORL,ToolRL, RAGEN,OTC,SkyRL-v0, GiGPO,Tool-N1 ,ARTIST, ZeroTIR, GRPO\n"},{"id":6,"href":"/www6vAlgo/docs/RL/Deep-Research/Search/Search-R1/","title":"Search-R1","section":"Search","content":" 论文 # Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning https://github.com/PeterGriffinJin/Search-R1 Methods # 详细方法和步骤:\n将搜索引擎建模为环境的一部分： SEARCH-R1将搜索引起作为环境的一部分， 让模型与环境交互，从而得到 reward。 支持多轮检索和推理： SEARCH-R1通过特定的标签（\u0026lt;search\u0026gt;, \u0026lt;/search\u0026gt;, \u0026lt;information\u0026gt;, \u0026lt;/information\u0026gt;, \u0026lt;think\u0026gt;, \u0026lt;/think\u0026gt;, \u0026lt;answer\u0026gt;, \u0026lt;/answer\u0026gt;）来支持多轮检索和推理。 优化算法兼容性： SEARCH-R1 与各种 RL 算法兼容，包括 PPO 和 GRPO。 简单结果奖励函数： 避免复杂的基于过程的奖励, 采用简单的基于结果的奖励函数 （字符串匹配作为reward!!!）。 总结 # 结论1: SEARCH-R1 显著提升了LLM在需要实时外部知识的复杂推理任务中的能力。 通过强化学习，LLM可以自主生成查询并有效利用检索到的信息，优于传统的RAG方法。\n结论2: SEARCH-R1在不同LLM架构和训练方法上具有广泛的适用性。 实验结果表明，无论使用基础模型还是指令调整模型，SEARCH-R1都能带来显著的性能提升，且对不同的RL算法（如PPO和GRPO）具有兼容性。\n结论3: SEARCH-R1有很强的实用价值。 SEARCH-R1能够显著提高LLM在需要实时外部知识的复杂推理任务中的能力。 可以用于智能问答，智能助手等领域。\n参考 # Search-R1：让大模型学会“检索+推理”的新范式 1xx. 【论文解读】Search-R1：强化学习如何教会 LLM 自主搜索？\n1xx. 有个学术的会议\n1xx. Search-R1：让 LLM 学会 “边搜边想”，强化学习赋能检索增强推理\n"},{"id":7,"href":"/www6vAlgo/docs/LLM/MoE/ModelMOE/","title":"(原理)Visual  MOE","section":"MOE","content":" Visual MOE # (原理)Visual MOE\n"},{"id":8,"href":"/www6vAlgo/docs/RL/core/GRPO-family/GRPODeepseek/","title":"(实战)GRPO","section":"GRPO Family","content":"\nGRPO # (实战)GRPO\n"},{"id":9,"href":"/www6vAlgo/docs/LLM/Reasoning/kimi/k2/","title":"(翻译)kimi k2","section":"kimi","content":" 论文 # https://github.com/MoonshotAI/Kimi-K2?tab=readme-ov-file\nhttps://moonshotai.github.io/Kimi-K2/\nhttps://arxiv.org/pdf/2507.20534\nKimi K2 # 概述 什么是 Kimi K2？ 模型架构 预训练（Pre-Training） 后训练（Post-Training） 评测结果 MuonClip 优化器 代理型智能（Agentic Intelligence）作为一种新范式 参考文献 概述 # Kimi K2 标志着大语言模型（LLM）从传统静态模型向代理型智能（agentic intelligence） 的根本性转变——模型不再仅依赖预收集的数据集进行模仿学习，而是通过与动态环境主动交互来持续学习。这一范式旨在赋予模型自主感知、规划、推理与行动的能力，使其行为可超越训练分布的边界。\n其深远意义在于：模型不再受限于复现人类撰写语料的被动角色，而是能通过合成数据、探索环境、实时适应，发展出全新能力（novel competencies），为超人推理、工具编排、软件开发及现实世界自主性开辟路径。\n代理型智能也重构了训练挑战本身：\n预训练阶段不仅需高效灌输广泛先验知识，还需在万亿 token 规模下保持训练稳定； 后训练阶段则需生成可执行、可验证的行为——即便真实语料中这类“代理轨迹”极为稀少。 Kimi K2 通过三大核心技术应对上述挑战：\n✅ MuonClip 稳定化训练\n✅ 大规模代理行为数据合成\n✅ 多信号强化学习（RL）\n最终打造出当前最强开源非“思考型”模型之一（即非 CoT-heavy 推理延迟模型，强调 reflexive 快速响应能力）。\n👉 您可在线体验：kimi.com\n下图为 Kimi K2 整体训练流程概览：\n什么是 Kimi K2？ # Kimi K2 是一个混合专家模型（Mixture-of-Experts, MoE），总参数量达 1 万亿（1T），但每次前向传播仅激活其中 320 亿（32B） 参数——在享受超大规模表征能力的同时，显著提升计算效率。\n更重要的是：Kimi K2 不只是聊天模型。其核心设计目标是实现代理行为（agentic behavior）——即具备规划、推理、调用工具、自主执行多步骤任务的能力。\n月之暗面（Moonshot AI）开源了两个版本：\nKimi-K2-Base：原始预训练模型，适用于科研与微调； Kimi-K2-Instruct：经后训练优化的指令遵循模型，专为快速响应类任务（reflexive tasks） 设计。 模型架构 # Kimi K2 采用与 DeepSeek-V3 类似的 MoE 架构，但存在关键差异：\n🔹 更多专家（experts）\n🔹 更少注意力头（attention heads）\n🔹 改进的负载均衡机制——防止“专家坍缩”（仅少数专家被激活）\n具体结构为 32-of-1024 MoE：即每步仅激活 32 位专家（激活率约 3.1%）。尽管稀疏性极高，模型在各类任务上仍保持强劲性能。\n下图为简化架构对比图（来源：Sebastian Raschka）：\n此架构使 Kimi K2 能在不显著增加推理成本的前提下，扩展至万亿参数规模；路由机制确保不同模块专业化分工，训练动态则保障专家均衡参与。 预训练（Pre-Training） # MuonClip 优化器 # Token 效率与稳定性融合：K2 引入 MuonClip，将高 token 效率的 Muon 优化器与QK-Clip 机制相结合。\nMuon 可提升单位 token 的学习信号强度，但在超大规模下易导致注意力 logits 爆炸； QK-Clip 通过动态缩放查询（Query）与键（Key）权重，抑制 logits 增长（当其超过阈值 τ 时）。 关键优势：缩放不改变当前步的前向/反向计算，从而保持优化动力学不变。 最终实现：15.5T token 全程无发散稳定训练【7†source】。 公式化裁剪：对每个注意力头 $h$，计算最大注意力 logit：\n$$ S^{h}{\\max} = \\frac{1}{\\sqrt{d}} \\max{X \\in B} \\max_{i,j} Q^{h}i {K^h_j}^\\top $$ 若 $S^{h}{\\max} \u0026gt; \\tau$，则对权重缩放：\n$$ W^h_q \\leftarrow W^h_q \\cdot \\sqrt{\\gamma_h}, \\quad W^h_k \\leftarrow W^h_k \\cdot \\sqrt{\\gamma_h}, \\quad \\text{其中}\\ \\gamma_h = \\min(1, \\tau / S^{h}_{\\max}) $$ → 逐头裁剪，最小化干预，确保 logits 有界。\nToken 利用率与合成改写 # 知识改写（Knowledge rephrasing）：\n替代易导致过拟合的多轮重复训练； 采用合成改写流水线：\n✅ 多风格/多视角 prompt 指导，提升语言多样性；\n✅ 分块自回归重写，保障全局连贯性；\n✅ 语义保真度校验，确保内容一致。 效果：SimpleQA 准确率从 23.8%（多轮训练）→ 28.9%（10 倍改写）。 数学数据增强：\n将数学语料重写为分步学习笔记（learning notes）（灵感源自 SwallowMath），迫使模型内化推理步骤； 多语言数学资料统一翻译为英文，增强多样性与推理鲁棒性。 模型架构细节 # 万亿参数 MoE 的稀疏扩展：\n实际使用 384 专家，稀疏度 48（每 token 激活 8 专家）； 实验表明：更高稀疏度可在同等 FLOPs 下降低验证损失； 相比稀疏度 8，性能相当时 FLOPs 减少 1.69 倍。 注意力头数量权衡：\n未像 DeepSeek-V3 那样将头数设为层数 2 倍，而是减半至 64 头； 原因：增加头数仅带来 0.5–1.2% 损失改善，却导致 128k 上下文推理 FLOPs 增加高达 83%； K2 选择优先保障长上下文效率。 训练基础设施 # 集群与并行：\n运行于 NVIDIA H800 集群（每节点 2TB RAM）； 采用 16 路流水线并行 + 16 路专家并行 + ZeRO-1 数据并行。 显存优化：\nFP8-E4M3 激活压缩； SwiGLU/LayerNorm 层选择性重计算； CPU 激活卸载——在有限 GPU 显存下实现万亿级稳定训练。 训练方案：\n总 token 数：15.5T； 学习率：前 10T token 固定 2e⁻⁴，后 5.5T 采用余弦衰减，末期加温退火； 上下文长度渐进扩展：4k → 32k → 128k（通过 YaRN 实现）； 全程训练曲线零震荡。 后训练（Post-Training） # 基于代理行为的监督微调（SFT） # 指令多样性：通过人类标注、prompt 工程改写、判别模型自动过滤构建高质量指令数据。\n代理行为合成流水线（受 ACEBench 启发）：\n1️⃣ 生成合成/真实工具规范（\u0026gt;20k 工具，\u0026gt;3k MCP 协议）；\n2️⃣ 构建具不同工具集的多样化代理；\n3️⃣ 生成基于评分标准（rubric）验证的任务；\n4️⃣ 在模拟/真实执行沙盒（如带单元测试的编程环境）中生成多轮轨迹，并按 rubric 评分。\n→ 同时保障覆盖广度与行为真实性【7†source】。\n强化学习（RL） # 双奖励信号设计：\n🔹 可验证任务奖励：数学、逻辑、编程（通过可执行判官，如单元测试）；\n🔹 自评 rubric 奖励：创意、安全性等主观维度；\n判别模型（critic）持续优化 rubric 权重，将主观判断锚定于可验证性能之上。 训练创新点：\n预算控制：惩罚冗长输出； PTX-loss 融合：保留高质量预训练知识； 温度衰减调度：平衡探索与收敛；\n→ 不仅提升准确率，更实现与人类复杂价值观的对齐。 RL 基础设施 # 检查点引擎：\n摒弃传统 NFS 参数重排，改用分布式全参数广播； 实现万亿参数规模下 \u0026lt;30 秒 的检查点同步更新。 代理 rollout 优化：\n部分 rollout（partial rollouts）； 环境并行化； 延迟均摊技术； RL 框架采用 Gym 式接口，可无缝集成任意新环境。 评测结果 # 编程与工程能力 # 基准 Kimi K2 对比模型 SWE-bench Verified（代理单次尝试） 65.8% Claude 4 Opus: 72.5% LiveCodeBench v6 53.7% \u0026gt; GPT-4.1, Claude Sonnet OJBench 27.1% \u0026gt; Gemini 2.5 Flash (19.5%) → K2 是当前最强开源模型，适用于真实工程与竞赛编程场景。\n代理工具使用能力 # 基准 Kimi K2 表现 Tau2-Bench（多轮工具编排） 66.1 Pass@1 — ACEBench 76.5% 准确率 \u0026gt; DeepSeek-V3 \u0026amp; Claude Sonnet → 展现强大具身化工具推理能力（grounded tool-use reasoning），为代理智能核心支柱。\n数学与 STEM 能力 # 基准 Kimi K2 说明 AIME 2025 49.5% \u0026gt; Qwen3 (24.7%) GPQA-Diamond 75.1% ≈ Claude Opus HMMT 2025 38.8% 开源最佳 ✅ 验证了学习笔记改写与可验证 RL 任务整合的有效性。\n通用能力与长上下文 # 基准 Kimi K2 备注 MMLU 89.5% ≈ 闭源顶尖模型 MMLU-Redux 92.7% 开源最佳 LongBench v2 49.1% ≈ GPT-4.1 DROP 93.5% 事实推理准确率 → K2 不仅专精特定领域，更是全面稳健的通用模型，覆盖推理、事实性与长上下文理解。\n下图为 Kimi K2 与 DeepSeek-V3、Claude 4 Opus、GPT-4.1、Gemini 2.5、Qwen3 的多任务性能对比：\nMuonClip 优化器详解 # 训练万亿参数模型的最大难点在于训练不稳定性，尤其表现为注意力 logits 爆炸。 Kimi K2 提出定制优化器 MuonClip——Muon 优化器的增强版，专为大规模 MoE 设计。 核心创新：qk-clip 机制 在每步训练中动态重缩放 Q/K 权重，使 logits 始终处于安全范围； 保障训练过程平滑收敛，为超大模型训练提供基础设施级保障。 代理型智能（Agentic Intelligence）作为一种新范式 # 代理型智能从根本上重构了 LLM 的运作范式——从被动文本补全引擎，进化为自适应、自主的智能体系统。\nKimi K2 通过以下机制实现这一跃迁：\n🔹 工具自主发现\n🔹 自我评估（self-evaluation）\n🔹 基于评分标准（rubric）的价值对齐\n🔹 混合真实/合成环境训练\n→ 模型由此能持续拓展其能力边界（competence frontier）。\n相比在静态预训练数据中遭遇收益递减，代理模型可自主生成：\n✅ 行动轨迹（trajectories of action）\n✅ 错误驱动的改进循环（error-driven improvements）\n→ 本质上成为自维持学习体（self-sustaining learners）。\n这一范式不仅为研究突破（当 scaling law 遇到瓶颈时）提供新路径，更对现实部署至关重要——因自主性、可靠性与具身工具使用已成为下一代 AI 的刚需。\nKimi K2 证明：构建此类系统需全栈创新：\n优化层：MuonClip 数据层：合成改写 + 工具流水线 强化学习层：RLVR + rubric 奖励 基础设施层：检查点引擎 + 代理 rollout → 它不仅是模型，更是一套通向未来 AI 的框架，模糊了静态基座模型与交互式演进智能体之间的界限。\n参考文献 # （原文未列具体文献，此处保留空节；可依需补充）\n如需将本文导出为 PDF、Markdown 或制作 PPT 汇报版，我可为您进一步整理。\n参考 # 以下是对 https://aman.ai/primers/ai/kimi-K2/ 的完整中文翻译，严格保留原文结构、技术术语与所有图片（含图注），便于读者对照理解 Kimi-K2 的核心技术突破。\nKimi K2 qwen 翻译 最新重量级报告，Kimi 开源 K2技术「Make Kimi Great Again」 翻译 最上面的总结\nhttps://www.alphaxiv.org/zh/overview/2507.20534v1\n"},{"id":10,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningLearningRate/","title":"(原理)学习率","section":"网络优化","content":"\n学习率 # (原理)学习率\n"},{"id":11,"href":"/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/WeightDecay/","title":"(原理\u0026实战)权重衰减","section":"正则化","content":"\n权重衰减 WeightDecay # (原理\u0026amp;实战)权重衰减\n"},{"id":12,"href":"/www6vAlgo/docs/LLM/Core/ScalingLaw/","title":"Scaling Law","section":"Core","content":" Scaling Law[10] # Scaling Law # 参数量 vs 数据量 # 参数量 vs 数据量 # 参考 # Scaling Law # 解析大模型中的Scaling Law\n1xx. 论文阅读，大模型的缩放定律，Scaling Laws for Neural Language Models\n2xx. Training Compute-Optimal Large Language Models 简读 2xx. 【预训练模型】推翻OpenAI结论, DeepMind重新定义预训练的训练参数和训练规模的关系！\n《Scaling Laws for Neural Language Models》\n《Training Compute-Optimal Large Language Models》\n"},{"id":13,"href":"/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLData/","title":"机器学习-数据","section":"机器学习","content":"\n机器学习-数据 # 机器学习-数据\n"},{"id":14,"href":"/www6vAlgo/docs/LLM/Dense/Family/","title":"GPT 系列","section":"Dense","content":" 进化时间线 # {% asset_img \u0026lsquo;family.jpg\u0026rsquo; %}\nGPT1 [1] # 它是最早一批提出在 NLP 任务上使用 pre-train + fine-tuning 范式的工作。 GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间 预训练模型具有 zero-shot 的能力，并且能随着预训练的进行不断增强 GPT2 [1] # 核心思想 # 当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，不需要在下游任务微调。\nGPT-2 vs. GPT-1 # 主推 zero-shot，而 GPT-1 为 pre-train + fine-tuning； 训练数据规模更大，GPT-2 为 800w 文档 40G，GPT-1 为 5GB； 模型大小，GPT-2 最大 15 亿参数，GPT-1为 1 亿参数； 模型结构调整，层归一化和参数初始化方式； 训练参数，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024，等等； GPT3 [1] # 下游任务评估方法 # GPT-3 在下游任务的评估与预测时，提供了三种不同的方法： Zero-shot：仅使用当前任务的自然语言描述，不进行任何梯度更新； One-shot：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新； Few-shot：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；\nShot[2] One-shot Few-Shot Zero-Shot Few-shot vs fine-tuning # 其中 Few-shot 也被称为 in-context learning，虽然它与 fine-tuning 一样都需要一些有监督标注数据，但是两者的区别是： 【本质区别】 fine-tuning 基于标注数据对模型参数进行更新 而in-context learning使用标注数据时不做任何的梯度回传, 模型参数不更新\nGPT-3 vs. GPT-2 # 效果上，超出 GPT-2 非常多，能生成人类难以区分的新闻文章； 主推 few-shot，相比于 GPT-2 的 zero-shot，具有很强的创新性； 模型结构略微变化，采用 sparse attention 模块； 海量训练语料 45TB（清洗后 570GB），相比于 GPT-2 的 40GB； 海量模型参数，最大模型为 1750 亿，GPT-2 最大为 15 亿参数； InstructGPT [1] # 步骤 # 有监督微调， 奖励模型训练， 强化学习训练 技术方案 # 有监督微调（SFT） 本质上来说，SFT 可以理解为人工标注了一批数据，然后去微调 GPT-3。但是值得一提的是，这里标注的数据与 GPT-3 之前用来做下游任务使用的 few-shot 格式，有非常本质的区别。 InstructGPT 在 SFT 中标注的数据，正是为了消除这种模型预测与用户表达习惯之间的 gap。在标注过程中，他们从 GPT-3 的用户真实请求中采样大量下游任务的描述，然后让标注人员对任务描述进行续写，从而得到该问题的高质量回答。\n基于人类反馈的强化学习（RLHF） {% asset_img \u0026lsquo;instructGPT.jpg\u0026rsquo; %}\n总结 # 解决 GPT-3 的输出与人类意图之间的Align问题； 让具备丰富世界知识的大模型，学习“人类偏好”； 标注人员明显感觉 InstructGPT 的输出比 GPT-3 的输出更好，更可靠； InstructGPT 在真实性，丰富度上表现更好； InstructGPT 对有害结果的生成控制的更好，但是对于**“偏见”没有明显改善**； ChatGPT 训练 [3] # 基于人类反馈的强化学习微调技术 RLHF 使用有监督微调 Supervised Fine-tuning（SFT）预训练语言模型 Supervised fine-tuning (SFT) = Instruction Tuning 训练奖励模型 Reward Model（RM） 使用强化学习算法微调语言模型 RLHF [本质 基于强化学习, 强化学习算法] 参考 # GPT / GPT-2 / GPT-3 / InstructGPT 进化之路 ***\nFew-Shot, Zero-Shot \u0026amp; One-shot 的通俗理解\nAI 大模型微调训练营大纲\n1xx. 万字拆解！追溯ChatGPT各项能力的起源 符尧\n1xx. [Transformer 101系列] ChatGPT是怎么炼成的? 未\n"},{"id":15,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Transformer/","title":"(原理)Transformer","section":"Transformer","content":"\n(原理)Transformer # (原理)Transformer\n"},{"id":16,"href":"/www6vAlgo/docs/LLM/Survey/LargeModelSurvey/","title":"(综述)大模型","section":"Survey","content":" LLMs的背景[1] # Scaling law of LLMs # KM scaling law Chinchilla Scaling law LLMs的涌现能力 # in-context learning instruction following step-by-step reasoning 大语言模型的关键技术 *** # Scaling Training Ability Eliciting Alignment Tuning Tool Manipulation Pre-training[1] # 数据收集 # 架构 # 模型训练 *** # 优化设置\nBatch Training Learning Rate Optimizer Stabilizing the Training 可扩展的训练技巧\n3D并行 数据并行 + 流水线并行 + 张量并行 ZeRO 混合精度训练 总体训练建议 Adaptation Tuning of LLMs[1] # 指令调优 *** # 本质上，指令微调是在自然语言格式的实例（instance）集合上微调预训练后的 LLM 的方法 [62]。\n指令微调后，LLM 可以展现出泛化到未见过任务的卓越能力 [28, 62, 64]，即使在多语言场景下也能有不错表现 [98]。\n格式化实例的构建 # 格式化已有数据集 格式化人类需求 构建实例的关键因素 增加指令 设计格式 总的来说，指令多样性似乎比实例数量更重要\n指令微调策略 # 平衡数据分布 一种广泛使用的方法是实例比例混合策略 [87]，即将所有数据集合并，然后从混合数据集中按比例采样每种实例。 此外，根据最近的研究发现 [64, 99]，提高高质量数据集（例如 FLAN [62] 和 P3 [209]）的采样比例通常可以带来性能提升。\n结合指令微调和预训练 为了使微调过程更加有效和稳定，OPT-IML [99] 在指令微调期间加入了预训练数据，这可以看作是对模型的正则化（regularization）。\n具体而言，GLM-130B [97] 和 Galactica [34] 将指令格式数据集作为预训练语料库的一小部分来预训练 LLM，这有可能同时获得预训练和指令微调的优势。\n指令微调的效果 # 性能改进 最近的研究在多个规模上（从 7700 百万到 5400 亿不等）对 LM 进行了实验，表明不同规模的模型都可以从指令微调中受益 [64, 216]，随着参数规模的增加，性能也得到了提升 [98]。 【普适性】 此外，经过指令微调的较小模型甚至可以比未经微调的较大模型表现更好 [28, 64]。\n任务泛化性 todo 对齐调优 # 高效调优 # 参考 # 大语言模型综述 中文 v10\n大语言模型综述 中文\nLLMSurvey Repo git\n[论文]大语言模型综述\n详谈大模型训练中的数据收集、处理与模型影响：A Survey of Large Language Models工作中的数据总结\n大模型综述-A Survey of Large Language Models 1xx. 值得一看的大模型最新综述：兼看多语种大模型微调数据集Aya 1xx. 43页预训练模型综述（清华、复旦、人大）\n"},{"id":17,"href":"/www6vAlgo/docs/DeepLearning/basic/DeepLearning/","title":"Deep Learning","section":"basic","content":"\nDeep Learning # Deep Learning\n"},{"id":18,"href":"/www6vAlgo/docs/LLM/Core/token-sampling/token-sampling/","title":"(翻译)token sampling","section":"Token Sampling","content":" Token 采样方法（Token Sampling Methods） # 概述 # 生成式大语言模型（LLM）将输入和输出文本理解为“token”序列；这些 token 可以是单词，也可以是标点符号或单词的一部分。 LLM 提供若干 token 选择参数，用以控制推理/运行时输出的随机性。选择输出 token 的方法（具体称为 token 采样方法 或 解码策略），是语言模型文本生成中的一个核心概念。 从技术底层来看，token 采样的核心是：模型不断生成一个称为概率分布的数学函数，用于决定下一个 token（例如单词）——这一决策会考虑所有先前已输出的 token。简单来说，LLM 在生成文本时执行的是采样：即根据条件概率分布随机选择下一个单词。 以 OpenAI 托管的系统（例如 ChatGPT）为例：在生成概率分布后，OpenAI 的服务器会根据该分布进行 token 采样。该过程存在一定随机性，因此相同的输入提示可能产生不同的输出。 本指南将介绍不同的 token 采样方法及相关概念，包括：温度（Temperature）、贪心解码、穷举搜索解码、束搜索、Top-$k$、Top-$p$（核心采样）以及 Min-$p$。 背景 # 自回归解码（Autoregressive Decoding） # 在使用语言模型生成文本序列时，我们通常从一段文本前缀（即提示 prompt）开始，然后按以下步骤循环： 使用语言模型预测下一个 token； 将该 token 加入当前输入序列； 重复上述过程。 通过这种持续生成下一个 token 的方式（即 自回归解码），我们可以生成整个文本序列（见下图；来源）。 Token 概率 # 那么，我们该如何选择/预测下一个 token（即上述第 1 步）？ 语言模型并不直接输出下一个 token，而是输出一个所有可能 token 的概率分布。简言之，LLM 本质上是在词汇表（所有唯一 token 的集合）上进行分类任务的神经网络。 基于该概率分布，我们可以采用多种策略来选择下一个 token。例如，后文将介绍的贪心解码（greedy decoding）直接选择概率最高的 token 作为下一个输出。 Logits 与 Softmax # LLM 通过 logits 向量 $\\mathbf{z} = (z_1, \\dots, z_n)$ 表示类别打分，并使用 softmax 函数 将其转化为概率向量 $\\mathbf{q} = (q_1, \\dots, q_n)$： $$ q_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)} $$ Softmax 函数通过对 logits 取指数并归一化，使得模型在每个时间步的输出均落在 $[0, 1]$ 区间，且总和为 1，从而便于将输出解释为概率（见下图；来源）。 相关概念：温度（Temperature） # 尽管温度本身并非一种 token 采样方法，但它显著影响采样过程，因此本篇纳入讨论。 温度参数允许我们调整 token 的概率分布。它作为 softmax 变换中的一个超参数（见下图；来源），在应用 softmax 前对 logits 进行缩放，从而控制预测的随机性。 例如在 TensorFlow 的 Magenta 项目中（LSTM 实现），温度参数控制 logits 在 softmax 前被缩放（或除以）的程度。 温度在 Softmax 中的作用 # 标准 softmax 引入温度超参数 $T$ 后的形式为： $$ q_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)} $$ 其中 $T$ 为温度（默认为 1）。\n当 $T=1$，即直接对 logits 计算 softmax；\n若 $T=0.6$，则对 $\\frac{\\text{logits}}{0.6}$ 计算 softmax —— 此时数值被放大，softmax 结果更“尖锐”；\n→ 模型更自信（更少输入即可激活输出层），但也更保守（不太可能采样低概率候选）。\n不同温度范围及其影响 # 低温（$T \\approx 0.0$–$0.5$） # 特征： 强烈偏好高概率 token； 输出确定性强、随机性低； 文本常重复，多样性差。 适用场景： 需要高精度或高置信度的场景（例如生成事实性回答）。 局限性： 可能过于保守，陷入重复环（repetitive loops）。 中温（$T \\approx 0.6$–$1.0$） # 特征： 在多样性与连贯性之间取得平衡； 允许探索较低概率选项，但不显著损伤文本合理性。 适用场景： 生成类人语句、代码补全、音乐作曲等需“有创意但合理”的任务。 局限性： 仍会偏向高概率 token，抑制极低概率创意探索。 高温（$T \u0026gt; 1.0$） # 特征： 生成更“平缓”的概率分布； 输出更随机、多样；更倾向低概率选项； 有助于跳出重复环，探索更广空间。 适用场景： 头脑风暴、高度创意内容生成（如艺术、故事、诗歌）。 局限性： 易出现逻辑错误或语义混乱； 采样到不合理 token 的风险增大。 Softmax 函数的深层理解（引自维基百科） # 当温度 $\\tau \\to \\infty$ 时，所有样本概率趋近相等；温度越低（$\\tau \\to 0^+$），预期奖励最高的样本概率趋近于 1。\n温度影响总结 # 低温 → 更自信、更保守 → 适合确定性任务； 中温 → 平衡随机性与连贯性 → 通用推荐区间； 高温 → 更富创意、更随机 → 适合探索性任务。 通过调节温度，可依任务需求灵活调整模型行为，极大增强 LLM 的适用性。 贪心解码（Greedy Decoding） # 贪心解码在每一步都使用 argmax 选择当前概率最高的 token 作为输出。 问题：它无法回溯修正之前生成的 token。\n举例：输入法语 “il a m’entarté”（他用派砸了我），若贪心解码已生成 “he hit a”，即使后续发现应为 “me”，也无法回头修改。 模型逐 token 生成序列，每步仅考虑当前最优——不评估该选择对未来步骤的影响。 解码通常持续至生成 \u0026lt;END\u0026gt; token 为止。例如：\n\u0026lt;START\u0026gt; he hit me with a pie \u0026lt;END\u0026gt;（来源） 优点：计算高效、实现简单；\n缺点：不保证全局最优输出序列。 改进方向：采用穷举搜索或束搜索（beam search）。 穷举搜索解码（Exhaustive Search Decoding） # 顾名思义，穷举搜索考察所有可能的输出序列组合，并选出评分最高的那个。 在序列到序列任务（如神经机器翻译）中，这意味着生成所有可能的译文，再用评分函数评估其与目标的匹配度。 问题：计算复杂度极高——候选数量随输出长度呈指数级增长。 时间复杂度为 $O(V^T)$，其中 $V$ 为词表大小，$T$ 为输出长度；实际中几乎不可行。 尽管理论上可得最优解，但因其高昂开销，极少用于真实场景。 束搜索（Beam Search） # 束搜索是机器翻译等任务中常用的搜索算法，用于高效生成最可能的词序列。 核心思想：在每步解码时，仅保留 $k$ 个最高分的部分候选序列（partial hypotheses），$k$ 即为束宽（beam size），通常取 5–10。 具体流程（见下图，束宽=2）： 每步计算若干候选项及其累积得分（通常为对数概率之和）； 保留 top-$k$ 路径继续扩展； 后续通过回溯获得完整输出。 不同候选可能在不同时间步生成 \u0026lt;END\u0026gt;： 一旦某候选产出 \u0026lt;END\u0026gt;，视为完成，暂存； 继续扩展其余候选，直至： 达到预设最大长度 $T$，或 已获得足够多（如 $n$ 个）完成候选。 得分归一化问题：较长序列通常累积得分更低（因每步概率 \u0026lt;1，连乘/求和后更小）→ 需按长度归一化（如使用平均对数概率）后再比较。 注意：束搜索不保证全局最优，但远优于穷举，兼顾质量与效率。\n详见：D2L.ai《动手学深度学习》— 束搜索章节。 约束束搜索（Constrained Beam Search） # 适用场景：需强制输出中包含特定词/短语（如机器翻译中必须包含某术语）。 基本思想：在束搜索过程中加入硬性约束条件，仅保留满足约束的候选路径。 实现方式： 修改评分函数，或 在每步生成后剔除违反约束的候选， 或引入惩罚项降低违规路径得分， 或用独立模块动态反馈约束满足情况。 示例：生成句子时需包含短语 “is fast”；\n除常规高概率词（如 “dog”、“nice”）外，强制加入 “is” 以推进约束达成（见下图）。 银行机制（Banking） # 强制插入 token 是否会导致荒谬输出？银行机制可解决此问题： 将候选按满足约束的程度分为多个“银行”（Bank）； Bank 2：已满足全部约束；\nBank 1：接近满足；\nBank 0：尚未开始满足。 采用轮询选择（round-robin）：依次从 Bank 2、1、0 中各选最高分候选，再从 Bank 2、1、0 选次高……\n（例：若用 3 束，则选出：[\u0026quot;The is fast\u0026quot;, \u0026quot;The dog is\u0026quot;, \u0026quot;The dog and\u0026quot;]） 这样既保证约束逐步满足，又维持高概率合理序列的竞争力。 下图为全流程结果： Top-$k$ 采样 # 核心思想：每步从概率最高的 $k$ 个 token中采样，而非仅选最大者。 采样方式可为： 均匀采样：top-$k$ 内各 token 等概率 → 提升多样性； 按原概率采样：保持分布权重 → 提升连贯性。 $k=1$ 时退化为贪心解码。 $k$ 越小 → 选择越窄 → 多样性↓、控制性↑；\n$k$ 越大 → 选择越宽 → 多样性↑、控制性↓。 适用于需平衡多样性与可控性的任务（如对话生成）。 Top-$p$（核心采样 / Nucleus Sampling） # 动机：Top-$k$ 中 $k$ 难以选取 → 需动态调整候选集大小。 Top-$p$ 方法： 按概率降序排列所有 token； 取最小的前缀子集，使其累积概率 ≥ $p$（如 $p=0.9$）； 重新归一化该子集概率（使其和为 1）； 从中按新概率采样。 与 Top-$k$ 关键区别：\nTop-$k$ 固定数量，Top-$p$ 固定概率质量；\n后者可根据分布“自适应”调整候选数。 实用价值 # 适合需精细调控多样性与流畅度的任务（如语言建模、摘要生成）； 实际中 $p$ 常设为 0.75 左右，以过滤长尾低概率噪声 token； 特殊情形： 若某 token 概率 \u0026gt; $p$，则必然被选（退化为贪心）； 若概率分布平坦，则候选集变大 → 更富创意。 注意：Top-$k$ 与 Top-$p$ 可联用（先取 top-$k$，再在其中做 top-$p$），但 $p$ 作用于 $k$ 之后。 与温度参数的关系 # OpenAI GPT-3 API 提示：温度与 top-$p$ 互斥（见下图）；\n二者是不同且互斥的随机性控制机制。 贪心 vs. Top-$k$ 与 Top-$p$ # 对比维度 贪心解码 Top-$k$/Top-$p$ 确定性 确定性（总是选最高概率） 随机性（引入采样） 采样方式 无（直接 argmax） 可均匀或按概率 文本风格倾向 安全、保守、缺乏创意 更新颖、多样，但可能不连贯 Min-$p$ 采样 # Min-$p$ 是 Hugging Face Transformers 库引入的新型解码策略，旨在改进 Top-$k$ 与 Top-$p$ 的不足。 现有方法局限回顾 # Top-$k$： 固定截断 → 可能丢弃高质量低频 token → 降低词汇多样性。 Top-$p$： 包含极低概率 token → 可能破坏连贯性。 Min-$p$ 核心思想 # 引入动态阈值：\n设定最小概率因子 min_p（如 0.05），\n计算阈值 = min_p × 最高概率 token 的概率；\n→ 仅保留概率 ≥ 该阈值的 token。 优势： 当存在绝对主导 token时 → 严格过滤，保证聚焦与连贯； 当概率分布较平坦时 → 宽松保留，支持创意发散。 推荐配置（尤其适合创意生成）： min_p ∈ [0.05, 0.1] 配合高温（$T \u0026gt; 1$）→ 充分激发创造力； 可减少甚至无需使用“重复惩罚（repetition penalty）”等补丁技巧。 简言之，Min-$p$ 通过自适应概率截断，在多样性与连贯性间取得更优平衡，有望成为新一代解码标准。 参考文献 # Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015). What is Temperature in LSTM (and neural networks generally)? Stanford CS224n Ketan Doshi, Foundations of NLP Explained Visually: Beam Search, How it Works Cohere: Top-k and Top-p HuggingFace: Constrained Beam Search 如需 PDF 或 Markdown 格式文档，我可为您整理输出。\n参考 # Token Sampling Methods qwen 翻译\n总结（Kimi） # 方法 是否随机 控制方式 优点 缺点 Greedy 否 argmax 快速、确定 易重复、局部最优 Beam Search 否 保留 top-k 序列 比贪心更优 不保证全局最优 Top-k 是 固定数量 简单有效 可能包含低质量词 Top-p 是 动态累积概率 更灵活 可能引入低概率词 Min-p 是 动态阈值 高温度下更稳定 新方法，需调参 "},{"id":19,"href":"/www6vAlgo/docs/LLM/Reasoning/kimi/k2-thinking/","title":"Kimi K2 Thinking","section":"kimi","content":" 0. 背景 # 核心关注三个问题：\n预训练阶段：如何用 MuonClip 优化器实现更高的 token 效率？ 后训练阶段：如何通过大规模 Agentic 数据合成和通用强化学习，让模型学会使用工具？ Test-Time Scaling：如何让模型在推理时进行长程思考和工具调用？ 1. 整体架构：从 K2 到 K2 Thinking # 1.2 K2 Thinking：加入 Test-Time Scaling # Kimi K2 Thinking 是在 K2 的基础上，通过额外的训练，让模型具备了 thinking 能力。它的核心特点是：\n边思考边使用工具：模型在推理过程中，会进行 think → search → browse → think → code 的循环，动态生成和验证假设 长程推理：可以执行 200-300 步连续的工具调用，保持推理的连贯性。（这点是让人比较惊喜的） Test-Time Scaling：通过增加推理时的 thinking tokens 和工具调用步数，提升模型性能 从架构上看，K2 Thinking = K2 + Thinking Ability + Test-Time Scaling。因此，理解 K2 的训练方法，就能理解 K2 Thinking 的 80%。\n2. 预训练 # 2.1 基于 MuonClip 优化器的 Token 效率优化 # 2.2 文本的改写优化 # K2 相比 K1.5 的一个关键进步是引入了合成数据生成策略来提高 token 利用率。核心思想是：通过精心设计的改写 pipeline，在不引入显著过拟合的情况下，扩大高质量 tokens 的数量。改写（Rephrasing） 就是数据合成的一种方式，主要是为了提高「高质量数据的占比」，尤其是「知识领域」和「数学领域」。：\n2.2.1 知识领域数据改写 # 2.2.2 数学领域数据改写 # 3. 后训练(重点) # K2 的增强 Agentic 能力源于两个重要方面：\n大规模 Agentic 数据合成 通用强化学习 3.1 大规模 Agentic 数据合成：教会模型使用工具 # 3.1.1 数据合成流程 # 具体流程如下：\n定义领域和工具：涵盖各种真实场景，如数据分析、网页开发、系统管理等 生成任务：所有任务都是基于 rubric 的（有明确的评分标准），确保一致的评估 模拟交互：Agents 与模拟环境和用户 agents 交互，创建真实的多轮工具使用场景 LLM 评判(LLM as judge)：根据任务 rubrics 评估模拟结果，过滤出高质量的训练数据 这个可扩展的 pipeline 生成了多样化、高质量的数据，为大规模拒绝采样和强化学习铺平了道路。\n3.2 通用强化学习：不可验证奖励 # 传统的强化学习主要应用于可验证奖励的任务，比如数学题（答案对错明确）和竞赛编程（能否通过测试用例）。但对于不可验证奖励的任务（如写研究报告、创意写作），传统 RL 就无能为力了。\n3.2.1 Self-Judging 机制 # 核心思想是：模型作为自己的评判者，为不可验证的任务提供可扩展的、基于 rubric 的反馈。\n具体做法：\n对于不可验证的任务，模型生成多个候选答案 模型自己根据 rubric 评估这些答案，给出分数 使用这些分数作为奖励信号，进行强化学习 3.2.2 用可验证奖励改进 Critic # 小结：通过大规模 Agentic 数据合成和通用强化学习，K2 学会了在各种场景下使用工具，并且能够处理可验证和不可验证的任务。这为 K2 Thinking 的长程推理能力打下了基础。\n4. K2 Thinking # 4.1 什么是 Test-Time Scaling？ # Test-Time Scaling 是指在推理时增加计算量，以提升模型性能。对于 K2 Thinking，这体现在两个方面：\n增加 thinking tokens：模型在生成答案前，会先生成大量的思考过程（类似 OpenAI o1，这其实就是 Long-CoT，这种技术在 Kimi-k1.5 就已经开始做了） 增加工具调用步数：模型可以执行 200-300 步连续的工具调用，进行长程规划（这是新增的，为了 Agentic 能力的提升） 这两者结合，使得 K2 Thinking 能够解决需要深度推理和多步操作的复杂问题。\n4.2 边思考边使用工具：Interleaved Reasoning # K2 Thinking 的核心能力是边思考边使用工具。它会进行动态的 think → search → browse → think → code 循环，这个循环可以重复数百次，直到找到答案：\nThink：分析问题，生成假设 Search：搜索相关信息 Browse：浏览网页，提取关键信息 Think：验证假设，调整策略 Code：编写代码，执行计算 4.4 小结 # 通过 test-time scaling，K2 Thinking 能够在推理时进行长程思考和工具调用，从而解决需要深度推理和多步操作的复杂问题。这使得它在 Agentic Reasoning、Agentic Search 和 Agentic Coding 任务上都达到了 SOTA 性能。（有点 claude 那味道了）\n5. 技术细节对比：K2 vs K2 Thinking # 维度 K2 (Instruct) K2 Thinking 模型类型 Non-thinking（无长思考） Thinking model（有长思考） 推理方式 直接生成答案 边思考边使用工具 工具调用 支持，但步数有限（其实也挺好的） 200-300 步连续调用 Test-Time Scaling 不支持 支持（thinking tokens + 工具调用） 适用场景 通用对话、快速响应 复杂推理、长程规划 参考 # Kimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程（含MuonClip优化、Agentic 数据合成等）\nKimi-K2 和 Kimi-K2-Thinking 深度解读：从预训练优化到 Agentic 能力训练的完整流程\n"},{"id":20,"href":"/www6vAlgo/docs/LLM/Reasoning/Qwen/Qwen3/","title":"Qwen3","section":"Qwen","content":" 论文 # https://github.com/QwenLM/Qwen3/\nArch [2] # Compare [1] # Post-training # 阶段三：思考模式融合 # **两种模式使用/think和/no_think标志进行区分，**注意“非思考模式”也有开始和结束的标志符，只是其思考过程置为空。并且在训练过程中，会针对多轮对话进行“思考”和“非思考”模式的混合训练。\n参考 # The Big LLM Architecture Comparison Understanding and Implementing Qwen3 From Scratch Qwen3技术报告的几点细节、ArXiv论文翻译实现方案及试错历程\n【LLM4】Qwen3-RL训练详解 ***\nup: 卢老师， 怀中猫\n"},{"id":21,"href":"/www6vAlgo/docs/LLM/Reasoning/Qwen/Qwen3-report/","title":"Qwen3 Report","section":"Qwen","content":" 简介 # Qwen 团队发布了 Qwen3，这是一个新的大型语言模型（LLM）系列，代表了开源人工智能领域的重大进步。Qwen3 涵盖了各种参数规模的密集和混合专家（MoE）架构，旨在平衡性能、效率和可访问性。\n这个最新版本建立在以前的 Qwen 模型之上，在推理能力、多语言支持和推理效率方面进行了重大改进。Qwen3 的一项关键创新是在单个模型中集成思考和非思考模式，无需在不同任务的专用模型之间切换。这些模型在 Apache 2.0 许可下发布，有助于开源人工智能技术生态系统的发展，并将 Qwen3 定位为 GPT-4o、Claude 3.7 和 Gemini 2.5 等专有模型的有竞争力的替代方案。\n模型架构与创新 # Qwen3 在 Qwen2.5 的基础上进行了多项架构增强：\n双模架构：该模型集成了思考和非思考模式，允许用户根据任务需求选择合适的模式。 混合专家（MoE）：Qwen3 系列包括 MoE 模型，这些模型以较少的激活参数在推理期间实现高性能，从而提高计算效率。 核心组件：该架构包含以下高级功能： 分组查询注意力（GQA） SwiGLU 激活函数 旋转位置嵌入（RoPE） RMSNorm 和 QK-Norm 模型变体：Qwen3 系列包括针对不同用例量身定制的多个模型： Qwen3-235B-A22B：一个旗舰级的 MoE 模型，总参数为 235B，但在推理期间只有 22B 处于激活状态 Qwen3-32B：一个旗舰级的密集模型 参数范围从 14B 到 0.6B 的较小模型 扩展上下文长度：这些模型支持高达 128K 的上下文长度，从而能够处理更长的文档和更复杂的交互。 训练方法 # Qwen3 采用一种复杂的、多阶段的训练过程，旨在增强各种能力：\n预训练数据：这些模型在包含 119 种语言和方言的 36 万亿个 token 的庞大数据集上进行训练，与以前版本中支持的 29 种语言相比，这是一个显着的扩展。\n三阶段预训练过程：\n一般知识获取 推理能力增强 长上下文适应 后训练对齐：一个四阶段的过程使模型与人类偏好对齐：\n基础模型 → Long-CoT 冷启动 → 推理 RL → 思考模式融合 → 通用 RL 每个阶段都针对特定的能力：\nLong-CoT 冷启动：引入思维链推理 推理 RL：强化准确的推理路径 思考模式融合：集成思考和非思考模式 通用 RL：增强整体模型对齐 数据整理：训练过程涉及多模态数据增强，包括从 PDF 中提取文本和生成合成数据，以及实例级数据混合优化。\n思考模式和预算 # Qwen3 最具创新性的功能之一是其思考模式和预算机制，这为用户提供了对模型推理深度进行细粒度控制的能力：\n思考模式与非思考模式： 非思考模式：针对简单任务进行了优化，可直接提供答案，无需大量推理。 思考模式：进行更深入的推理，展示复杂问题的工作和中间步骤。 思考预算： 用户可以指定一个“思考预算”（以千个 tokens 为单位），以控制模型应应用的推理量。这在两种模式之间创建了一个频谱，而不是二元选择。 性能相关性：如上图所示，增加思考预算可以持续提高各种基准测试的性能，包括 AIME 数学推理、LiveCodeBench 编程和 GPQA Diamond 科学推理。 数学表达式：思考预算和模型性能之间的关系可以近似表示为： \\(P(b) = P_{\\text{non-thinking}} + (P_{\\text{thinking}} - P_{\\text{non-thinking}}) \\cdot \\min\\left(\\frac{b}{b_{\\text{max}}}, 1\\right) \\) 其中 \\( P(b) \\) 是预算为 \\(b\\) 时的性能， \\(b_{\\text{max}}\\) 是最大有效预算。\n多语言能力 # Qwen3 代表了多语言支持方面的重大进步：\n扩展的语言覆盖范围：该模型支持 119 种语言和方言，比以前版本中的 29 种语言有了大幅增加。 预训练方法：多语言能力嵌入在预训练期间，并经过精心的数据管理，以确保不同语言的平衡表示。 性能改进：Qwen3 展示了增强的跨语言理解和生成能力，使其对全球用户更具可访问性。 语言分布：训练数据包括英语以外的各种语言，其中普通话和其他广泛使用的语言占了很大比例，并且改进了对低资源语言的覆盖。 强到弱的知识蒸馏 # 为了增强 Qwen3 的可访问性，该团队采用了强到弱的知识蒸馏方法：\n知识转移：来自较大模型（Qwen3-235B-A22B 和 Qwen3-32B）的知识被提炼到较小的模型中（范围从 14B 到 0.6B 参数）。 蒸馏过程：该过程包括训练较小的模型来模仿较大模型的行为，同时保持双重模式能力。 优点： 减少了部署所需的计算资源 保持了各种任务的性能 保持了思考和非思考模式能力 实施： # 用于强到弱的知识蒸馏的简化伪代码 def distill_knowledge(teacher_model, student_model, training_data): for batch in training_data: # 获取教师模型在思考和非思考模式下的输出 teacher_thinking_output = teacher_model(batch, mode=\u0026#34;thinking\u0026#34;) teacher_nonthinking_output = teacher_model(batch, mode=\u0026#34;non-thinking\u0026#34;) # 训练学生模型以匹配两种模式 student_thinking_loss = loss_fn(student_model(batch, mode=\u0026#34;thinking\u0026#34;), teacher_thinking_output) student_nonthinking_loss = loss_fn(student_model(batch, mode=\u0026#34;non-thinking\u0026#34;), teacher_nonthinking_output) # 更新学生模型的参数 total_loss = student_thinking_loss + student_nonthinking_loss total_loss.backward() optimizer.step() 性能和基准测试 # Qwen3 在各种基准测试中表现出具有竞争力的性能：\n通用任务：在衡量常识推理、阅读理解和知识检索的基准测试中表现出色。 数学与 STEM：数学推理能力显著提高，在 MATH、AIME 和 GPQA 等基准测试中得到验证。 编码：在 HumanEval 和 LiveCodeBench 等编程基准测试中表现出竞争优势，并且随着思考预算的增加，思考模式明显优于非思考模式。 多语言任务：在不同语言中性能得到增强，表明扩展多语言支持的有效性。 与其他模型的比较：Qwen3 在开源模型中取得了最先进的结果，并缩小了与更大的专有模型之间的差距。 效率优势：与具有相似能力的密集模型相比，MoE 模型以更少的激活参数表现出高性能。 开源贡献 # 在 Apache 2.0 许可下发布 Qwen3 代表了对开源 AI 社区的重大贡献：\n模型可访问性：完整的模型权重和代码通过 Hugging Face、ModelScope 和 GitHub 等平台提供。 文档和示例：提供全面的文档和示例代码，以方便采用和实验。 社区参与：开源发布能够更广泛地研究、开发和部署先进的 LLM。 AI 民主化：通过免费提供最先进的模型，Qwen3 有助于缩小大型组织与小型实体或个体研究人员之间的资源差距。 未来方向 # Qwen 团队概述了未来研究和开发的几个方向：\n扩展预训练：进一步增加预训练数据的大小和多样性，以增强模型能力。 架构改进：继续改进模型架构，以提高效率和性能。 扩展上下文长度：探索处理超出当前 128K 限制的更长上下文的技术。 增强推理：开发更复杂的推理控制和验证方法。 多模态能力：扩展到多模态理解和生成。 强化学习：增加专门用于强化学习的计算资源，以提高模型对齐和指令遵循能力。 总而言之，Qwen3 代表了开源大型语言模型的重大进步，提供了一套全面的模型，具有竞争力的性能、创新的思考控制机制、扩展的多语言支持和高效的架构设计。 在单个模型中集成思考和非思考模式，以及思考预算机制，为用户提供了前所未有的控制，可以控制应用于不同任务的推理深度，从而优化性能和效率之间的平衡。\n参考 # https://www.alphaxiv.org/zh/overview/2505.09388v1\n"},{"id":22,"href":"/www6vAlgo/docs/LLM/MoE/ModelMOECode/","title":"(代码)MOE","section":"MOE","content":" import torch import torch.nn as nn import torch.nn.functional as F import torch_npu from torch_npu.contrib import transfer_to_npu class Expert(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super().__init__() self.net = nn.Sequential( nn.Linear(input_dim, hidden_dim), nn.GELU(), nn.Linear(hidden_dim, output_dim)) def forward(self, x): return self.net(x) class MoE(nn.Module): def __init__(self, input_dim, num_experts, top_k, expert_capacity, hidden_dim, output_dim): super().__init__() self.num_experts = num_experts self.top_k = top_k self.expert_capacity = expert_capacity # 路由网络 self.gate = nn.Linear(input_dim, num_experts) # 专家集合 self.experts = nn.ModuleList( [Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)]) def forward(self, x): batch_size, input_dim = x.shape device = x.device # 路由计算 logits = self.gate(x) probs = torch.softmax(logits, dim=-1) topk_probs, topk_indices = torch.topk(probs, self.top_k, dim=-1) # 辅助损失计算 if self.training: # 重要性损失（专家利用率均衡） importance = probs.sum(0) importance_loss = torch.var(importance) / (self.num_experts ** 2) # 负载均衡损失（样本分配均衡） mask = torch.zeros_like(probs, dtype=torch.bool) mask.scatter_(1, topk_indices, True) routing_probs = probs * mask expert_usage = mask.float().mean(0) routing_weights = routing_probs.mean(0) load_balance_loss = self.num_experts * (expert_usage * routing_weights).sum() aux_loss = importance_loss + load_balance_loss else: aux_loss = 0.0 # 专家分配逻辑 flat_indices = topk_indices.view(-1) flat_probs = topk_probs.view(-1) sample_indices = torch.arange(batch_size, device=device)[:, None]\\ .expand(-1, self.top_k).flatten() # 初始化输出 outputs = torch.zeros(batch_size, self.experts[0].net[-1].out_features, device=device) # 处理每个专家 for expert_idx in range(self.num_experts): # 获取分配给当前专家的样本 expert_mask = flat_indices == expert_idx expert_samples = sample_indices[expert_mask] expert_weights = flat_probs[expert_mask] # 容量控制 if len(expert_samples) \u0026gt; self.expert_capacity: expert_samples = expert_samples[:self.expert_capacity] expert_weights = expert_weights[:self.expert_capacity] if len(expert_samples) == 0: continue # 处理专家计算 expert_input = x[expert_samples] expert_output = self.experts[expert_idx](expert_input) weighted_output = expert_output * expert_weights.unsqueeze(-1) # 累加输出 outputs.index_add_(0, expert_samples, weighted_output) return outputs, aux_loss # 测试示例 if __name__ == \u0026#34;__main__\u0026#34;: input_dim = 128 output_dim = 256 num_experts = 8 top_k = 2 expert_capacity = 32 hidden_dim = 512 batch_size = 64 # add device = torch.device(\u0026#34;npu:4\u0026#34; if torch.npu.is_available() else \u0026#34;cpu\u0026#34;) moe = MoE(input_dim, num_experts, top_k, expert_capacity, hidden_dim, output_dim).to(device) x = torch.randn(batch_size, input_dim).to(device) experimental_config = torch_npu.profiler._ExperimentalConfig( export_type=torch_npu.profiler.ExportType.Text, profiler_level=torch_npu.profiler.ProfilerLevel.Level0, msprof_tx=False, aic_metrics=torch_npu.profiler.AiCMetrics.AiCoreNone, l2_cache=False, op_attr=False, data_simplification=False, record_op_args=False, gc_detect_threshold=None ) with torch_npu.profiler.profile( activities=[ torch_npu.profiler.ProfilerActivity.CPU, torch_npu.profiler.ProfilerActivity.NPU ], schedule=torch_npu.profiler.schedule(wait=0, warmup=0, active=1, repeat=1, skip_first=1), on_trace_ready=torch_npu.profiler.tensorboard_trace_handler(\u0026#34;./moe_stand_npu_result\u0026#34;), record_shapes=False, profile_memory=False, with_stack=False, with_modules=False, with_flops=False, experimental_config=experimental_config) as prof: # 训练模式 for _ in range(10): moe.train() output, loss = moe(x) print(f\u0026#34;Using device: {x.device}\u0026#34;) print(f\u0026#34;Training output shape: {output.shape}\u0026#34;) # torch.Size([64, 256]) print(f\u0026#34;Training auxiliary loss: {loss.item():.4f}\u0026#34;) # 示例值，如0.1234 prof.step() print(\u0026#34;=\u0026#34; * 80) # 推理模式 moe.eval() output, _ = moe(x) print(f\u0026#34;Eval output shape: {output.shape}\u0026#34;) # torch.Size([64, 256]) 参考 # MoE git\n"},{"id":23,"href":"/www6vAlgo/docs/LLM/MoE/MoE/","title":"(原理)MoE","section":"MOE","content":" Mixture of Experts (MoE) 文章重点归纳 # Overview # MoE通过条件计算范式解决模型扩展瓶颈，针对不同输入选择性激活参数子集，实现近乎线性参数扩展而无需成比例增加计算成本 MoE概念起源于1991年Jacobs等人的工作，建立了\u0026quot;门控\u0026quot;和\u0026quot;专家\u0026quot;的基础原则 关键演进里程碑: 稀疏门控革命 (2017): Shazeer等人引入top-k路由机制，大幅降低计算量同时保持性能，使得训练数十亿参数神经网络成为可能 简化扩展 (2021): Fedus等人简化MoE架构，使用top-1路由，大幅降低通信开销 结构化稀疏性 (2022): Dropless MoE将稀疏MoE计算重构为块稀疏矩阵乘法，移除令牌\u0026quot;丢弃\u0026quot;需求 现代影响: 当前模型如Mixtral-8×7B、DeepSeek-V2、Gemini 1.5和Claude 3将MoE原则应用于多模态对齐和融合 MoE已从集成学习技术演变为可扩展智能的核心架构原则，实现计算支出与信息复杂度的一致性 Mixture-of-Experts: The Classic Approach # MoE是集成学习技术，将复杂预测问题分为子任务，训练专家在特定子任务上表现最佳 架构元素: 数据集分区: 将预测问题分为子任务，基于特征与标签之间的关系相关性 专家模型: 专门处理特定子任务的神经网络层 门控网络 (路由器): 估计输入数据与各专家的兼容性，输出softmax分布 池化方法: 聚合机制，基于门控网络和专家输出进行预测 专家和门控网络联合训练以最小化整体损失函数 MoE的核心优势是效率，通过动态选择每个输入的参数子集(专家)，实现更大模型同时保持计算成本可控 Hands-On Exercise: How does an MoE model work? # 演示配置: 2个专家，2个令牌，稀疏激活 工作流程: MoE块接收两个令牌(蓝色、橙色) 门控网络处理X₁(蓝色)并确定激活专家₂ 专家₂处理X₁(蓝色) 门控网络处理X₂(橙色)并确定激活专家₁ 专家₁处理X₂(橙色) ReLU激活函数处理专家输出并产生最终输出 主要优势: 规模: 通过添加更多专家轻松扩展模型 效率: 门控网络只为每个令牌选择部分专家计算，大幅降低计算成本 The Deep Learning Way: Sparsely-Gated MoE # 2017年Shazeer等人提出适用于深度学习的MoE扩展 传统密集模型面临训练成本二次方增长问题，而条件计算存在挑战: GPU/TPU在算术操作上优于网络分支 条件计算会减少批次大小 网络带宽限制计算效率 需要损失项以达到所需稀疏度 稀疏门控MoE层: 由多个专家网络和可训练门控网络组成 门控网络动态选择少量专家处理每个输入 通过噪声Top-K门控机制添加高斯噪声，确保门控稀疏性 门控网络和专家通过反向传播联合训练 该架构已成为LLM领域的游戏规则改变者，使模型容量扩展而计算复杂度几乎保持不变 The \u0026ldquo;How\u0026rdquo; Behind MoE # MoE成功背后的机制尚不完全清楚 专家模型初始化和训练方式相同，门控网络通常配置为均匀分配数据 有趣的是，专家能够\u0026quot;专业化\u0026quot;于不同任务，且不会崩溃为单一模型 Chen等人研究指出，\u0026ldquo;基础问题的聚类结构和专家的非线性对MoE成功至关重要\u0026rdquo; 这一简单而有效的MoE方法仍有待深入理解 Expert Capacity and Capacity Factor # 专家容量定义了每个专家在训练/推理步骤中可处理的令牌/样本/激活的上限 容量公式 (来自Switch Transformer): expert_capacity = (T/N) × α T = 批次中的令牌数 N = 专家数量 α = 容量因子(超参数) 历史演进: 早期条件计算研究缺乏明确容量概念 2017年稀疏门控MoE揭示了显式负载控制需求 2020年GShard在系统层面处理容量 2021年Switch Transformer正式定义专家容量公式和容量因子 专家容量的作用: 作为控制机制保持平衡令牌路由 作为稳定性约束防止计算过载 通过容量因子(α \u0026gt; 1.0)提供安全缓冲 在分布式训练中作为通信边界 实用考虑: 选择容量因子(默认: top-1路由α=1.25，top-2路由α=1.0) 持续监控路由分布和丢弃率 适当硬件和内存配置 动态容量调整 Load Balancing # 负载平衡确保MoE模型中所有专家被均匀使用，防止一些专家过载而其他专家未充分利用 负载不平衡影响: \u0026ldquo;富者更富\u0026quot;效应—少数专家获得更多令牌，快速改进，而其他专家停滞不前 总负载计算 (Switch Transformer): 令牌分配比例: f_i = (1/T) ∑ 1{expert(x)=i} 路由概率平均: P_i = (1/T) ∑ p_i(x) 专家i的总负载: Load_i = f_i × P_i 负载平衡损失函数: 辅助负载平衡项: L_bal = λN ∑(f_i P_i) 其中λ控制正则化强度 潜在解决方案: 正则化项: 添加惩罚不均匀专家使用 门控网络设计: 采用带高斯噪声的top-k门控 专家容量约束: 定义每专家容量上限 MegaBlocks方法: 通过块级并行性和结构化稀疏性改进负载平衡 Expert Choice Routing # 传统MoE模型面临专家利用不足问题，某些专家过载而其他专家训练不足 专家选择(EC)路由 (Zhou等人，2022): 颠覆了路由逻辑—从\u0026quot;令牌选择专家\u0026quot;变为\u0026quot;专家选择令牌\u0026rdquo; 专家根据其容量和亲和力独立选择要处理的令牌 工作流程: 令牌到专家评分: 计算令牌-专家分数矩阵S 专家容量定义: C_e = 容量因子 × (T/E) 专家令牌选择: 每个专家独立选择top-k令牌 排列和数据混洗: 重组令牌以实现计算效率 专家计算和输出重组: 专家处理分配的令牌并重新组合 优势: 负载平衡效率高 专家专业化改善 减少丢弃和填充开销 增强可扩展性 动态令牌优先级 挑战: 路由复杂性增加 通信开销 超参数敏感性 梯度路由挑战 专家专业化漂移 实际实现复杂性 Mixture-of-Experts Beyond MLP Layers # 传统上，MoE层主要集成到Transformer架构的前馈(MLP)块中 动机扩展: 专家不应仅限于MLP层—现在扩展到注意力、连接器和编码器 启用跨模态的语义专业化 解决不同模态或架构组件需要不同专业化的问题 注意力层中的MoE: **MoA(注意力混合)**概念: 用一组注意力专家替代单一自注意力机制 每个专家专注于不同的令牌依赖关系或上下文类型 模态编码器和连接器中的MoE: 视觉编码器: CuMo集成稀疏Top-K MoE块到视觉编码器 连接器和适配器: 专家专门化的适配器改进模态对齐 跨模态和多模态MoE: 作为跨模态融合机制，如在CLIP风格架构中 联合MoE架构: 注意力和前馈层都是专家式的，创建分层稀疏模式 理论和实践意义: 路由复杂性增加 负载平衡变成多维 专家可转移性提高 Routing Beyond Tokens: Structural and Hierarchical Routing Paradigms # 早期MoE框架将每个令牌视为独立路由单元，忽略结构关系 动机: 语义和结构一致性: 相关令牌应由相同专家处理 避免令牌碎片化: 独立路由碎片化语义相关令牌 捕捉令牌间依赖关系 效率和可解释性 可扩展性和负载平衡 结构和概念感知路由: 基于聚类的路由: 门控网络学习隐式令牌聚类 概念驱动的专业化: 专家与输入空间的概念区域关联 分层路由架构: 多级专家图: 全局路由器选择专家组，局部路由器在组内选择最终专家 分层负载平衡: 递归定义组级负载 基于图和注意力的路由: **令牌图(GoT)**框架: 将路由建模为消息传递过程 自适应和令牌组路由: 动态令牌分组: 令牌自适应决定激活多少专家 优势与挑战: 优势: 专业化改善、路由动态稳定、可解释性高、效率提升 限制: 潜在专业化、计算开销、负载平衡挑战 开放问题: 专家标记、扩展到万亿参数模型、语义连贯性评估 Limitations and Disadvantages of Mixture-of-Experts Architectures # 训练不稳定和负载不平衡: 某些专家接收大部分路由分配，其他专家未充分利用 需要辅助负载平衡损失和专家选择路由 通信开销和硬件依赖: 分布式环境中引入大量all-to-all通信开销 需要自定义内核和高速TPU互连，限制在商品硬件上的应用 路由复杂性和梯度碎片化: 门控机制添加显著复杂性和非可微性 离散性质破坏梯度流，导致梯度碎片化 模型容量利用不足: 每个令牌只激活总参数的小部分(通常1-2个专家) 巨大参数库中大部分在大多数前向传递中闲置 推理不稳定性和延迟变化: 相同输入序列可能激活不同专家，导致不可预测的延迟 批量推理放大此问题，集体同步延迟主导运行时间 高VRAM和内存驻留要求: 所有专家必须同时加载到GPU内存中 内存占用与完整密集模型相当，尽管每前向传递只使用少数专家 限制推理批处理大小和并行吞吐量 Expert Parallelism # **专家并行(EP)**是一种模型并行策略，将不同专家子网络分布到不同设备上 定位: 与数据并行(DP)、张量并行(TP)、流水线并行(PP)并列 EP是一种特定于MoE架构的模型并行形式 动机: 参数规模: 分布大量参数跨设备 FLOP/激活效率: 仅路由到少量专家，每输入执行更少FLOPs 内存效率: 专家不全部激活，减少每设备峰值内存占用 可扩展性: 扩展专家数量而不线性增加每令牌计算预算 设备分区、令牌路由和通信机制: 概念概述: 专家跨多个设备分区，令牌动态路由 通信流程: 令牌分组→all-to-all分派→本地专家计算→all-to-all收集→组合 负载平衡: 辅助负载平衡损失确保均匀令牌分布 通信-计算权衡: 优化通信时间与计算时间比率 混合并行策略: 结合DP+TP+EP实现三维扩展 容量管理和自适应令牌-专家分配: 容量因子: 每专家最大令牌处理数 令牌丢弃: 超出容量的令牌被丢弃或重新分配 动态容量调整: 基于路由统计自动调整每专家容量 路由策略: 包括噪声Top-k门控、负载平衡路由、专家选择路由等 What\u0026rsquo;s Next? # 理论理解: 需要更深入理解MoE架构及其工作原理 门控机制设计: 探索更有效的门控机制和专家模型，专家选择路由提供有希望的方向 领域扩展: 探索MoE在强化学习、表格数据等领域应用 未来前景: MoE范式将通过将复杂任务分为由专业专家模型处理的更简单子任务，继续推动深度学习边界 Popular MoE Models # GPT-4 (据传): 可能是8路MoE模型，总计约1.76T参数 16个专家，每个约111B参数，每前向传递路由2个专家 每前向传递仅使用约280B参数(560 TFLOPs)，而密集模型需要1.8T参数(3,700 TFLOPs) 训练于约13T令牌，使用8路张量并行和15路流水线并行 Mixtral 8x7B: Mistral的8x7B MoE模型(56B参数) 每层8个专家，每令牌选择2个专家 Apache 2.0许可下免费使用 性能超过Llama 2 70B，推理速度快6倍 匹配或超过GPT-3.5，在多语言任务上表现优异 32K上下文长度 OpenMoE: 最早的开源MoE实现之一 Colossal AI提供PyTorch OpenMoE实现，包括训练和推理的专家并行 Learning Resources # A Visual Guide to Mixture of Experts (MoE): 深入探讨MoE架构 详细讨论各专家学习内容、专家间路由方法、视觉MoE 参考 # MoE qwen-max 总结\n"},{"id":24,"href":"/www6vAlgo/docs/RL/core/PPO-family/PPO/","title":"(原理)PPO","section":"PPO family","content":"\nPPO # (原理)PPO\n"},{"id":25,"href":"/www6vAlgo/docs/RL/core/PPO-family/PPO1/","title":"(原理)PPO","section":"PPO family","content":" PPO训练中四种模型的合作关系 # PPO训练中各模型的输入与输出 # 基于PPO进行RLHF训练的原理图 # 参考 # 第8部分：RLHF 与 RLAIF\n"},{"id":26,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningBatchsize/","title":"(原理)Batchsize","section":"网络优化","content":"\n最佳实践 # batchsize # batchsize 下限 [1] 别太小的限制在于，batch size太小，会来不及收敛。\n所以在常见的setting（～100 epochs），batch size一般不会低于16。\nbatchsize 上限 [1] batch size别太大的限制在于两个点，\n1）batch size太大，memory容易不够用。这个很显然，就不多说了。\n2）batch size太大，深度学习的优化（training loss降不下去）和泛化（generalization gap很大）都会出问题。\nlearning rate \u0026amp; batch size # 总之，可以证明，learning rate/batch size的比值对深度学习是有指数级的影响[3]，所以非常重要，没事别瞎调。[1]\n这也是为什么大的batch_size往往建议可以相应取大点learning_rate, 因为梯度震荡小，大learning_rate可以加速收敛过程，也可以防止陷入到局部最小值，而小batch_size用小learning_rate迭代，防止错过最优点，一直上下震荡没法收敛（这也是一个小trick）。[2]\n参考 # 怎么选取训练神经网络时的Batch size? Summer Clover 训练神经网络时batchsize扩大一倍的同时需要增加epoch数量吗? 新一 7.1 批大小调整实验 百度邱\n7.1 批大小调整实验\n设置BatchSize\n深度学习中的batch的大小对学习效果有何影响？\n"},{"id":27,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningForwardBackward/","title":"(原理\u0026实战)前向/反向传播","section":"basic","content":"\n前向/反向传播 # (原理\u0026amp;实战)前向/反向传播\n"},{"id":28,"href":"/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/Dropout/","title":"(原理\u0026实战)Dropout","section":"正则化","content":"\nDropout # (原理\u0026amp;实战)Dropout\n"},{"id":29,"href":"/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLModel/","title":"机器学习-模型","section":"机器学习","content":"\n机器学习-模型 # 机器学习-模型\n"},{"id":30,"href":"/www6vAlgo/docs/DeepLearning/Transformer/SelfAttention/","title":"(原理)Self-Attention","section":"Transformer","content":"\nSelf-Attention # (原理)Self-Attention\n"},{"id":31,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Embedding/survey/","title":"(Survey)Embedding","section":"Embedding","content":" 论文 # 论文地址\nOn The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey 通用文本嵌入（GPTE）模型的典型架构和训练方式 # 通用文本embedding的代表模型及参数 # 参考 # Embedding的9点总结-从架构、数据到代表模型\n"},{"id":32,"href":"/www6vAlgo/docs/LLM/Survey/LargeModel/","title":"大模型","section":"Survey","content":" 参考 # 1xx. [译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023） 实战\n1xx. 通向AGI之路：大型语言模型（LLM）技术精要 ***\n1xx. 必看的十二个大模型前沿综述：兼论HALO大模型幻觉检测与缓解方案及Google小模型预测大模型训练不稳定的探索 12个综述\n"},{"id":33,"href":"/www6vAlgo/docs/LLM/Core/Emergent/","title":"(原理)涌现现象","section":"Core","content":" Emergent Abilities # 🔗 文章：Emergent Abilities of Large Language Models (2022.10) (arxiv.org) 🔑关键词和摘要 Keywords: LLMs, Emergent Ability, Scaling abstract 不可预测 不能从小模型的的性能外推 是否能通过继续扩大模型规模来获得更多涌现能力 ⚙️研究设计和结论 定义 通常的涌现现象 大模型的涌现现象 小模型接近随机 大模型突然出现 相变 实验框架 performance vs 1. FLOPs, model parameters Training datasets 叠甲：emergent 与很多因素都有关，本文并不是说到哪个 scale 就会出现 emergent，而是说 emergent 现象普遍存在。 实验1 Few-shot Prompting 测试数据说明: A: 三位数加法，两位数乘法 B: [dɪfərənt], 复原 \u0026ldquo;different,\u0026rdquo; C: 从 e l h l o 复原 hello D: 波斯语问答 E: 针对GPT-3 对抗标的问答 \u0026hellip; 结果 这些 task，以 few-shot 形式展示过以后，都有 emergent 不同模型 emergent scale 不一样 有的 task，只有 540B 的 PaLM emerge了 实验2 增强语言模型能力的 emerge 现象 已知的一些大模型技巧在何种规模下发挥作用？ 大模型技巧 思维链 Chain-of-thought: Let\u0026rsquo;s think step by step. 指令微调 请写一段XXX的描述 草稿本方法： 计算 15+16, 让模型在草稿本上写“5+6=11，进位1” 这些增强语言模型能力的方法都有一定程度的涌现 联想：之前的 prompt tuning，parameter efficient tuning，都是某种随着模型规模扩大的涌现？ 讨论 Emergent 现象的解释 多步能力说 每个子能力达到 90% -\u0026gt; 一无是处 每个子能力达到 95% -\u0026gt; 能完成一些任务了 指标缺陷说 奇怪的现象：交叉熵损失不是 emergent 的，而是在逐步下降 Emergent 的阈值可能会越来越小 更干净的数据，更好的训练技巧，更优秀的模型结构都可以是 Emergent阈值变小 未来方向： 继续扩大 model scale，远未达到上限 一些新结构的 scaling 数据的 scaling 理解 prompt 机制 更前沿的 task，用来指导 emergent 理解 emergence 📚论文贡献 优点 第一次正式提出 emergent 实验 做了充分的实验表明该现象在各种数据集上广泛存在 甚至验证了一些“方法”的涌现 提出了一些解释该现象的观点，并提出质疑 改进点 还是不知道为啥 emerge 实验采用各种不同模型，无法得出哪个计算量级对哪种能力有 emerge 参考 # 清华博士带你思考大语言模型LLM的涌现现象（Emergent） 有脑图\nEmergent Abilities of Large Language Models （https://arxiv.org/abs/2206.07682）\n再谈ChatGPT等大模型的涌现能力：关于涌现能力的定义、测试方法及分析工作总结 "},{"id":34,"href":"/www6vAlgo/docs/LLM/Dense/Llama/","title":"LLaMA","section":"Dense","content":" LLaMA # LLaMA\n"},{"id":35,"href":"/www6vAlgo/docs/RL/Deep-Research/Search/websailer/","title":"WebSailor","section":"Search","content":" 论文 # 《WebSailor: Navigating Super-human Reasoning for Web Agent》阿里巴巴集团通义实验室\n主要介绍了一种名为WebSailor的新型网页智能体（Web Agent），其在复杂信息寻求任务中展现了超越人类的推理能力。 以下是对文档的深度阅读总结，涵盖其核心问题、方法、实验及贡献。\n一、研究背景与问题定义 # 1.1 信息寻求的挑战 # 互联网时代的信息爆炸超越了人类认知极限（有限记忆、脆弱注意力、无法并行探索）。 专有智能体系统（如OpenAI的Deep Research）已展现出超越人类的性能，但开源智能体仍存在巨大性能差距。 1.2 任务分级 # 论文将信息寻求任务分为三个级别：\nLevel 1：低不确定性任务（如单次搜索即可解决）。 Level 2：高初始不确定性但解决路径清晰（如标准多跳问答）。 Level 3（本文焦点）：高不确定性且解决路径复杂、无预定义路径（如BrowseComp基准中的任务）。 1.3 性能差距根源 # 现有训练范式集中于Level 1–2任务，缺乏对Level 3复杂推理模式的暴露，导致模型无法发展出多步推理能力。\n二、核心方法：WebSailor框架 # 2.1 训练数据合成（SailorFog-QA） # a) 构建复杂知识图 # 通过随机游走从真实网站中提取互联的知识结构，生成具有涌现式非线性结构的图。 b) 生成高不确定性问题 # 采样多样化拓扑的子图（包含新颖的实体与关系组合）。 信息模糊化处理（如将精确日期转为“2010年代初”，名称部分掩码等），强制智能体进行推理而非简单查找。 c) 优势 # 数据基于真实互联网，贴合实际挑战。 子图拓扑多样性自然产生需复杂推理模式（多步演绎、组合与比较分析）的问题。 高度可扩展（子图数量随图规模非线性增长）。 2.2 推理轨迹重建 # a) 问题 # 开源大型推理模型（LRM，如QwQ、DeepSeek-R1）能生成正确轨迹，但其原生推理输出冗长、风格化，直接用于微调会限制智能体的探索策略泛化能力，且长轨迹易超出上下文窗口限制。\nb) 解决方案 # 用LRM生成完整动作-观察序列（丢弃原生冗长思考）。 用另一指令遵循模型（如Qwen-2.5-72B）为每一步动作重建简洁、目标导向的思考（“短链思维”风格），形成高质量监督信号。 2.3 训练流程优化 # a) 冷启动（RFT） # 必要性：RL奖励稀疏（初始近零反馈），且蒸馏依赖低（仅需2k+高质量样本）。 过滤：保留正确轨迹、长度＜32k token、工具调用＞5次（确保复杂性）。 训练目标：增强决策能力（掩码环境观察的损失计算）。 b) 强化学习（DUPO算法） # 挑战：多轮推理与工具使用导致训练缓慢。 创新：复制采样策略优化（训练前过滤全正确样本，训练中复制同一批次内标准差非零的样本），提速2–3倍。 奖励设计：结合格式验证（0.1权重）和答案验证（0.9权重），避免奖励黑客。 三、实验结果 # 3.1 基准测试 # BrowseComp-en/zh：最具挑战性的网页浏览基准，需复杂策略。 GAIA：需多模态与工具使用（仅用文本子集）。 Xbench-DeepSearch：动态深度搜索基准。 3.2 性能对比 # WebSailor（3B/7B/32B/72B）在所有开源模型与智能体方法中领先，且超越部分结合浏览能力的专有LRM（如Grok-3、Doubao）。 WebSailor-72B在BrowseComp-zh上与Doubao持平，虽仍落后于DeepResearch（SOTA），但显著缩小了开源与专有系统的差距。 向下兼容性：在简单任务（如SimpleQA）上也表现优异。 3.3 关键分析 # 数据复杂性：SailorFog-QA的工具调用分布与BrowseComp-en高度相似（长尾、多＞5次调用），而WebDancer数据集中＞50%仅需2次调用。 RL有效性：RL训练显著提升Pass@1性能（尤其BrowseComp），增强样本效率与稳定性。 冷启动必要性：无冷启动的RL模型工具调用数更低，无法掌握长视距推理，性能差距大。 四、局限与未来工作 # 上下文长度限制（32k token）可能制约更复杂问题的解决。 过度思考倾向：对简单问题也进行多步工具调用（但常为交叉验证，非无意义探索）。 训练效率：同步RL框架低效（仅50步），未来将转向异步训练。 五、结论 # WebSailor通过合成高不确定性数据、重建简洁推理轨迹、以及RFT冷启动与DUPO算法，实现了开源智能体在复杂信息寻求任务上的突破性进展，证明了开源模型可达到接近专有系统的性能。未来将继续探索更复杂任务与高效RL训练，以追求更广泛的“超人类”性能。\n附录与案例 # 文档还提供了：\n工具细节（search、visit）； QA构建流程； 训练超参数； 完整轨迹案例（如BrowseComp-en中关于Joey Hess的查询），展示多步推理与工具调用的实际应用。 总结 # 该研究在数据合成、推理重建和训练优化方面均有显著创新，为开源社区提供了可复现的高性能网页智能体方案，推动了对“超人类推理”能力的探索。\n参考 # 本文由元宝生成\nAgent智能体 | 深入解读阿里开源Web Agent新王者：WebSailor\n"},{"id":36,"href":"/www6vAlgo/docs/RL/core/PPO-family/RewardModel/","title":"(原理|实现)PPO-RewardModel","section":"PPO family","content":"\nPPO-RewardModel # (原理|实现)PPO-RewardModel\n"},{"id":37,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningLoss/","title":"(原理\u0026实战)交叉熵损失","section":"basic","content":"\n交叉熵损失 # (原理\u0026amp;实战)交叉熵损失\n"},{"id":38,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningNorm/","title":"规范化 Norm","section":"网络优化","content":"\nNorm 作用[1] # dnn 的标准组件，稳定和加速训练过程\nBatch Norm[1] # reduce cross batch size mini-batch dimension 一般用于图像，不涉及到padding的问题；\nLayer Norm[1] # reduce cross hidden dim reduce across the feature dimension. 一般用于序列，一个 batch size 内存在 padding；\nRMSNorm: 对 LN 的一种变体，llama 💡 https://spaces.ac.cn/archives/9009 Pre LN: llama Post LN: attention is all you need llama在工程上使用Pre LN\n[pytorch] BN、LN、RMSNorm 及 pre LN vs. post LN 对比，标准化 v *** ​\tnormalization.ipynb\n​\t[pytorch] BN、LN、RMSNorm 及 pre LN vs. post LN 对比，标准化 1xx. Batch Normalization, Layer Normalization and Root Mean Square Layer Normalization: A Comprehensive Guide with Python Implementations\ntodo\n7.5 逐层规范化 百度邱 有代码\nhttps://aistudio.baidu.com/education/lessonvideo/3048901\n"},{"id":39,"href":"/www6vAlgo/docs/DeepLearning/Transformer/ModelGQA/","title":"(原理)GQA","section":"Transformer","content":"\n论文 # GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\nMHA vs. MQA vs. GQA [1] # MHA # 首先是原始的 MHA(Multi-Head Attention)，QKV 三部分有相同数量的头，且一一对应。每次做 Attention，head1 的 QKV 就做好自己运算就可以，输出时各个头加起来就行。\nMQA # 而 MQA 则是，让 Q 仍然保持原来的头数，但 K 和 V 只有一个头，相当于所有的 Q 头共享一组 K 和 V 头，所以叫做 Multi-Query 了。实现改变了会不会影响效果呢？确实会影响但相对它能带来的收益，性能的些微降低是可以接受的。\n能带来多大的收益呢，实验发现一般能提高 30%-40% 的吞吐。\n收益主要就是由降低了 KV cache 带来的。实际上 MQA 运算量和 MHA 是差不多的，可理解为读取一组 KV 头之后，给所有 Q 头用，但因为之前提到的内存和计算的不对称，所以是有利的。\nGQA # 而 GQA 呢，是 MHA 和 MQA 的折衷方案，既不想损失性能太多，又想获得 MQA 带来的推理加速好处。具体思想是，不是所有 Q 头共享一组 KV，而是分组一定头数 Q 共享一组 KV，比如上面图片就是两组 Q 共享一组 KV。\nMQA 和 GQA 形式在推理加速方面，主要是通过两方面来完成：\n降低了从内存中读取的数据量，所以也就减少了计算单元等待时间，提高了计算利用率； KV cache 变小了 head_num 倍，也就是显存中需要保存的 tensor 变小了，空出来空间就可以加大 batch size，从而又能提高利用率。 如果要用 MQA 和 GQA，可以是从头训练的时候就加上，也可以像 GQA 论文里面一样，用已有的开源模型，挑一些头取个 mean 用来初始化 MQA 或 GQA 继续训练一段时间。\nGQA \u0026amp; MQA [2] # 图 4.1 Multi-head attention 拥有 H 个查询、键和值头。Multi-query attention 在所有 查询头之间共享单个键和值头。Grouped-query attention 则在每个查询头组之间共享单 个键和值头，从而在多头和多查询注意力之间进行插值。\nFig. 4.1 Multi-head attention has H query, key, and value heads. Multi-query attention shares single key and value heads across all query heads. Grouped-query attention instead shares single key and value heads for each group of query heads, interpolating between multi-head and multi-query attention.\n参考 # 为什么现在大家都在用 MQA 和 GQA？ ***\nLLM学习系列1：大模型架构要点总结 主流大语言模型的技术原理细节 *** 腾讯 [架构] + 训练 + 微调\n1xx. 理解Attention:从起源到MHA,MQA和GQA *** 1xx. 深度解析Group Query Attention(GQA)为什么能给LLM decoder带来极大推理加速 1xx. 深度学习中的注意力机制：MHA、MQA和GQA\n1xx. 【研1基本功 （真的很简单）Group Query-Attention】大模型训练必备方法——bonus(位置编码讲解) v *** Nomolization[post, pre, sandwich] + Position Encoding[RoPE] + GQA代码 1xx. 一文通透各种注意力：从多头注意力MHA到分组查询注意力GQA、多查询注意力MQA 删除\n手写大模型组件之Group Query Attention，从 MHA，MQA 到 GQA\n"},{"id":40,"href":"/www6vAlgo/docs/LLM/Dense/LlamaFamily/","title":"LLaMA 家族","section":"Dense","content":" LLaMA 家族[1] # 项目 描述 数据集 LLaMa 基座模型 公开可用的数据集(1T token) Stanford Alpaca 结合英文语料通过Self Instruct方式微调LLaMA 7B Self Instruct from davinci-003 API(52K) Vicuna-13B 通过ShareGPT.com的7万条对话数据微调LLaMA(Alpaca基础之上, 多轮对话和长序列, full fine-tune) 用户共享对话(70K sample) BELLE 结合中文语料通过Self Instruct方式微调BLOOMZ-7B或LLaMA Chinese-LLaMA/Chinese-Alpaca 通过中文数据预训练/指令微调LLaMA 姜子牙系列模型Ziya-LLaMA-13B-v1 基于LLaMA-13B的中英文模型 ChatLLaMA(英文版) LLaMA的RLHF版 ColossalChat 通过self-instruct技术指令微调LLaMA且加上RLHF {% asset_img \u0026rsquo;llama2-famaly.jpg\u0026rsquo; %}\n参考 # 家族 # LLaMA的解读与其微调：Alpaca-LoRA/Vicuna/BELLE/中文LLaMA/姜子牙/LLaMA 2 *** 1xx. 我想学大模型，应该从哪个模型开始？LLaMA生态家谱整理和分析 1xx. NLP（九）：LLaMA, Alpaca, ColossalChat 系列模型研究\n1xx. \u0026laquo;千帆增强版 Llama 2-提升大模型对话指令遵循能力\u0026raquo; v\n1xx. 近期大模型动态：LLaMA-2-7B-32K的训练数据组织情况及面向儿童心理健康领域的微调模型推介 llama-2-7b-32k - LLaMA-2的上下文长度为4Ktoken。要将其扩展到32K上下文，该工作分成了三个部分：建模、数据和系统优化。\n实战 # 1xx. 从0到1复现斯坦福羊驼（Stanford Alpaca 7B） GPUs: 8 卡 A800 80GB GPUs\n汉化 # 1xx. 掘力计划 23 期-Linly-Chinese-LLaMA2 中文开源大模型方案分享 v\n"},{"id":41,"href":"/www6vAlgo/docs/LLM/challenge/Hallucination/","title":"(原理)幻觉问题","section":"Challenge","content":" 幻觉[3] # Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge. 大型语言模型中的幻觉通常是指模型生成不忠实、捏造、不一致或无意义的内容。作为一个术语，幻觉在某种程度上被推广到模型犯错的情况。在这里，我想将幻觉问题缩小到模型输出是捏造的， 而不是基于所提供的上下文或世界知识的情况。\nThere are two types of hallucination: 幻觉有两种类型：\nIn-context hallucination: The model output should be consistent with the source content in context. 上下文幻觉：模型输出应与上下文中的源内容一致。 Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so. 外在幻觉：模型输出应以训练前数据集为基础。但是，考虑到预训练数据集的大小，检索和识别每代冲突的成本太高。如果我们将预训练数据语料库视为世界知识的代理，我们基本上会尝试确保模型输出是真实的，并且可以通过外部世界知识进行验证。同样重要的是，当模型不知道某个事实时，它应该这么说。 Anti-Hallucination Methods[3] # RAG → Edits and Attribution # Self-RAG (“Self-reflective retrieval-augmented generation”; Asai et al. 2024) trains a LM end-to-end to learn to reflect on its own generation by outputting both task output and intermittent special reflection tokens. They created a supervision dataset for a critic model and a generator model by prompting GPT-4 and then distilled that into an in-house model to reduce inference cost. Self-RAG（“自反射检索增强一代”;Asai 等人，2024 年）通过输出任务输出和间歇性特殊反射令牌 ，端到端训练 LM 以学习反射自己的生成。他们通过提示 GPT-4 为 critic 模型和生成器模型创建了一个监督数据集，然后将其提炼成内部模型以降低推理成本。\nChain of Actions # Without grounding by external retrieved knowledge, we can design a process for using the model itself to do verification and revision to reduce hallucination. 在没有外部检索知识的基础的情况下，我们可以设计一个流程，使用模型本身进行验证和修改，以减少幻觉。\nDhuliawala et al. (2023) proposed a method named Chain-of-Verification (CoVe) based on a chain of actions to plan and execute verification. [10] Dhuliawala 等人（2023 年） 提出了一种名为验证链 （CoVe） 的方法，该方法基于一系列行动来计划和执行验证。\nRECITE (“Recitation-augmented generation”; Sun et al. 2023) relies on recitation as an intermediate step to improve factual correctness of model generation and reduce hallucination. The motivation is to utilize Transformer memory as an information retrieval mechanism. Within RECITE’s recite-and-answer scheme, the LLM is asked to first recite relevant information and then generate the output. Precisely, we can use few-shot in-context prompting to teach the model to generate recitation and then generate answers conditioned on recitation. Further it can be combined with self-consistency ensemble consuming multiple samples and extended to support multi-hop QA. RECITE （“朗诵增强生成”;Sun 等人，2023 年）依靠背诵作为中间步骤来提高模型生成的事实正确性并减少幻觉。动机是利用 Transformer 内存作为信息检索机制。在 RECITE 的背诵和回答方案中，要求 LLM 首先背诵相关信息，然后生成输出。准确地说，我们可以使用小镜头上下文提示来教模型生成背诵，然后生成以背诵为条件的答案。此外，它可以与使用多个样本的自一致性集成相结合，并扩展以支持多跳 QA。\ntodo\nSurvey # 论文 # 论文地址 A Survey of Hallucination in Large Foundation Models Paper 1xx. 大模型前沿热点最新综述：大模型微调遗忘、Agent智能体、幻觉及RAG检索增强模型推介 大模型微调遗忘 幻觉\n论文 # 论文地址 Siren\u0026rsquo;s Song in the AI Ocean: A Survey on Hallucination in Large Language Models Paper 1xx. 人工智能海洋中的塞壬之歌：大型语言模型LLM中的幻觉研究综述（一） 1xx. 大型语言模型的幻觉研究｜减轻及避免大模型LLM幻觉（二） 1xx. 值得一读的大模型生成幻觉研究综述：大模型幻觉的起因、评估以及减轻策略总结 幻觉 vs 事实性[1] # 幻觉主要是指LLM生成毫无根据或毫无根据的内容，幻觉可以理解为模型倾向于\u0026quot;生成与某些来源相关的无意义或不真实的内容\u0026quot;。这与事实性问题不同，后者强调模型学习、获取和利用事实性知识的能力。\n举例说明两者的区别：\n如果一个LLM在被要求创作\u0026quot;一个关于兔子和狼交朋友的童话故事\u0026quot;时，创作出了一个关于\u0026quot;兔子和狗交朋友\u0026quot;的故事，那么它就表现出了幻觉。不过，这并不一定是事实性错误。 如果生成的内容包含准确的信息，但与提示的具体内容有出入，那就是幻觉，而不是事实性问题。 例如，如果LLM的输出包含了比提示指定更多的细节或不同的元素，但事实仍然正确，这就是幻觉。\n相反，如果LLM避免给出直接答案，而是说\u0026quot;我不知道\u0026quot;，或者给出了一个准确的答案，但遗漏了一些正确的细节，那么这就是事实性问题，而不是幻觉。\n此外，值得注意的是，幻觉有时会产生一些内容，虽然与原始输入内容有偏差，但在事实方面仍然是准确的。\n解决方案[2] # Prompt 工程 *\nFew-shot 外部知识 *\nRAG 后处理 *\n实事检查 * 人工检查 * 提升数据质量\nPretraining的数据质量 SFT的数据质量 模型能力提升 *\n微调 参考 # 再看大模型事实性的界定、错误的起因、评估及前沿缓解方案：Survey on Factuality in LLMS\n降低大模型幻觉的5种方案 v\n减少大模型幻觉，你必须要掌握的 6 个方法！ v\nExtrinsic Hallucinations in LLMs ***\n【译】LLM中的外部幻觉\nWork # 再看大模型幻觉问题如何缓解 ：Chain-of-Verification-一种基于链式验证思想的自我修正工作解读 1xx. 也看缓解大模型幻觉的多阶段RAG框架：加入混合检索、过程理由生成与验证的方案 survey # 1xx. 大模型的幻觉问题调研: LLM Hallucination Survey\n1xx. 网络安全领域微调模型SecGPT：兼看大模型幻觉的度量方式、评估benchmark及RAG增强不同方式 大模型幻觉综述\n1xx. LLM之幻觉（一）：大语言模型幻觉解决方案综述\n"},{"id":42,"href":"/www6vAlgo/docs/LLM/Dense/Llama3-1/","title":"Llama3.1","section":"Dense","content":" Llama3.1 # Llama3.1\n"},{"id":43,"href":"/www6vAlgo/docs/RL/core/compare/","title":"(原理) 综述","section":"Core","content":" 算法 # 算法 核心思想 是否使用评论家 主要创新点 关键优势 目标问题 PPO 将策略更新限制在信任区域内以稳定学习。 是 截断代理目标函数。 稳定、鲁棒。 通用RL对齐(RLHF)。 DPO 通过分类损失直接在偏好对上优化策略。 否 将奖励重参数化为最优策略的函数。 简洁、稳定、无RM/RL循环。 通用RL对齐(RLHF)。 GRPO 使用一组样本的奖励统计量来估计优势。 否 基于组的优势估计。 内存/计算效率高。 资源密集型的推理任务。 DAPO 系统性地应用一套技术来解决大规模RL问题。 否 Clip-Higher、动态采样等的组合。 解决特定的训练病理问题。 规模化训练的可扩展性和稳定性。 Dr. GRPO 从GRPO目标中移除已识别的长度和难度偏差。 否 无偏的损失函数形式。 token效率更高，偏差更小。 GRPO中的长度/难度偏差。 GSPO 在序列级别执行重要性采样和截断。 否 序列级重要性采样。 极高的稳定性，尤其对MoE模型。 token级更新的不稳定性。 GMPO 使用奖励的几何平均值以对异常值保持鲁棒。 否 目标函数中使用几何平均。 对异常奖励值的鲁棒性。 奖励异常值导致的不稳定。 GFPO 在更新前根据行为指标过滤采样的轨迹。 否 对轨迹进行拒绝采样。 生成简洁、高效的回答。 回答长度膨胀问题。 LitePPO 组合归一化和损失聚合的最佳实践。 否 对现有技术的有原则配置。 以最小的复杂性实现高性能。 RL流程中的过度工程化。 演化脉络 # PPO：token-level，价值函数依赖，clip在token上。 GRPO：引入 group 原则，reward group内归一，无需value，但clip/S依然是token-level，variance大。 Dr.GRPO：修正GRPO的长度和方差归一偏置，不再token归一。 DAPO：进一步吸收众多工程技巧（Clip-Higher、Dynamic Sampling等）来缓解大模型RL的瓶颈，token-level范式不变。 GSPO：范式跃迁，off-policy与clip全部sequence-level，variance低，性能和算法纯粹性最佳，最新Qwen3 RL实践基础。\n核心公式对照表 # 参考 # PPO、DPO、GRPO及其变体（Dr. GRPO、DAPO、GSPO、GMPO、GFPO、LitePPO）策略优化算法综述 PO 系列工作解析 (一)：从PPO到GRPO/DAPO/Dr.GRPO再到GSPO的演化\n"},{"id":44,"href":"/www6vAlgo/docs/RL/core/DPO/","title":"(原理|实现)DPO","section":"Core","content":"\nDPO # (原理|实现)DPO\n"},{"id":45,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningGradOpt/","title":"(原理)梯度优化","section":"网络优化","content":"\n梯度优化 # Gradient accumulation # Gradient checkpointing [10] # 显存占用优化算法\nmemory usage 与 computation time 之间的 tradeoff ； gradient checkpointing\nIn deep neural networks, backpropagation requires storing intermediate activations for computing gradients during the backward pass.\n但是当层数变多时，存储所有的中间层的激活值（intermediate activations）非常地占用显存；\ngradient checkpointing\n选择性地重新计算（recompute）一部分的 intermediate activations 在反向传播过程中来缓解显存的压力；\nGradient Clipping (梯度裁剪) # 目的[21] # 梯度爆炸问题的常见应对方式为“梯度裁剪”，也就是通过“clip”方式来防止迭代中梯度值过大。\n两种常见形式[20] # 梯度范数裁剪（Gradient Norm Clipping）: 这种方法涉及计算所有参数梯度的范数（例如L2范数），如果这个范数超过了设定的阈值，就将梯度缩放到这个阈值以内。在PyTorch中，这可以通过 torch.nn.utils.clip_grad_norm_ 函数实现。 梯度值裁剪（Gradient Value Clipping）: 这种方法对每个参数的梯度值进行独立裁剪，确保它们不会超过一个设定的最大值或最小值。在PyTorch中，这可以通过 torch.nn.utils.clip_grad_value_ 函数实现。 参考 # overview # Performance and Scalability: How To Fit a Bigger Model and Train It Faster ***\ngradient accumulation # 1xx. [LLMs 实践] 11 gradient accumulation 显存优化 trick v\n​\tgradient_accumulation.ipynb\n​\t[ LLMs 实践] 11 gradient accumulation 显存优化 trick 1xx. Pytorch入门（7）—— 梯度累加（Gradient Accumulation）\n1xx. 聊聊梯度累加(Gradient Accumulation)\n1xx. What is Gradient Accumulation in Deep Learning?\n1xx. Performing gradient accumulation with Accelerate\n​\t使用Accelerate进行梯度累积\ngradient checkpointing # [LLMs 实践] 13 gradient checkpointing 显存优化 trick v ​\tgradient_checkpointing.ipynb\n​\t[LLMs 实践] 13 gradient checkpointing 显存优化 trick ​\tFitting larger networks into memory. *** 看动图\n​\tBackprop and systolic arrays.\nGradient Clipping # 梯度裁剪（Gradient Clipping） ​\thttps://github.com/pytorch/pytorch/blob/main/torch/nn/utils/clip_grad.py\n深度炼丹之梯度裁剪 1xx. 【深度学习】第6.2节 梯度裁剪\n1xx. 【Pytorch】梯度裁剪——torch.nn.utils.clip_grad_norm_的原理及计算过程\n1xx. PyTorch使用Tricks：梯度裁剪-防止梯度爆炸或梯度消失 ！！\n"},{"id":46,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningOverfitting/","title":"(原理)过拟合","section":"basic","content":"\n(原理)过拟合 # (原理)过拟合\n"},{"id":47,"href":"/www6vAlgo/docs/DeepLearning/Transformer/TransformerCode/","title":"(实战)Transformer","section":"Transformer","content":"\n参考 # 1xx. transformer.ipynb git Transformer代码实现\n1xx. Transformer transformer.py git\n1xx. [译] Transformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019） V, github Transformers from scratch\n1xx. 从零实现Transformer的简易版与强大版：从300多行到3000多行\n1xx. Transformer源码详解（Pytorch版本）\n"},{"id":48,"href":"/www6vAlgo/docs/LLM/challenge/ImpossibleTriangle/","title":"(原理)不可能三角","section":"Challenge","content":" 不可能三角[1] # 不可能三角 # 预训练模型之所以是划时代的进展，是它具备了中等尺寸（一张卡即可精调）和全任务SOTA的精调效果 而最近两年预训练模型都在往大尺寸发展，也就是具备了少样本效果，但他们的少样本效果依旧比不过中等模型的精调 弥补方法 # 优化size 对于减少模型尺寸，一条典型的故事线就是蒸馏。但其中仍存在两个问题：一是学生模型很难达到原始模型的效果，二是原始的大尺寸模型的推理效率太低 优化few-shot 对于提升少样本表现，数据增强是一个好办法，比如用无监督数据做自监督训练、或者基于其他模型生成一些伪样本，但这类方法依旧受限于现有标注样本的多样性，泛化性能提升有限 fine-tuning 对于提升精调表现和效率（其实也偏少样本），最近一个比较火的故事是prompt，但这种方式对prompt的设计非常敏感，同时效果也很难超过目前的有监督SOTA 其他 不可能三角 # 分布式系统 # CAP理论 C 一致性 A 可用性 P 分区 分布式存储 # RUM猜想 Read-overhead Update-overhead Memory-overhead 范式 # pretrain, finetune 范式[3] # 第三阶段范式\npretrain, prompt, predict 范式[3] # 第四阶段范式\n总结 # 根据不可能三角形， pretrain, finetune 范式[3] 向pretrain, prompt, predict 范式[3]的迁移是受大模型大小的影响\n参考 # 不可能三角 # 预训练模型的下一步？突破Impossible Triangle Impossible Triangle: What’s Next for Pre-trained Language Models? 微软朱晨光：预训练模型下一步怎么走？突破PLM的「不可能三角」 Go to Page self "},{"id":49,"href":"/www6vAlgo/docs/RL/framework/veRLConfig/","title":"(原理\u0026实战)veRL Config","section":"Framework","content":"\nveRL Config # (原理\u0026amp;实战)veRL Config\n"},{"id":50,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Embedding/Embedding/","title":"(原理)Embedding","section":"Embedding","content":" example [1] # 降维: t-SNE K-Means 聚类 文本搜索 相似度搜索 Embedding 价值 [2] # 降维 将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。 捕捉语义信息 Embedding不仅仅是降维，更重要的是，它能够捕捉到数据的语义信息。 泛化能力 由于Embedding能够捕捉到数据的一些内在规律，因此对于这些未见过的数据，Embedding仍然能够给出合理的表示 应用 [2] # 语义表示和语义相似度 词语关系和类比推理 上下文理解 文本分类和情感分析 机器翻译和生成模型 天梯榜 # mteb/leaderboard\nexample[3] # m3e模型 bge模型 参考 # embedding git\n《AI 大模型应用开发实战营》 03-大模型开发基础：Embedding\n一文通透Text Embedding模型：从text2vec、openai-ada-002到m3e、bge\n1xx. 如何选取RAG中的embedding模型 v ***\nhuggingface embedding模型排行榜\nSentence Bert Demo Repo git\n1xx. 引入任务Instruction指令的句子向量化方案：Instructor的实现思路及训练数据集构造方案\nRepo git\n1xx. 也看利用大模型进行RAG文本嵌入训练数据生成：兼看面向NLP任务的开源指令微调数据集 《Improving Text Embeddings with Large Language Models》\n1xx. 如何提高LLMs的文本表征(Text Embedding)能力?\n《Improving Text Embeddings with Large Language Models》\n1xx. 文本转向量教程s2——认识文本转向量方法（sbert本质和推理加速） V\n"},{"id":51,"href":"/www6vAlgo/docs/LLM/Core/Eval/","title":"测评 *","section":"Core","content":" 基础指标[1] # 分类任务\nAccuracy Precision Recall F1-score AUC-ROC 曲线 生成任务\nBLEU ROUGE METEOR 人工评估 回归任务\nMSE MAE R2 PPL 困惑度\n测评集 # MMLU C-EVAL Framework # OpenCompass 参考 # AI大模型面试题：5.模型微调怎么评估效果 1xx. 一些讨论：三张关于大模型微调方案的脑图及几点llama2等行业落地的问题思考 1xx. https://github.com/CLUEbenchmark/SuperCLUE-Llama2-Chinese\n1xx. 如何让自己的大模型榜单评分更高：也谈榜单评测评分的一些常见方案和典型案例 CEval # 1xx. 大模型落地的一些前沿观点：兼看知识图谱增强大模型问答的几个方案及CEVAL榜单评测启发 二、CEVAL榜单评测中能够得到一些启示\n1. C-Eval 数据集评测简明教程\n1xx. 大模型B端落地“牛刀杀鸡”的奇怪感觉：兼看CEVAl通用评测到金融、医疗两大垂域评测的转变 CEVAl\nFramework # https://opencompass.org.cn/home\n"},{"id":52,"href":"/www6vAlgo/docs/DeepLearning/Transformer/TrainTokenizer/","title":"Tokenizer","section":"Transformer","content":"\ntokenizer 分词 # 单词分词法 单字分词法 子词分词法 BPE [GPT系列], WordPiece 参考 # 1xx. 大模型词表扩充必备工具SentencePiece 1xx. NLP（二）：浅谈分词 1xx. https://www.bilibili.com/video/BV1vN411p7t2/ 1xx. 开源大模型如何更好地适应中文场景：LLAMA扩充词表、BLOOM裁剪词表基本原理与开源实现\n"},{"id":53,"href":"/www6vAlgo/docs/RL/Deep-Research/Search/Kimi-Researcher/","title":"Kimi-Researcher","section":"Search","content":" 论文 # Kimi-Researcher - End-to-End RL Training for Emerging Agentic Capabilities\n参考 # 强化学习智能体新模板：深入解析Kimi‑Researcher\n1xx. Kimi-Researcher：端到端强化学习驱动的自主智能体\n1xx. up: 有个视频 "},{"id":54,"href":"/www6vAlgo/docs/RL/core/unified/","title":"unified paradigm","section":"Core","content":" RL unified paradigm # RL unified paradigm # DPO # PPO # GRPO # 参考 # DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n"},{"id":55,"href":"/www6vAlgo/docs/LLM/Reasoning/kimi/1.5/","title":"Kimi1.5","section":"kimi","content":" 📌 0. 背景 # Kimi-K1.5 与 DeepSeek-R1 几乎同步发布（2025-01-20），二者推理能力均达到 OpenAI o1 水平。 Kimi-K1.5 是多模态模型；DeepSeek-R1 仅文本。 相比 DeepSeek-R1，Kimi-K1.5 技术报告披露了更多可落地的算法细节，尤其在 RL 数据构建、评估、采样方面极具参考价值。 🧱 1. 整体架构 # 采用标准三阶段流程：\nPre-training → SFT → Reinforcement Learning（RL）\n（图源：木尧｜知乎）\n🔤 2. 预训练（Pre-training） # 分三阶段：\n阶段一：视觉-语言预训练 # 先训纯语言模型（LLM），再逐步加入多模态数据； Vision tower 独立训练，初期不更新 LLM 参数； 后期图文交织数据从 0% → 30%，逐步放开 LLM 更新。 阶段二：冷却（Cooling）阶段 # 用精选 + 合成数据（QA 对）巩固推理/知识能力； 合成方式：用专有模型生成 → 拒绝采样保质量。 阶段三：长上下文激活 # 目标：支持 131,072 token 上下文； 关键技术： 过采样 long-context 数据：40% 全注意力 + 60% partial attention； 渐进训练：4k → 32k → 128k； RoPE base 频率设为 1,000,000（更大上下文需更高频率）。 → 得到基础模型 kimi-k1.5-base。\n✍️ 3. SFT 训练 # 3.1 常规 SFT # 数据构建： 非推理任务：人工种子集 → 模型生成多回复 → 人工排序+优化； 推理任务（数学/代码）：规则+奖励模型验证 + 拒绝采样（更高效准确）。 数据分布（~1M）： 类型 数量 一般问答 500k 编码 200k 数学/科学 200k 创意写作 5k 长上下文任务 20k 图文任务（图表/OCR/视觉推理等） 1M 训练细节： Epoch 1：seq_len=32k, lr 2e-5 → 2e-6 Epoch 2：seq_len=128k, lr 1e-5 → 1e-6 packing 多样本训练（提升 GPU 利用率） 🔍 3.2 Long-CoT SFT（重点①） # 目标：让模型学会人类式深度思考： 规划 → 评估 → 反思 → 探索 数据构造：对高质量问题，用 Prompt Engineering 生成含完整思考链的长答案； 本质仍是 SFT，差异仅在 answer 长度与结构。 🎯 4. 强化学习（RL）——核心亮点 # 4.1 RL 数据集构建原则（三大关键） # 维度 具体做法 多样性 多领域数据 + 公司自建标签系统（核心资产未公开） 难度平衡 高温采样 10 次 → 计算通过率定难度；动态更新（每次用最新 checkpoint）；课程学习：由易→难 可精确评估 移除易猜题型（多选/判断/证明题）；移除易 hack prompt：不走 CoT 也能高概率答对 → 剔除 ✅ 核心原则：“答案必须可精确评估”——RL 的“宪法”。\n4.2 问题定义 # 将推理建模为搜索空间优化问题：\n固定答案题：规则判断 → reward ∈ {0, 1} 开放问答：用奖励模型打分 4.3 策略优化 # 无 Value Model：与 DeepSeek-R1 的 GRPO 一致 → 保留错误路径梯度，对提升推理能力有帮助； 长度惩罚：防过度思考，奖励随推理长度衰减： 4.4 采样策略 # 课程采样：按难度递进； 优先采样：按 1 − 成功率 比例采样 → 专攻短板问题。 🔥 4.5 Long2Short（重点②） # 实现 长链推理 → 短链推理 的高效迁移：\n方法 说明 权重融合 Long/Short checkpoint 直接加权平均（无需训练，可直接工程复用） 最短拒绝采样 生成多条 → 选最短且正确者 长短 DPO 构建 pair：短且正确（+） vs 长/错误（−） 长度惩罚 RL RL 阶段显式抑制冗长输出 4.6 其他亮点细节 # 4.6.1 代码 RL：自动生成测试用例 # 用 CYaRon + Kimi-k1.5 生成 → 用 10 个正确提交验证 → 通过率 ≥70% 为有效； 1000 题中 → 614 无特殊评测 → 323 题最终入训。 4.6.2 数学 RL：双奖励模型 # 类型 优势 Classic RM 输入：问题+标准答案+模型作答 → 输出标量 Chain-of-Thought RM 训练时看 CoT，效果更好（用 800k CoT+label 数据训练） 4.6.3 视觉 RL 数据三类 # 类别 作用 真实世界数据 科学题、地点猜测、图表分析 → 提升现实推理 合成视觉数据 程序生成图像 → 训练空间/几何推理 文本渲染数据 文档→图像（截图/照片）→ 提升文字密集图理解，保证跨模态一致性 4.6.4 RL 框架优化 # 训练/推理分离框架； Rollout 采样优化 → 提升数据利用率： 📊 5. 实验结论 # 5.1 主要结果 # 多项 benchmark 达 o1 水平，首个追平 o1 的多模态模型： Long2Short 后，Short 模型性能几乎无损： 5.2 自我进化：CoT 长度 ↔ 能力正相关 # 模型自发生成更长 CoT； CoT 越长，性能越好；模型越大，提升斜率越陡： 5.3 课程学习显著有效 # 固定难度训练（蓝线）快速饱和； 课程学习（橙线）持续提升： 5.4 负样本梯度有用 # 对比 ReFT（仅用正样本）→ 含负样本策略表现更优： ✅ 总结：Kimi-K1.5 的三大工程可复用亮点 # RL 数据精筛三原则（多样+动态难度+可评估）——可直接用于自建 RL pipeline； Long2Short 迁移技术（尤其权重融合）——低成本部署轻量高能模型； 拒绝采样 + 优先采样 + 课程学习 组合拳 —— 提升训练效率与收敛质量。 作者：chaofa｜全网同名\n原文链接：https://yuanchaofa.com/post/kimi-k1.5-paper-reading-notes.html\n如需导出为 PDF/PPT 或提取某部分细节（如代码测试生成流程），我可进一步整理。\n参考 # 以下是对 《深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的》 的结构化摘要，保留原文关键图片与核心要点，便于快速把握 Kimi-K1.5 的技术亮点与工程实践细节。\n"},{"id":56,"href":"/www6vAlgo/docs/RL/core/RLVR/","title":"RLVR","section":"Core","content":" RLVR # RLVR\n参考 # bili\n"},{"id":57,"href":"/www6vAlgo/docs/DeepLearning/basic/Pytorch/","title":"(实战)PyTorch","section":"basic","content":"\nPyTorch 实战 # (实战)PyTorch\n"},{"id":58,"href":"/www6vAlgo/docs/LLM/Survey/LeaderBoard/","title":"大模型 排行榜","section":"Survey","content":" 大模型 # 排行榜 # HuggingFaceH 大模型排行榜\nLLM Collection\n中国排行榜 # 中国大模型 通用 39 金融 25 司法 8 法律 6 医学 13 医疗 24 教育 13 科研 17 工业 23 政务 12 运维 7 "},{"id":59,"href":"/www6vAlgo/docs/LLM/Reasoning/Survey/ReasoningTTS/","title":"(Survey) Test-Time Scaling","section":"Survey","content":"\nTest-Time Scaling # (Survey) Test-Time Scaling\n"},{"id":60,"href":"/www6vAlgo/docs/LLM/Reasoning/Survey/ReasoningPostTraining/","title":"(Survey)Reasoning LLM Post-Training","section":"Survey","content":"\nReasoning LLM Post-Training # "},{"id":61,"href":"/www6vAlgo/docs/RL/core/GRPO-family/GRPO/","title":"(原理)GRPO","section":"GRPO Family","content":"\nGRPO # GRPO\n"},{"id":62,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/post-training/DeepseekDistill/","title":"(实战)Deepseek 蒸馏","section":"Post-training","content":"\n(实战)Deepseek 蒸馏\n"},{"id":63,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/post-training/DeepseekSFT/","title":"(实战)Deepseek R1 SFT","section":"Post-training","content":"\nDeepSeek R1 SFT # (实战)DeepSeek R1 SFT\n"},{"id":64,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/DeepSeekExplain2/","title":"解读 DeepSeek[邱锡鹏]","section":"R1","content":"\n这基本上就是R1的技术路线。我简单列一些关于DeepSeek R1的思考和启发：\n1、R1/R1-zero的技术路线和社区对o1复现的差异\n此前社区对o1的复现基本都会涉及到蒸馏和搜索。 R1-Zero没有SFT，没有过程监督，没有搜索，也能训练出类似o1的效果。学术界之前也有很多实验，但在较小的模型上都没有成功。说明只有基模型足够强，Scaling RL才能取得比较好的效果。 虽然R1强调MCTS没有效果，但是简单的majority vote能大幅提升R1的效果，说明搜索仍然是重要的Scale的范式。 R1的成功还依赖DeepSeek强大的系统效率和RL调教能力。 2、策略初始化\nR1-zero是一个比较好的尝试，但是R1还是经过了先SFT（大概几干条）后再进行RL。 未来后训练的重心会逐步倾向于RL，但是少量训练用于SFT可能还是必须的。 3、奖励模型\nR1的奖励设计跟普通的后训练没特别大的区别（Qwen2，Tulu3），有ground truth用ground truth做EM，否则用RM。 RM的（训练数据量，模型大小，OOD问题，选代周期）的相关问题在整个训练的流程中还是比较关键。可能使用当前开源的比较强大的RM可以达到比较好的效果，也有可能基于内部的数据重新进行了偏好标注。 奖励设计（例如RPM的技巧）可能会在基于少量样本的强化学习微调上仍然起到显著作用。 4、PRM和MCIS\nDS给了两个PRM和MCTS的“不成功尝试”。但PRM部分说的比较笼统，并且DS的PRM只评估Correctness（与OAI的Lets verify step by step一致）。 R1给的是一个简单而且可规模化的可行解，这样做不一定是最优的。基于R1的Test-time search也继续优化它的效果。 PRM总归是一种比较稠密的监督信号，按照传统R1的理论，对OR进行shaping可以使训练更稳定或收敛得更快。 PRM不应该是一个被完全放弃的东西，可以让模型收敛得更快速或更稳定（Scaling曲线的斜率更大）。 5、写作能力提升\no1相比4o在写作等任务上的提升非常小，但R1的创作经常会令人眼前一亮，可能主要是强基模型在Scale RL后涌现的能力，也有人猜测是因为R1的安全对齐做的比较少，没有太约束模型的创作能力。 6、过度优化问题\nR1经常会使用一些高端词汇，典型的如量子纠缠和熵增熵减（会用在各个领域）。猜测是某种形式的reward hacking导致的。 R1在一些通用领域没有ground truth的任务上的推理效果还并不理想，强化学习的训练并不能保证泛化。 7、Test-Time Scaling\no1出来后大家讨论比较多的是Test-Time Scaling，但重要的还是Training-Time Scaling，包括数据和Training Step。蒸馏见效快，但上限不高，重要的还是高质量致据的缺失，蒸馏数据无法提供训练Scaling。RL是其中的关键，因为它可以保障有足够的数据和足够的训练步骤。 8、Agentic展望\nR1是目前唯一同时具有强推理能力和联网搜索的产品，效果很好，可以调研一些复杂的信息并进行回答。强推理模型最终的落脚点大概率是Agent，怎么用强推理模型帮助Agent更好更鲁棒是一个比较重要的问题。 参考 # DeepSeek最强专业拆解来了，清交复教授超硬核解读\n"},{"id":65,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/DeepSeekExplain1/","title":"解读 DeepSeek[刘知远]","section":"R1","content":"\n硬核解读 DeepSeek：大模型强化学习技术原理与大模型技术发展研判\n"},{"id":66,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/ReasoningLLM/","title":"(原理)Reasoning LLM","section":"R1","content":"\nReasoning LLM # (原理)Reasoning LLM\n"},{"id":67,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/DeepSeekR1/","title":"(原理) DeepSeek R1","section":"R1","content":"\n(原理) DeepSeek R1 # (原理)DeepSeek R1\n"},{"id":68,"href":"/www6vAlgo/docs/LLM/Reasoning/Overthinking/survey/","title":"(Survey)Overthinking","section":"Overthinking","content":" 论文 # Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models\nhttps://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs\n参考 # NICE 有个分享\n全景解读高效推理：LLM 也会「想太多」？\nhttps://zhuanlan.zhihu.com/p/1888902868641244612 https://www.cnblogs.com/Orzjh/p/18957909 ***\nhttps://blog.csdn.net/qq_27590277/article/details/146432960 ***\n"},{"id":69,"href":"/www6vAlgo/docs/LLM/Reasoning/Qwen/qwen-next/","title":"(原理)Qwen3-Next","section":"Qwen","content":" 模型结构 # 混合架构：Gated DeltaNet + Gated Attention 线性注意力打破了标准注意力的二次复杂度，在处理长上下文时有着更高的效率。我们发现，单纯使用线性注意力或标准注意力均存在局限：前者在长序列建模上效率高但召回能力弱，后者计算开销大、推理不友好。通过系统实验，我们发现 Gated DeltaNet [1] 相比常用的滑动窗口注意力（Sliding Window Attention）和 Mamba2 有更强的上下文学习（in-context learning）能力，并在 3:1 的混合比例（即 75% 层使用 Gated DeltaNet，25% 层保留标准注意力）下能一致超过超越单一架构，实现性能与效率的双重优化。 在保留的标准注意力中，我们进一步引入多项增强设计：\n（1）沿用我们先前工作 [2] 中的输出门控机制，缓解注意力中的低秩问题。\n（2）将单个注意力头维度从 128 扩展至 256。\n（3）仅对注意力头前 25% 的位置维度添加旋转位置编码，提高长度外推效果。\n极致稀疏 MoE：仅激活 3.7% 参数 Qwen3-Next 采用了高稀疏度的 Mixture-of-Experts (MoE) 架构，总参数量达80B，每次推理仅激活约 3B 参数。我们实验表明，在使用全局负载均衡 [4] 后，当激活专家固定时，持续增加专家总参数可带来训练 loss 的稳定下降。相比Qwen3 MoE的128个总专家和8个路由专家，Qwen3-Next我们扩展到了512总专家，10路由专家与1共享专家的组合，在不牺牲效果的前提下最大化资源利用率。\n训练稳定性友好设计 我们发现， 注意力输出门控机制能消除注意力池 [5] 与极大激活 [6] 等现象，保证模型各部分的数值稳定。在Qwen3中我们采用了QK-Norm，我们发现部分层的 norm weight 值会出现异常高的现象。为了缓解这一现象，进一步提高模型的稳定性，我们在Qwen3-Next中采用了 Zero-Centered RMSNorm [7]，并在此基础上，对 norm weight 施加 weight decay，以避免权重无界增长。我们还在初始化时归一化了 MoE router 的参数 [8]，确保每个 expert 在训练早期都能被无偏地选中，减小初始化对实验结果的扰动。\nMulti-Token Prediction Qwen3-Next 引入原生 Multi-Token Prediction (MTP) 机制 [3][9][10]，既得到了 Speculative Decoding 接受率较高的 MTP 模块，又提升了主干本身的综合性能。Qwen3-Next 还特别优化了 MTP 多步推理性能，通过训练推理一致的多步训练，进一步提高了实用场景下的 Speculative Decoding 接受率。\n参考文献 # Gated Delta Networks: Improving Mamba2 with Delta Rule\nGated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free\nDeepSeek-V3 Technical Report\nDemons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models\nEfficient Streaming Language Models with Attention Sinks\nMassive Activations in Large Language Models\nGemma: Open Models Based on Gemini Research and Technology\nApproximating Two-Layer Feedforward Networks for Efficient Transformers\nBetter \u0026amp; faster large language models via multi-token prediction\nProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training\n参考 # Qwen3-Next：迈向更极致的训练推理性价比\nhttps://qwen3-next.com/\nQwen3-Next：混合注意力 + 超稀疏 MoE + MTP = SOTA 推理速度\n"},{"id":70,"href":"/www6vAlgo/docs/LLM/Reasoning/Deepseek/V3/DeepSeek/","title":"DeepSeek V3","section":"V3","content":" 论文 # DeepSeek-V3 Technical Report\nhttps://github.com/deepseek-ai/DeepSeek-V3\n参考 # 【LLM技术报告】DeepSeek-V3技术报告（全文） 翻译\nDeepSeek-V3 关键点解读：架构篇\n"},{"id":71,"href":"/www6vAlgo/docs/LLM/Dense/BERT/","title":"(原理)BERT","section":"Dense","content":"\nBERT # (原理)BERT\n"},{"id":72,"href":"/www6vAlgo/docs/LLM/Core/token-sampling/temperature/","title":"Token Sampling Methods","section":"Token Sampling","content":" Token Sampling Methods # Token Sampling Methods\n"}]