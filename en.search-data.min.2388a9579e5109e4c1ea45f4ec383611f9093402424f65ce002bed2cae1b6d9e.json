[{"id":0,"href":"/www6vAlgo/docs/RL/framework/verl/","title":"HybridFlow[veRL]","section":"Framework","content":" 论文 # HybridFlow: A Flexible and Efficient RLHF Framework\n解读 # HybridFlow[veRL]\n"},{"id":1,"href":"/www6vAlgo/docs/RL/Agentic-RL/survey/survey/","title":"(Survey)Agentic RL","section":"Survey","content":" 论文 # The Landscape of Agentic Reinforcement Learning for LLMs: A Survey git 解读 # (Survey)Agentic RL\n参考 # The Landscape of Agentic Reinforcement Learning for LLMs: A Survey\nThe Landscape of Agentic Reinforcement Learning for LLMs: A Survey\n"},{"id":2,"href":"/www6vAlgo/docs/RL/Agentic-RL/Tool/OTC/","title":"OTC","section":"Tool","content":" 参考 # 2025年大模型agent rl训练多轮planning技术TORL,ToolRL, RAGEN,OTC,SkyRL-v0, GiGPO,Tool-N1 ,ARTIST, ZeroTIR, GRPO\n"},{"id":3,"href":"/www6vAlgo/docs/RL/Agentic-RL/Tool/ReTool/","title":"ReTool","section":"Tool","content":" 论文 # ReTool: Reinforcement Learning for Strategic Tool Use in LLMs\nhttps://retool-rl.github.io/\n引言 # 大型语言模型（LLM）通过思维链提示和强化学习等技术，在推理任务中取得了显著进展。然而，这些基于文本的方法在需要精确数值计算或符号操作的任务中常常力不从心。ReTool 通过引入一个强化学习框架来解决这一限制，该框架教导 LLM 如何战略性地将代码解释器整合到其推理过程中。\n图1：传统基于文本的强化学习训练（上）与 ReTool 采用交错代码执行的方法（下）的比较。ReTool 允许在生成过程中与代码沙盒进行动态交互。\n这项工作表明，LLM 不仅能学习如何使用计算工具，还能通过基于结果的反馈学习何时以及为何调用它们。这代表着与仅仅模仿预定义工具使用模式的监督微调方法相比，取得了重大进步。\n方法论 # ReTool 采用两阶段训练框架，结合监督微调和强化学习，以开发战略性工具使用能力。\n冷启动监督微调 # 第一阶段通过细致的数据整理和监督训练，建立基础的工具使用能力：\n数据构建过程：\n从 Open-Thoughts 等来源初步收集高质量数学推理数据 （ \\( D_{init} \\) ） 使用人类专家和 DeepSeek-R1 进行双重验证以确保质量 使用结构化提示将基于文本的推理自动转换为代码集成推理（ \\( D_{CI} \\) ） 对代码集成数据集进行额外的格式和答案验证 转换过程使用系统化的提示模板，该模板识别文本推理中的计算步骤，并将其替换为可执行的代码片段及其输出。这创建了演示何时以及如何调用代码解释器的训练数据。\n带有交错代码执行的强化学习 # 核心创新在于 ReTool 的强化学习训练过程，该过程动态集成了实时代码执行：\n训练算法： 采用简单基于准确性的奖励函数的近端策略优化（PPO）：\n\\[\rR(a, \\hat{a}) = \\begin{cases} 1 \u0026 \\text{if predicted answer } \\hat{a} \\text{ equals ground truth } a \\\\ -1 \u0026 \\text{otherwise} \\end{cases}\r\\] 展开过程： 与传统模型生成纯文本的强化学习不同，ReTool 的展开过程包括：\n模型生成自然语言推理 当代码调用被触发（通过 \u0026lt;code\u0026gt; 标签）时，生成暂停 代码被提取并在外部沙盒环境中执行 执行结果（成功或错误）通过 \u0026lt;interpreter\u0026gt; 标签反馈给模型 模型使用解释器反馈继续生成 过程重复直到得到最终答案或达到最大长度 技术优化：\n解释器反馈token在损失计算中被屏蔽，以保证训练稳定性 KV缓存重用最小化了展开过程中的内存开销 异步代码沙盒支持并行执行和更快的训练 结果与分析 # ReTool 在具有挑战性的数学推理基准测试中，相较于基线方法展现出显著改进。\n性能指标 # AIME 基准测试结果：\nAIME 2024：67.0% 准确率（对比基于文本的强化学习基线为 40.0%） AIME 2025：49.3% 准确率（对比基于文本的强化学习基线为 36.7%） 训练效率：在 400 步内实现了卓越性能，而基线则需要 1000+ 步 图2：训练曲线显示了 ReTool 在 AIME 基准测试中相较于基于文本的强化学习方法，具有快速的改进和卓越的最终性能。\n高级骨干模型结果： 以 DeepSeek-R1-Distill-Qwen-32B 作为基础模型，ReTool 实现了更高的性能：\nAIME 2024：72.5% 准确率 AIME 2025：54.3% 准确率 涌现认知行为 # 分析揭示了在强化学习训练过程中出现的几个显著的涌现特性：\n图3：ReTool 训练过程中行为变化的详细分析，展示了响应长度、代码使用模式和执行成功率的演变。\n关键涌现行为：\n令牌效率： 平均响应长度减少 40%（从 10k 减少到 6k 令牌） 代码采用： 代码使用率增加到近 98% 的响应 战略时机： 模型学会了在推理过程的早期调用代码 自我纠正： 自主错误检测和代码修订能力 多样化工具使用： 扩展了基本计算之外的功能，包括验证、优化和分析 自我纠正能力 # 最重要的发现之一是模型能够自主纠正代码执行错误：\n图4：涌现的自我纠正行为示例，模型识别并修复了其生成的代码中的 NameError。\n当遇到 NameError: name 'greedy' is not defined 等错误时，模型通过反思错误并生成修正后的代码，展示了元认知意识，而无需为此行为进行明确的训练。\n工具使用演变 # 这项研究提供了关于代码使用模式在训练期间如何演变的见解：\n图5：词云分析显示了 ReTool 训练前后代码目的的演变，展示了工具使用策略多样性的增加。\n分析表明，虽然“计算”仍然是主要目的，但强化学习训练导致了更多样化的代码应用，包括几何分析、概率计算和解决方案验证。\n比较分析 # ReTool 的方法比传统的基于文本的推理具有明显的优势：\n图6：在复杂数学问题上，基于文本的推理（左）与代码集成推理（右）的并排比较，突出了精度和效率的提升。\n比较表明，代码集成消除了容易出错的冗长手动计算，使模型能够专注于更高级别的战略推理，同时将精确计算卸载到解释器。\n意义和影响 # ReTool 代表了混合神经-符号人工智能系统的重大进步，它证明了大型语言模型可以通过强化学习而不是单纯的模仿来学习战略性工具使用。这项工作的贡献包括：\n技术贡献：\n首次成功将强化学习应用于大规模战略性代码解释器使用 展示了无需明确训练即可涌现的自我纠正能力 高效的训练框架，以更少的步骤实现了卓越的性能 更广泛的影响：\n弥合了神经网络模式识别与符号计算之间的鸿沟 为跨不同领域更通用化的工具使用奠定了基础 通过明确的计算步骤增强了可解释性 展示了自主发现问题解决策略的潜力 这项研究确立了 ReTool 作为开发更强大、更通用的人工智能系统的一种引人注目的方法，这些系统能够有效地结合神经网络和符号计算范式的优势。\n参考 # 通过工具增强 LLM Agent 能力：veRL+ReTool 的完整实践指南 https://www.alphaxiv.org/zh/overview/2504.11536v2 "},{"id":4,"href":"/www6vAlgo/docs/RL/Agentic-RL/Tool/ToolRL/","title":"ToolRL","section":"Tool","content":" 论文 # ToolRL: Reward is All Tool Learning Needs\n为了确定最佳奖励策略，探索了四个关键维度的各种奖励配置：(1) 奖励类型（奖励哪些方面），(2) 奖励尺度（奖励多少），(3) 奖励粒度（奖励信号的详细程度），以及 (4) 奖励动态（奖励如何随时间演变）。通过大量的实验确定了最符合主体工具使用情况的奖励设计，并揭示了奖励对于调用工具的 LLM 而言“有用”的原因。论文得出的核心见解总结如下：\n推理轨迹越长并不一定越好，而且过长的奖励可能会降低性能。 动态奖励尺度有助于模型从简单行为平稳过渡到复杂行为。 细粒度的奖励分解可实现更稳定、更有效的学习。 论文得出的核心见解总结如下：\n推理轨迹越长并不一定越好，而且过长的奖励可能会降低性能。 动态奖励尺度有助于模型从简单行为平稳过渡到复杂行为。 细粒度的奖励分解可实现更稳定、更有效的学习。 Format Reward Correctness Reward\n参考 # 2025年大模型agent rl训练多轮planning技术TORL,ToolRL, RAGEN,OTC,SkyRL-v0, GiGPO,Tool-N1 ,ARTIST, ZeroTIR, GRPO\n"},{"id":5,"href":"/www6vAlgo/docs/RL/Agentic-RL/Search/Search-R1/","title":"Search-R1","section":"Search","content":" 论文 # Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning https://github.com/PeterGriffinJin/Search-R1 Methods # 详细方法和步骤:\n将搜索引擎建模为环境的一部分： SEARCH-R1将搜索引起作为环境的一部分， 让模型与环境交互，从而得到 reward。 支持多轮检索和推理： SEARCH-R1通过特定的标签（\u0026lt;search\u0026gt;, \u0026lt;/search\u0026gt;, \u0026lt;information\u0026gt;, \u0026lt;/information\u0026gt;, \u0026lt;think\u0026gt;, \u0026lt;/think\u0026gt;, \u0026lt;answer\u0026gt;, \u0026lt;/answer\u0026gt;）来支持多轮检索和推理。 优化算法兼容性： SEARCH-R1 与各种 RL 算法兼容，包括 PPO 和 GRPO。 简单结果奖励函数： 避免复杂的基于过程的奖励, 采用简单的基于结果的奖励函数 （字符串匹配作为reward!!!）。 总结 # 结论1: SEARCH-R1 显著提升了LLM在需要实时外部知识的复杂推理任务中的能力。 通过强化学习，LLM可以自主生成查询并有效利用检索到的信息，优于传统的RAG方法。\n结论2: SEARCH-R1在不同LLM架构和训练方法上具有广泛的适用性。 实验结果表明，无论使用基础模型还是指令调整模型，SEARCH-R1都能带来显著的性能提升，且对不同的RL算法（如PPO和GRPO）具有兼容性。\n结论3: SEARCH-R1有很强的实用价值。 SEARCH-R1能够显著提高LLM在需要实时外部知识的复杂推理任务中的能力。 可以用于智能问答，智能助手等领域。\n参考 # Search-R1：让大模型学会“检索+推理”的新范式 1xx. 【论文解读】Search-R1：强化学习如何教会 LLM 自主搜索？\n1xx. 有个学术的会议\n1xx. Search-R1：让 LLM 学会 “边搜边想”，强化学习赋能检索增强推理\n"},{"id":6,"href":"/www6vAlgo/docs/LLM/MoE/ModelMOE/","title":"(原理)Visual  MOE","section":"MOE","content":" Visual MOE # (原理)Visual MOE\n"},{"id":7,"href":"/www6vAlgo/docs/RL/core/GRPO-family/GRPODeepseek/","title":"(实战)GRPO","section":"GRPO Family","content":"\nGRPO # (实战)GRPO\n"},{"id":8,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningLearningRate/","title":"(原理)学习率","section":"网络优化","content":"\n学习率 # (原理)学习率\n"},{"id":9,"href":"/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/WeightDecay/","title":"(原理\u0026实战)权重衰减","section":"正则化","content":"\n权重衰减 WeightDecay # (原理\u0026amp;实战)权重衰减\n"},{"id":10,"href":"/www6vAlgo/docs/LLM/Core/ScalingLaw/","title":"Scaling Law","section":"Core","content":" Scaling Law[10] # Scaling Law # 参数量 vs 数据量 # 参数量 vs 数据量 # 参考 # Scaling Law # 解析大模型中的Scaling Law 1xx. 论文阅读，大模型的缩放定律，Scaling Laws for Neural Language Models 2xx. Training Compute-Optimal Large Language Models 简读 2xx. 【预训练模型】推翻OpenAI结论, DeepMind重新定义预训练的训练参数和训练规模的关系！ 《Scaling Laws for Neural Language Models》 《Training Compute-Optimal Large Language Models》\n"},{"id":11,"href":"/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLData/","title":"机器学习-数据","section":"机器学习","content":"\n机器学习-数据 # 机器学习-数据\n"},{"id":12,"href":"/www6vAlgo/docs/LLM/Dense/Family/","title":"GPT 系列","section":"Dense","content":" 进化时间线 # {% asset_img \u0026lsquo;family.jpg\u0026rsquo; %}\nGPT1 [1] # 它是最早一批提出在 NLP 任务上使用 pre-train + fine-tuning 范式的工作。 GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间 预训练模型具有 zero-shot 的能力，并且能随着预训练的进行不断增强 GPT2 [1] # 核心思想 # 当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，不需要在下游任务微调。\nGPT-2 vs. GPT-1 # 主推 zero-shot，而 GPT-1 为 pre-train + fine-tuning； 训练数据规模更大，GPT-2 为 800w 文档 40G，GPT-1 为 5GB； 模型大小，GPT-2 最大 15 亿参数，GPT-1为 1 亿参数； 模型结构调整，层归一化和参数初始化方式； 训练参数，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024，等等； GPT3 [1] # 下游任务评估方法 # GPT-3 在下游任务的评估与预测时，提供了三种不同的方法： Zero-shot：仅使用当前任务的自然语言描述，不进行任何梯度更新； One-shot：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新； Few-shot：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；\nShot[2] One-shot Few-Shot Zero-Shot Few-shot vs fine-tuning # 其中 Few-shot 也被称为 in-context learning，虽然它与 fine-tuning 一样都需要一些有监督标注数据，但是两者的区别是： 【本质区别】 fine-tuning 基于标注数据对模型参数进行更新 而in-context learning使用标注数据时不做任何的梯度回传, 模型参数不更新\nGPT-3 vs. GPT-2 # 效果上，超出 GPT-2 非常多，能生成人类难以区分的新闻文章； 主推 few-shot，相比于 GPT-2 的 zero-shot，具有很强的创新性； 模型结构略微变化，采用 sparse attention 模块； 海量训练语料 45TB（清洗后 570GB），相比于 GPT-2 的 40GB； 海量模型参数，最大模型为 1750 亿，GPT-2 最大为 15 亿参数； InstructGPT [1] # 步骤 # 有监督微调， 奖励模型训练， 强化学习训练 技术方案 # 有监督微调（SFT） 本质上来说，SFT 可以理解为人工标注了一批数据，然后去微调 GPT-3。但是值得一提的是，这里标注的数据与 GPT-3 之前用来做下游任务使用的 few-shot 格式，有非常本质的区别。 InstructGPT 在 SFT 中标注的数据，正是为了消除这种模型预测与用户表达习惯之间的 gap。在标注过程中，他们从 GPT-3 的用户真实请求中采样大量下游任务的描述，然后让标注人员对任务描述进行续写，从而得到该问题的高质量回答。\n基于人类反馈的强化学习（RLHF） {% asset_img \u0026lsquo;instructGPT.jpg\u0026rsquo; %}\n总结 # 解决 GPT-3 的输出与人类意图之间的Align问题； 让具备丰富世界知识的大模型，学习“人类偏好”； 标注人员明显感觉 InstructGPT 的输出比 GPT-3 的输出更好，更可靠； InstructGPT 在真实性，丰富度上表现更好； InstructGPT 对有害结果的生成控制的更好，但是对于**“偏见”没有明显改善**； ChatGPT 训练 [3] # 基于人类反馈的强化学习微调技术 RLHF 使用有监督微调 Supervised Fine-tuning（SFT）预训练语言模型 Supervised fine-tuning (SFT) = Instruction Tuning 训练奖励模型 Reward Model（RM） 使用强化学习算法微调语言模型 RLHF [本质 基于强化学习, 强化学习算法] 参考 # GPT / GPT-2 / GPT-3 / InstructGPT 进化之路 ***\nFew-Shot, Zero-Shot \u0026amp; One-shot 的通俗理解\nAI 大模型微调训练营大纲\n1xx. 万字拆解！追溯ChatGPT各项能力的起源 符尧\n1xx. [Transformer 101系列] ChatGPT是怎么炼成的? 未\n"},{"id":13,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Transformer/","title":"(原理)Transformer","section":"Transformer","content":"\n(原理)Transformer # (原理)Transformer\n"},{"id":14,"href":"/www6vAlgo/docs/LLM/Survey/LargeModelSurvey/","title":"(综述)大模型","section":"Survey","content":" LLMs的背景[1] # Scaling law of LLMs # KM scaling law Chinchilla Scaling law LLMs的涌现能力 # in-context learning instruction following step-by-step reasoning 大语言模型的关键技术 *** # Scaling Training Ability Eliciting Alignment Tuning Tool Manipulation Pre-training[1] # 数据收集 # 架构 # 模型训练 *** # 优化设置\nBatch Training Learning Rate Optimizer Stabilizing the Training 可扩展的训练技巧\n3D并行 数据并行 + 流水线并行 + 张量并行 ZeRO 混合精度训练 总体训练建议 Adaptation Tuning of LLMs[1] # 指令调优 *** # 本质上，指令微调是在自然语言格式的实例（instance）集合上微调预训练后的 LLM 的方法 [62]。\n指令微调后，LLM 可以展现出泛化到未见过任务的卓越能力 [28, 62, 64]，即使在多语言场景下也能有不错表现 [98]。\n格式化实例的构建 # 格式化已有数据集 格式化人类需求 构建实例的关键因素 增加指令 设计格式 总的来说，指令多样性似乎比实例数量更重要\n指令微调策略 # 平衡数据分布 一种广泛使用的方法是实例比例混合策略 [87]，即将所有数据集合并，然后从混合数据集中按比例采样每种实例。 此外，根据最近的研究发现 [64, 99]，提高高质量数据集（例如 FLAN [62] 和 P3 [209]）的采样比例通常可以带来性能提升。\n结合指令微调和预训练 为了使微调过程更加有效和稳定，OPT-IML [99] 在指令微调期间加入了预训练数据，这可以看作是对模型的正则化（regularization）。\n具体而言，GLM-130B [97] 和 Galactica [34] 将指令格式数据集作为预训练语料库的一小部分来预训练 LLM，这有可能同时获得预训练和指令微调的优势。\n指令微调的效果 # 性能改进 最近的研究在多个规模上（从 7700 百万到 5400 亿不等）对 LM 进行了实验，表明不同规模的模型都可以从指令微调中受益 [64, 216]，随着参数规模的增加，性能也得到了提升 [98]。 【普适性】 此外，经过指令微调的较小模型甚至可以比未经微调的较大模型表现更好 [28, 64]。\n任务泛化性 todo 对齐调优 # 高效调优 # 参考 # 大语言模型综述 中文 v10\n大语言模型综述 中文\nLLMSurvey Repo git\n[论文]大语言模型综述\n详谈大模型训练中的数据收集、处理与模型影响：A Survey of Large Language Models工作中的数据总结\n大模型综述-A Survey of Large Language Models 1xx. 值得一看的大模型最新综述：兼看多语种大模型微调数据集Aya 1xx. 43页预训练模型综述（清华、复旦、人大）\n"},{"id":15,"href":"/www6vAlgo/docs/DeepLearning/basic/DeepLearning/","title":"Deep Learning","section":"basic","content":"\nDeep Learning # Deep Learning\n"},{"id":16,"href":"/www6vAlgo/docs/LLM/Reasoning/kimi/kimi1.5/","title":"Kimi1.5","section":"kimi","content":" 论文 # KIMI K1.5:SCALING REINFORCEMENT LEARNING WITH LLMS\ngit\n参考 # 深度解读 Kimi-K1.5，真正了解 RL 数据是怎么筛选的 ***\n细节之王 Kimi K1.5，大模型算法工程师复现推理模型必读文章之一\n"},{"id":17,"href":"/www6vAlgo/docs/LLM/Reasoning/Qwen/Qwen3/","title":"Qwen3","section":"Qwen","content":" 论文 # https://github.com/QwenLM/Qwen3/\nArch [2] # Compare [1] # Post-training # 阶段三：思考模式融合 # **两种模式使用/think和/no_think标志进行区分，**注意“非思考模式”也有开始和结束的标志符，只是其思考过程置为空。并且在训练过程中，会针对多轮对话进行“思考”和“非思考”模式的混合训练。\n参考 # The Big LLM Architecture Comparison Understanding and Implementing Qwen3 From Scratch Qwen3技术报告的几点细节、ArXiv论文翻译实现方案及试错历程\n【LLM4】Qwen3-RL训练详解 ***\nup: 卢老师， 怀中猫\n"},{"id":18,"href":"/www6vAlgo/docs/LLM/Reasoning/Qwen/Qwen3-report/","title":"Qwen3 Report","section":"Qwen","content":" 简介 # Qwen 团队发布了 Qwen3，这是一个新的大型语言模型（LLM）系列，代表了开源人工智能领域的重大进步。Qwen3 涵盖了各种参数规模的密集和混合专家（MoE）架构，旨在平衡性能、效率和可访问性。\n这个最新版本建立在以前的 Qwen 模型之上，在推理能力、多语言支持和推理效率方面进行了重大改进。Qwen3 的一项关键创新是在单个模型中集成思考和非思考模式，无需在不同任务的专用模型之间切换。这些模型在 Apache 2.0 许可下发布，有助于开源人工智能技术生态系统的发展，并将 Qwen3 定位为 GPT-4o、Claude 3.7 和 Gemini 2.5 等专有模型的有竞争力的替代方案。\n模型架构与创新 # Qwen3 在 Qwen2.5 的基础上进行了多项架构增强：\n双模架构：该模型集成了思考和非思考模式，允许用户根据任务需求选择合适的模式。 混合专家（MoE）：Qwen3 系列包括 MoE 模型，这些模型以较少的激活参数在推理期间实现高性能，从而提高计算效率。 核心组件：该架构包含以下高级功能： 分组查询注意力（GQA） SwiGLU 激活函数 旋转位置嵌入（RoPE） RMSNorm 和 QK-Norm 模型变体：Qwen3 系列包括针对不同用例量身定制的多个模型： Qwen3-235B-A22B：一个旗舰级的 MoE 模型，总参数为 235B，但在推理期间只有 22B 处于激活状态 Qwen3-32B：一个旗舰级的密集模型 参数范围从 14B 到 0.6B 的较小模型 扩展上下文长度：这些模型支持高达 128K 的上下文长度，从而能够处理更长的文档和更复杂的交互。 训练方法 # Qwen3 采用一种复杂的、多阶段的训练过程，旨在增强各种能力：\n预训练数据：这些模型在包含 119 种语言和方言的 36 万亿个 token 的庞大数据集上进行训练，与以前版本中支持的 29 种语言相比，这是一个显着的扩展。\n三阶段预训练过程：\n一般知识获取 推理能力增强 长上下文适应 后训练对齐：一个四阶段的过程使模型与人类偏好对齐：\n基础模型 → Long-CoT 冷启动 → 推理 RL → 思考模式融合 → 通用 RL 每个阶段都针对特定的能力：\nLong-CoT 冷启动：引入思维链推理 推理 RL：强化准确的推理路径 思考模式融合：集成思考和非思考模式 通用 RL：增强整体模型对齐 数据整理：训练过程涉及多模态数据增强，包括从 PDF 中提取文本和生成合成数据，以及实例级数据混合优化。\n思考模式和预算 # Qwen3 最具创新性的功能之一是其思考模式和预算机制，这为用户提供了对模型推理深度进行细粒度控制的能力：\n思考模式与非思考模式： 非思考模式：针对简单任务进行了优化，可直接提供答案，无需大量推理。 思考模式：进行更深入的推理，展示复杂问题的工作和中间步骤。 思考预算： 用户可以指定一个“思考预算”（以千个 tokens 为单位），以控制模型应应用的推理量。这在两种模式之间创建了一个频谱，而不是二元选择。 性能相关性：如上图所示，增加思考预算可以持续提高各种基准测试的性能，包括 AIME 数学推理、LiveCodeBench 编程和 GPQA Diamond 科学推理。 数学表达式：思考预算和模型性能之间的关系可以近似表示为： \\(P(b) = P_{\\text{non-thinking}} + (P_{\\text{thinking}} - P_{\\text{non-thinking}}) \\cdot \\min\\left(\\frac{b}{b_{\\text{max}}}, 1\\right) \\) 其中 \\( P(b) \\) 是预算为 \\(b\\) 时的性能， \\(b_{\\text{max}}\\) 是最大有效预算。\n多语言能力 # Qwen3 代表了多语言支持方面的重大进步：\n扩展的语言覆盖范围：该模型支持 119 种语言和方言，比以前版本中的 29 种语言有了大幅增加。 预训练方法：多语言能力嵌入在预训练期间，并经过精心的数据管理，以确保不同语言的平衡表示。 性能改进：Qwen3 展示了增强的跨语言理解和生成能力，使其对全球用户更具可访问性。 语言分布：训练数据包括英语以外的各种语言，其中普通话和其他广泛使用的语言占了很大比例，并且改进了对低资源语言的覆盖。 强到弱的知识蒸馏 # 为了增强 Qwen3 的可访问性，该团队采用了强到弱的知识蒸馏方法：\n知识转移：来自较大模型（Qwen3-235B-A22B 和 Qwen3-32B）的知识被提炼到较小的模型中（范围从 14B 到 0.6B 参数）。 蒸馏过程：该过程包括训练较小的模型来模仿较大模型的行为，同时保持双重模式能力。 优点： 减少了部署所需的计算资源 保持了各种任务的性能 保持了思考和非思考模式能力 实施： # 用于强到弱的知识蒸馏的简化伪代码 def distill_knowledge(teacher_model, student_model, training_data): for batch in training_data: # 获取教师模型在思考和非思考模式下的输出 teacher_thinking_output = teacher_model(batch, mode=\u0026#34;thinking\u0026#34;) teacher_nonthinking_output = teacher_model(batch, mode=\u0026#34;non-thinking\u0026#34;) # 训练学生模型以匹配两种模式 student_thinking_loss = loss_fn(student_model(batch, mode=\u0026#34;thinking\u0026#34;), teacher_thinking_output) student_nonthinking_loss = loss_fn(student_model(batch, mode=\u0026#34;non-thinking\u0026#34;), teacher_nonthinking_output) # 更新学生模型的参数 total_loss = student_thinking_loss + student_nonthinking_loss total_loss.backward() optimizer.step() 性能和基准测试 # Qwen3 在各种基准测试中表现出具有竞争力的性能：\n通用任务：在衡量常识推理、阅读理解和知识检索的基准测试中表现出色。 数学与 STEM：数学推理能力显著提高，在 MATH、AIME 和 GPQA 等基准测试中得到验证。 编码：在 HumanEval 和 LiveCodeBench 等编程基准测试中表现出竞争优势，并且随着思考预算的增加，思考模式明显优于非思考模式。 多语言任务：在不同语言中性能得到增强，表明扩展多语言支持的有效性。 与其他模型的比较：Qwen3 在开源模型中取得了最先进的结果，并缩小了与更大的专有模型之间的差距。 效率优势：与具有相似能力的密集模型相比，MoE 模型以更少的激活参数表现出高性能。 开源贡献 # 在 Apache 2.0 许可下发布 Qwen3 代表了对开源 AI 社区的重大贡献：\n模型可访问性：完整的模型权重和代码通过 Hugging Face、ModelScope 和 GitHub 等平台提供。 文档和示例：提供全面的文档和示例代码，以方便采用和实验。 社区参与：开源发布能够更广泛地研究、开发和部署先进的 LLM。 AI 民主化：通过免费提供最先进的模型，Qwen3 有助于缩小大型组织与小型实体或个体研究人员之间的资源差距。 未来方向 # Qwen 团队概述了未来研究和开发的几个方向：\n扩展预训练：进一步增加预训练数据的大小和多样性，以增强模型能力。 架构改进：继续改进模型架构，以提高效率和性能。 扩展上下文长度：探索处理超出当前 128K 限制的更长上下文的技术。 增强推理：开发更复杂的推理控制和验证方法。 多模态能力：扩展到多模态理解和生成。 强化学习：增加专门用于强化学习的计算资源，以提高模型对齐和指令遵循能力。 总而言之，Qwen3 代表了开源大型语言模型的重大进步，提供了一套全面的模型，具有竞争力的性能、创新的思考控制机制、扩展的多语言支持和高效的架构设计。 在单个模型中集成思考和非思考模式，以及思考预算机制，为用户提供了前所未有的控制，可以控制应用于不同任务的推理深度，从而优化性能和效率之间的平衡。\n参考 # https://www.alphaxiv.org/zh/overview/2505.09388v1\n"},{"id":19,"href":"/www6vAlgo/docs/LLM/MoE/ModelMOECode/","title":"(代码)MOE","section":"MOE","content":" import torch import torch.nn as nn import torch.nn.functional as F import torch_npu from torch_npu.contrib import transfer_to_npu class Expert(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super().__init__() self.net = nn.Sequential( nn.Linear(input_dim, hidden_dim), nn.GELU(), nn.Linear(hidden_dim, output_dim)) def forward(self, x): return self.net(x) class MoE(nn.Module): def __init__(self, input_dim, num_experts, top_k, expert_capacity, hidden_dim, output_dim): super().__init__() self.num_experts = num_experts self.top_k = top_k self.expert_capacity = expert_capacity # 路由网络 self.gate = nn.Linear(input_dim, num_experts) # 专家集合 self.experts = nn.ModuleList( [Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)]) def forward(self, x): batch_size, input_dim = x.shape device = x.device # 路由计算 logits = self.gate(x) probs = torch.softmax(logits, dim=-1) topk_probs, topk_indices = torch.topk(probs, self.top_k, dim=-1) # 辅助损失计算 if self.training: # 重要性损失（专家利用率均衡） importance = probs.sum(0) importance_loss = torch.var(importance) / (self.num_experts ** 2) # 负载均衡损失（样本分配均衡） mask = torch.zeros_like(probs, dtype=torch.bool) mask.scatter_(1, topk_indices, True) routing_probs = probs * mask expert_usage = mask.float().mean(0) routing_weights = routing_probs.mean(0) load_balance_loss = self.num_experts * (expert_usage * routing_weights).sum() aux_loss = importance_loss + load_balance_loss else: aux_loss = 0.0 # 专家分配逻辑 flat_indices = topk_indices.view(-1) flat_probs = topk_probs.view(-1) sample_indices = torch.arange(batch_size, device=device)[:, None]\\ .expand(-1, self.top_k).flatten() # 初始化输出 outputs = torch.zeros(batch_size, self.experts[0].net[-1].out_features, device=device) # 处理每个专家 for expert_idx in range(self.num_experts): # 获取分配给当前专家的样本 expert_mask = flat_indices == expert_idx expert_samples = sample_indices[expert_mask] expert_weights = flat_probs[expert_mask] # 容量控制 if len(expert_samples) \u0026gt; self.expert_capacity: expert_samples = expert_samples[:self.expert_capacity] expert_weights = expert_weights[:self.expert_capacity] if len(expert_samples) == 0: continue # 处理专家计算 expert_input = x[expert_samples] expert_output = self.experts[expert_idx](expert_input) weighted_output = expert_output * expert_weights.unsqueeze(-1) # 累加输出 outputs.index_add_(0, expert_samples, weighted_output) return outputs, aux_loss # 测试示例 if __name__ == \u0026#34;__main__\u0026#34;: input_dim = 128 output_dim = 256 num_experts = 8 top_k = 2 expert_capacity = 32 hidden_dim = 512 batch_size = 64 # add device = torch.device(\u0026#34;npu:4\u0026#34; if torch.npu.is_available() else \u0026#34;cpu\u0026#34;) moe = MoE(input_dim, num_experts, top_k, expert_capacity, hidden_dim, output_dim).to(device) x = torch.randn(batch_size, input_dim).to(device) experimental_config = torch_npu.profiler._ExperimentalConfig( export_type=torch_npu.profiler.ExportType.Text, profiler_level=torch_npu.profiler.ProfilerLevel.Level0, msprof_tx=False, aic_metrics=torch_npu.profiler.AiCMetrics.AiCoreNone, l2_cache=False, op_attr=False, data_simplification=False, record_op_args=False, gc_detect_threshold=None ) with torch_npu.profiler.profile( activities=[ torch_npu.profiler.ProfilerActivity.CPU, torch_npu.profiler.ProfilerActivity.NPU ], schedule=torch_npu.profiler.schedule(wait=0, warmup=0, active=1, repeat=1, skip_first=1), on_trace_ready=torch_npu.profiler.tensorboard_trace_handler(\u0026#34;./moe_stand_npu_result\u0026#34;), record_shapes=False, profile_memory=False, with_stack=False, with_modules=False, with_flops=False, experimental_config=experimental_config) as prof: # 训练模式 for _ in range(10): moe.train() output, loss = moe(x) print(f\u0026#34;Using device: {x.device}\u0026#34;) print(f\u0026#34;Training output shape: {output.shape}\u0026#34;) # torch.Size([64, 256]) print(f\u0026#34;Training auxiliary loss: {loss.item():.4f}\u0026#34;) # 示例值，如0.1234 prof.step() print(\u0026#34;=\u0026#34; * 80) # 推理模式 moe.eval() output, _ = moe(x) print(f\u0026#34;Eval output shape: {output.shape}\u0026#34;) # torch.Size([64, 256]) 参考 # MoE git\n"},{"id":20,"href":"/www6vAlgo/docs/RL/core/PPO-family/PPO/","title":"(原理)PPO","section":"PPO family","content":"\nPPO # (原理)PPO\n"},{"id":21,"href":"/www6vAlgo/docs/RL/core/PPO-family/PPO1/","title":"(原理)PPO","section":"PPO family","content":" PPO训练中四种模型的合作关系 # PPO训练中各模型的输入与输出 # 基于PPO进行RLHF训练的原理图 # 参考 # 第8部分：RLHF 与 RLAIF\n"},{"id":22,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningBatchsize/","title":"(原理)Batchsize","section":"网络优化","content":"\n最佳实践 # batchsize # batchsize 下限 [1] 别太小的限制在于，batch size太小，会来不及收敛。\n所以在常见的setting（～100 epochs），batch size一般不会低于16。\nbatchsize 上限 [1] batch size别太大的限制在于两个点，\n1）batch size太大，memory容易不够用。这个很显然，就不多说了。\n2）batch size太大，深度学习的优化（training loss降不下去）和泛化（generalization gap很大）都会出问题。\nlearning rate \u0026amp; batch size # 总之，可以证明，learning rate/batch size的比值对深度学习是有指数级的影响[3]，所以非常重要，没事别瞎调。[1]\n这也是为什么大的batch_size往往建议可以相应取大点learning_rate, 因为梯度震荡小，大learning_rate可以加速收敛过程，也可以防止陷入到局部最小值，而小batch_size用小learning_rate迭代，防止错过最优点，一直上下震荡没法收敛（这也是一个小trick）。[2]\n参考 # 怎么选取训练神经网络时的Batch size? Summer Clover 训练神经网络时batchsize扩大一倍的同时需要增加epoch数量吗? 新一 7.1 批大小调整实验 百度邱\n7.1 批大小调整实验\n设置BatchSize\n深度学习中的batch的大小对学习效果有何影响？\n"},{"id":23,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningForwardBackward/","title":"(原理\u0026实战)前向/反向传播","section":"basic","content":"\n前向/反向传播 # (原理\u0026amp;实战)前向/反向传播\n"},{"id":24,"href":"/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/Dropout/","title":"(原理\u0026实战)Dropout","section":"正则化","content":"\nDropout # (原理\u0026amp;实战)Dropout\n"},{"id":25,"href":"/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLModel/","title":"机器学习-模型","section":"机器学习","content":"\n机器学习-模型 # 机器学习-模型\n"},{"id":26,"href":"/www6vAlgo/docs/DeepLearning/Transformer/SelfAttention/","title":"(原理)Self-Attention","section":"Transformer","content":"\nSelf-Attention # (原理)Self-Attention\n"},{"id":27,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Embedding/survey/","title":"(Survey)Embedding","section":"Embedding","content":" 论文 # 论文地址\nOn The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey 通用文本嵌入（GPTE）模型的典型架构和训练方式 # 通用文本embedding的代表模型及参数 # 参考 # Embedding的9点总结-从架构、数据到代表模型\n"},{"id":28,"href":"/www6vAlgo/docs/LLM/Survey/LargeModel/","title":"大模型","section":"Survey","content":" 参考 # 1xx. [译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023） 实战\n1xx. 通向AGI之路：大型语言模型（LLM）技术精要 ***\n1xx. 必看的十二个大模型前沿综述：兼论HALO大模型幻觉检测与缓解方案及Google小模型预测大模型训练不稳定的探索 12个综述\n"},{"id":29,"href":"/www6vAlgo/docs/LLM/Core/Emergent/","title":"(原理)涌现现象","section":"Core","content":" Emergent Abilities # 🔗 文章：Emergent Abilities of Large Language Models (2022.10) (arxiv.org) 🔑关键词和摘要 Keywords: LLMs, Emergent Ability, Scaling abstract 不可预测 不能从小模型的的性能外推 是否能通过继续扩大模型规模来获得更多涌现能力 ⚙️研究设计和结论 定义 通常的涌现现象 大模型的涌现现象 小模型接近随机 大模型突然出现 相变 实验框架 performance vs 1. FLOPs, model parameters Training datasets 叠甲：emergent 与很多因素都有关，本文并不是说到哪个 scale 就会出现 emergent，而是说 emergent 现象普遍存在。 实验1 Few-shot Prompting 测试数据说明: A: 三位数加法，两位数乘法 B: [dɪfərənt], 复原 \u0026ldquo;different,\u0026rdquo; C: 从 e l h l o 复原 hello D: 波斯语问答 E: 针对GPT-3 对抗标的问答 \u0026hellip; 结果 这些 task，以 few-shot 形式展示过以后，都有 emergent 不同模型 emergent scale 不一样 有的 task，只有 540B 的 PaLM emerge了 实验2 增强语言模型能力的 emerge 现象 已知的一些大模型技巧在何种规模下发挥作用？ 大模型技巧 思维链 Chain-of-thought: Let\u0026rsquo;s think step by step. 指令微调 请写一段XXX的描述 草稿本方法： 计算 15+16, 让模型在草稿本上写“5+6=11，进位1” 这些增强语言模型能力的方法都有一定程度的涌现 联想：之前的 prompt tuning，parameter efficient tuning，都是某种随着模型规模扩大的涌现？ 讨论 Emergent 现象的解释 多步能力说 每个子能力达到 90% -\u0026gt; 一无是处 每个子能力达到 95% -\u0026gt; 能完成一些任务了 指标缺陷说 奇怪的现象：交叉熵损失不是 emergent 的，而是在逐步下降 Emergent 的阈值可能会越来越小 更干净的数据，更好的训练技巧，更优秀的模型结构都可以是 Emergent阈值变小 未来方向： 继续扩大 model scale，远未达到上限 一些新结构的 scaling 数据的 scaling 理解 prompt 机制 更前沿的 task，用来指导 emergent 理解 emergence 📚论文贡献 优点 第一次正式提出 emergent 实验 做了充分的实验表明该现象在各种数据集上广泛存在 甚至验证了一些“方法”的涌现 提出了一些解释该现象的观点，并提出质疑 改进点 还是不知道为啥 emerge 实验采用各种不同模型，无法得出哪个计算量级对哪种能力有 emerge 参考 # 清华博士带你思考大语言模型LLM的涌现现象（Emergent） 有脑图 Emergent Abilities of Large Language Models （https://arxiv.org/abs/2206.07682）\n再谈ChatGPT等大模型的涌现能力：关于涌现能力的定义、测试方法及分析工作总结 "},{"id":30,"href":"/www6vAlgo/docs/LLM/Dense/Llama/","title":"LLaMA","section":"Dense","content":" LLaMA # LLaMA\n"},{"id":31,"href":"/www6vAlgo/docs/RL/Agentic-RL/Search/websailer/","title":"WebSailor","section":"Search","content":" 论文 # 《WebSailor: Navigating Super-human Reasoning for Web Agent》阿里巴巴集团通义实验室\n主要介绍了一种名为WebSailor的新型网页智能体（Web Agent），其在复杂信息寻求任务中展现了超越人类的推理能力。 以下是对文档的深度阅读总结，涵盖其核心问题、方法、实验及贡献。\n一、研究背景与问题定义 # 1.1 信息寻求的挑战 # 互联网时代的信息爆炸超越了人类认知极限（有限记忆、脆弱注意力、无法并行探索）。 专有智能体系统（如OpenAI的Deep Research）已展现出超越人类的性能，但开源智能体仍存在巨大性能差距。 1.2 任务分级 # 论文将信息寻求任务分为三个级别：\nLevel 1：低不确定性任务（如单次搜索即可解决）。 Level 2：高初始不确定性但解决路径清晰（如标准多跳问答）。 Level 3（本文焦点）：高不确定性且解决路径复杂、无预定义路径（如BrowseComp基准中的任务）。 1.3 性能差距根源 # 现有训练范式集中于Level 1–2任务，缺乏对Level 3复杂推理模式的暴露，导致模型无法发展出多步推理能力。\n二、核心方法：WebSailor框架 # 2.1 训练数据合成（SailorFog-QA） # a) 构建复杂知识图 # 通过随机游走从真实网站中提取互联的知识结构，生成具有涌现式非线性结构的图。 b) 生成高不确定性问题 # 采样多样化拓扑的子图（包含新颖的实体与关系组合）。 信息模糊化处理（如将精确日期转为“2010年代初”，名称部分掩码等），强制智能体进行推理而非简单查找。 c) 优势 # 数据基于真实互联网，贴合实际挑战。 子图拓扑多样性自然产生需复杂推理模式（多步演绎、组合与比较分析）的问题。 高度可扩展（子图数量随图规模非线性增长）。 2.2 推理轨迹重建 # a) 问题 # 开源大型推理模型（LRM，如QwQ、DeepSeek-R1）能生成正确轨迹，但其原生推理输出冗长、风格化，直接用于微调会限制智能体的探索策略泛化能力，且长轨迹易超出上下文窗口限制。\nb) 解决方案 # 用LRM生成完整动作-观察序列（丢弃原生冗长思考）。 用另一指令遵循模型（如Qwen-2.5-72B）为每一步动作重建简洁、目标导向的思考（“短链思维”风格），形成高质量监督信号。 2.3 训练流程优化 # a) 冷启动（RFT） # 必要性：RL奖励稀疏（初始近零反馈），且蒸馏依赖低（仅需2k+高质量样本）。 过滤：保留正确轨迹、长度＜32k token、工具调用＞5次（确保复杂性）。 训练目标：增强决策能力（掩码环境观察的损失计算）。 b) 强化学习（DUPO算法） # 挑战：多轮推理与工具使用导致训练缓慢。 创新：复制采样策略优化（训练前过滤全正确样本，训练中复制同一批次内标准差非零的样本），提速2–3倍。 奖励设计：结合格式验证（0.1权重）和答案验证（0.9权重），避免奖励黑客。 三、实验结果 # 3.1 基准测试 # BrowseComp-en/zh：最具挑战性的网页浏览基准，需复杂策略。 GAIA：需多模态与工具使用（仅用文本子集）。 Xbench-DeepSearch：动态深度搜索基准。 3.2 性能对比 # WebSailor（3B/7B/32B/72B）在所有开源模型与智能体方法中领先，且超越部分结合浏览能力的专有LRM（如Grok-3、Doubao）。 WebSailor-72B在BrowseComp-zh上与Doubao持平，虽仍落后于DeepResearch（SOTA），但显著缩小了开源与专有系统的差距。 向下兼容性：在简单任务（如SimpleQA）上也表现优异。 3.3 关键分析 # 数据复杂性：SailorFog-QA的工具调用分布与BrowseComp-en高度相似（长尾、多＞5次调用），而WebDancer数据集中＞50%仅需2次调用。 RL有效性：RL训练显著提升Pass@1性能（尤其BrowseComp），增强样本效率与稳定性。 冷启动必要性：无冷启动的RL模型工具调用数更低，无法掌握长视距推理，性能差距大。 四、局限与未来工作 # 上下文长度限制（32k token）可能制约更复杂问题的解决。 过度思考倾向：对简单问题也进行多步工具调用（但常为交叉验证，非无意义探索）。 训练效率：同步RL框架低效（仅50步），未来将转向异步训练。 五、结论 # WebSailor通过合成高不确定性数据、重建简洁推理轨迹、以及RFT冷启动与DUPO算法，实现了开源智能体在复杂信息寻求任务上的突破性进展，证明了开源模型可达到接近专有系统的性能。未来将继续探索更复杂任务与高效RL训练，以追求更广泛的“超人类”性能。\n附录与案例 # 文档还提供了：\n工具细节（search、visit）； QA构建流程； 训练超参数； 完整轨迹案例（如BrowseComp-en中关于Joey Hess的查询），展示多步推理与工具调用的实际应用。 总结 # 该研究在数据合成、推理重建和训练优化方面均有显著创新，为开源社区提供了可复现的高性能网页智能体方案，推动了对“超人类推理”能力的探索。\n参考 # 本文由元宝生成\nAgent智能体 | 深入解读阿里开源Web Agent新王者：WebSailor\n"},{"id":32,"href":"/www6vAlgo/docs/RL/core/PPO-family/RewardModel/","title":"(原理|实现)PPO-RewardModel","section":"PPO family","content":"\nPPO-RewardModel # (原理|实现)PPO-RewardModel\n"},{"id":33,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningLoss/","title":"(原理\u0026实战)交叉熵损失","section":"basic","content":"\n交叉熵损失 # (原理\u0026amp;实战)交叉熵损失\n"},{"id":34,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningNorm/","title":"规范化 Norm","section":"网络优化","content":"\nNorm 作用[1] # dnn 的标准组件，稳定和加速训练过程\nBatch Norm[1] # reduce cross batch size mini-batch dimension 一般用于图像，不涉及到padding的问题；\nLayer Norm[1] # reduce cross hidden dim reduce across the feature dimension. 一般用于序列，一个 batch size 内存在 padding；\nRMSNorm: 对 LN 的一种变体，llama 💡 https://spaces.ac.cn/archives/9009 Pre LN: llama Post LN: attention is all you need llama在工程上使用Pre LN\n[pytorch] BN、LN、RMSNorm 及 pre LN vs. post LN 对比，标准化 v *** ​\tnormalization.ipynb\n​\t[pytorch] BN、LN、RMSNorm 及 pre LN vs. post LN 对比，标准化 1xx. Batch Normalization, Layer Normalization and Root Mean Square Layer Normalization: A Comprehensive Guide with Python Implementations\ntodo\n7.5 逐层规范化 百度邱 有代码\nhttps://aistudio.baidu.com/education/lessonvideo/3048901\n"},{"id":35,"href":"/www6vAlgo/docs/DeepLearning/Transformer/ModelGQA/","title":"(原理)GQA","section":"Transformer","content":"\n论文 # GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\nMHA vs. MQA vs. GQA [1] # MHA # 首先是原始的 MHA(Multi-Head Attention)，QKV 三部分有相同数量的头，且一一对应。每次做 Attention，head1 的 QKV 就做好自己运算就可以，输出时各个头加起来就行。\nMQA # 而 MQA 则是，让 Q 仍然保持原来的头数，但 K 和 V 只有一个头，相当于所有的 Q 头共享一组 K 和 V 头，所以叫做 Multi-Query 了。实现改变了会不会影响效果呢？确实会影响但相对它能带来的收益，性能的些微降低是可以接受的。\n能带来多大的收益呢，实验发现一般能提高 30%-40% 的吞吐。\n收益主要就是由降低了 KV cache 带来的。实际上 MQA 运算量和 MHA 是差不多的，可理解为读取一组 KV 头之后，给所有 Q 头用，但因为之前提到的内存和计算的不对称，所以是有利的。\nGQA # 而 GQA 呢，是 MHA 和 MQA 的折衷方案，既不想损失性能太多，又想获得 MQA 带来的推理加速好处。具体思想是，不是所有 Q 头共享一组 KV，而是分组一定头数 Q 共享一组 KV，比如上面图片就是两组 Q 共享一组 KV。\nMQA 和 GQA 形式在推理加速方面，主要是通过两方面来完成：\n降低了从内存中读取的数据量，所以也就减少了计算单元等待时间，提高了计算利用率； KV cache 变小了 head_num 倍，也就是显存中需要保存的 tensor 变小了，空出来空间就可以加大 batch size，从而又能提高利用率。 如果要用 MQA 和 GQA，可以是从头训练的时候就加上，也可以像 GQA 论文里面一样，用已有的开源模型，挑一些头取个 mean 用来初始化 MQA 或 GQA 继续训练一段时间。\nGQA \u0026amp; MQA [2] # 图 4.1 Multi-head attention 拥有 H 个查询、键和值头。Multi-query attention 在所有 查询头之间共享单个键和值头。Grouped-query attention 则在每个查询头组之间共享单 个键和值头，从而在多头和多查询注意力之间进行插值。\nFig. 4.1 Multi-head attention has H query, key, and value heads. Multi-query attention shares single key and value heads across all query heads. Grouped-query attention instead shares single key and value heads for each group of query heads, interpolating between multi-head and multi-query attention.\n参考 # 为什么现在大家都在用 MQA 和 GQA？ ***\nLLM学习系列1：大模型架构要点总结 主流大语言模型的技术原理细节 *** 腾讯 [架构] + 训练 + 微调\n1xx. 理解Attention:从起源到MHA,MQA和GQA *** 1xx. 深度解析Group Query Attention(GQA)为什么能给LLM decoder带来极大推理加速 1xx. 深度学习中的注意力机制：MHA、MQA和GQA\n1xx. 【研1基本功 （真的很简单）Group Query-Attention】大模型训练必备方法——bonus(位置编码讲解) v *** Nomolization[post, pre, sandwich] + Position Encoding[RoPE] + GQA代码 1xx. 一文通透各种注意力：从多头注意力MHA到分组查询注意力GQA、多查询注意力MQA 删除\n手写大模型组件之Group Query Attention，从 MHA，MQA 到 GQA\n"},{"id":36,"href":"/www6vAlgo/docs/LLM/Dense/LlamaFamily/","title":"LLaMA 家族","section":"Dense","content":" LLaMA 家族[1] # 项目 描述 数据集 LLaMa 基座模型 公开可用的数据集(1T token) Stanford Alpaca 结合英文语料通过Self Instruct方式微调LLaMA 7B Self Instruct from davinci-003 API(52K) Vicuna-13B 通过ShareGPT.com的7万条对话数据微调LLaMA(Alpaca基础之上, 多轮对话和长序列, full fine-tune) 用户共享对话(70K sample) BELLE 结合中文语料通过Self Instruct方式微调BLOOMZ-7B或LLaMA Chinese-LLaMA/Chinese-Alpaca 通过中文数据预训练/指令微调LLaMA 姜子牙系列模型Ziya-LLaMA-13B-v1 基于LLaMA-13B的中英文模型 ChatLLaMA(英文版) LLaMA的RLHF版 ColossalChat 通过self-instruct技术指令微调LLaMA且加上RLHF {% asset_img \u0026rsquo;llama2-famaly.jpg\u0026rsquo; %}\n参考 # 家族 # LLaMA的解读与其微调：Alpaca-LoRA/Vicuna/BELLE/中文LLaMA/姜子牙/LLaMA 2 *** 1xx. 我想学大模型，应该从哪个模型开始？LLaMA生态家谱整理和分析 1xx. NLP（九）：LLaMA, Alpaca, ColossalChat 系列模型研究\n1xx. \u0026laquo;千帆增强版 Llama 2-提升大模型对话指令遵循能力\u0026raquo; v\n1xx. 近期大模型动态：LLaMA-2-7B-32K的训练数据组织情况及面向儿童心理健康领域的微调模型推介 llama-2-7b-32k - LLaMA-2的上下文长度为4Ktoken。要将其扩展到32K上下文，该工作分成了三个部分：建模、数据和系统优化。\n实战 # 1xx. 从0到1复现斯坦福羊驼（Stanford Alpaca 7B） GPUs: 8 卡 A800 80GB GPUs\n汉化 # 1xx. 掘力计划 23 期-Linly-Chinese-LLaMA2 中文开源大模型方案分享 v\n"},{"id":37,"href":"/www6vAlgo/docs/LLM/Core/Hallucination/","title":"(原理)幻觉问题","section":"Core","content":" 幻觉[3] # Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge. 大型语言模型中的幻觉通常是指模型生成不忠实、捏造、不一致或无意义的内容。作为一个术语，幻觉在某种程度上被推广到模型犯错的情况。在这里，我想将幻觉问题缩小到模型输出是捏造的， 而不是基于所提供的上下文或世界知识的情况。\nThere are two types of hallucination: 幻觉有两种类型：\nIn-context hallucination: The model output should be consistent with the source content in context. 上下文幻觉：模型输出应与上下文中的源内容一致。 Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so. 外在幻觉：模型输出应以训练前数据集为基础。但是，考虑到预训练数据集的大小，检索和识别每代冲突的成本太高。如果我们将预训练数据语料库视为世界知识的代理，我们基本上会尝试确保模型输出是真实的，并且可以通过外部世界知识进行验证。同样重要的是，当模型不知道某个事实时，它应该这么说。 Anti-Hallucination Methods[3] # RAG → Edits and Attribution # Self-RAG (“Self-reflective retrieval-augmented generation”; Asai et al. 2024) trains a LM end-to-end to learn to reflect on its own generation by outputting both task output and intermittent special reflection tokens. They created a supervision dataset for a critic model and a generator model by prompting GPT-4 and then distilled that into an in-house model to reduce inference cost. Self-RAG（“自反射检索增强一代”;Asai 等人，2024 年）通过输出任务输出和间歇性特殊反射令牌 ，端到端训练 LM 以学习反射自己的生成。他们通过提示 GPT-4 为 critic 模型和生成器模型创建了一个监督数据集，然后将其提炼成内部模型以降低推理成本。\nChain of Actions # Without grounding by external retrieved knowledge, we can design a process for using the model itself to do verification and revision to reduce hallucination. 在没有外部检索知识的基础的情况下，我们可以设计一个流程，使用模型本身进行验证和修改，以减少幻觉。\nDhuliawala et al. (2023) proposed a method named Chain-of-Verification (CoVe) based on a chain of actions to plan and execute verification. [10] Dhuliawala 等人（2023 年） 提出了一种名为验证链 （CoVe） 的方法，该方法基于一系列行动来计划和执行验证。\nRECITE (“Recitation-augmented generation”; Sun et al. 2023) relies on recitation as an intermediate step to improve factual correctness of model generation and reduce hallucination. The motivation is to utilize Transformer memory as an information retrieval mechanism. Within RECITE’s recite-and-answer scheme, the LLM is asked to first recite relevant information and then generate the output. Precisely, we can use few-shot in-context prompting to teach the model to generate recitation and then generate answers conditioned on recitation. Further it can be combined with self-consistency ensemble consuming multiple samples and extended to support multi-hop QA. RECITE （“朗诵增强生成”;Sun 等人，2023 年）依靠背诵作为中间步骤来提高模型生成的事实正确性并减少幻觉。动机是利用 Transformer 内存作为信息检索机制。在 RECITE 的背诵和回答方案中，要求 LLM 首先背诵相关信息，然后生成输出。准确地说，我们可以使用小镜头上下文提示来教模型生成背诵，然后生成以背诵为条件的答案。此外，它可以与使用多个样本的自一致性集成相结合，并扩展以支持多跳 QA。\ntodo\nSurvey # 论文 # 论文地址 A Survey of Hallucination in Large Foundation Models Paper 1xx. 大模型前沿热点最新综述：大模型微调遗忘、Agent智能体、幻觉及RAG检索增强模型推介 大模型微调遗忘 幻觉\n论文 # 论文地址 Siren\u0026rsquo;s Song in the AI Ocean: A Survey on Hallucination in Large Language Models Paper 1xx. 人工智能海洋中的塞壬之歌：大型语言模型LLM中的幻觉研究综述（一） 1xx. 大型语言模型的幻觉研究｜减轻及避免大模型LLM幻觉（二） 1xx. 值得一读的大模型生成幻觉研究综述：大模型幻觉的起因、评估以及减轻策略总结 幻觉 vs 事实性[1] # 幻觉主要是指LLM生成毫无根据或毫无根据的内容，幻觉可以理解为模型倾向于\u0026quot;生成与某些来源相关的无意义或不真实的内容\u0026quot;。这与事实性问题不同，后者强调模型学习、获取和利用事实性知识的能力。\n举例说明两者的区别：\n如果一个LLM在被要求创作\u0026quot;一个关于兔子和狼交朋友的童话故事\u0026quot;时，创作出了一个关于\u0026quot;兔子和狗交朋友\u0026quot;的故事，那么它就表现出了幻觉。不过，这并不一定是事实性错误。 如果生成的内容包含准确的信息，但与提示的具体内容有出入，那就是幻觉，而不是事实性问题。 例如，如果LLM的输出包含了比提示指定更多的细节或不同的元素，但事实仍然正确，这就是幻觉。\n相反，如果LLM避免给出直接答案，而是说\u0026quot;我不知道\u0026quot;，或者给出了一个准确的答案，但遗漏了一些正确的细节，那么这就是事实性问题，而不是幻觉。\n此外，值得注意的是，幻觉有时会产生一些内容，虽然与原始输入内容有偏差，但在事实方面仍然是准确的。\n解决方案[2] # Prompt 工程 *\nFew-shot 外部知识 *\nRAG 后处理 *\n实事检查 * 人工检查 * 提升数据质量\nPretraining的数据质量 SFT的数据质量 模型能力提升 *\n微调 参考 # 再看大模型事实性的界定、错误的起因、评估及前沿缓解方案：Survey on Factuality in LLMS\n降低大模型幻觉的5种方案 v\n减少大模型幻觉，你必须要掌握的 6 个方法！ v\nExtrinsic Hallucinations in LLMs ***\n【译】LLM中的外部幻觉\nWork # 再看大模型幻觉问题如何缓解 ：Chain-of-Verification-一种基于链式验证思想的自我修正工作解读 1xx. 也看缓解大模型幻觉的多阶段RAG框架：加入混合检索、过程理由生成与验证的方案 survey # 1xx. 大模型的幻觉问题调研: LLM Hallucination Survey\n1xx. 网络安全领域微调模型SecGPT：兼看大模型幻觉的度量方式、评估benchmark及RAG增强不同方式 大模型幻觉综述\n1xx. LLM之幻觉（一）：大语言模型幻觉解决方案综述\n"},{"id":38,"href":"/www6vAlgo/docs/LLM/Dense/Llama3-1/","title":"Llama3.1","section":"Dense","content":" Llama3.1 # Llama3.1\n"},{"id":39,"href":"/www6vAlgo/docs/RL/core/DPO/","title":"(原理|实现)DPO","section":"Core","content":"\nDPO # (原理|实现)DPO\n"},{"id":40,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningGradOpt/","title":"(原理)梯度优化","section":"网络优化","content":"\n梯度优化 # Gradient accumulation # Gradient checkpointing [10] # 显存占用优化算法\nmemory usage 与 computation time 之间的 tradeoff ； gradient checkpointing\nIn deep neural networks, backpropagation requires storing intermediate activations for computing gradients during the backward pass.\n但是当层数变多时，存储所有的中间层的激活值（intermediate activations）非常地占用显存；\ngradient checkpointing\n选择性地重新计算（recompute）一部分的 intermediate activations 在反向传播过程中来缓解显存的压力；\nGradient Clipping (梯度裁剪) # 目的[21] # 梯度爆炸问题的常见应对方式为“梯度裁剪”，也就是通过“clip”方式来防止迭代中梯度值过大。\n两种常见形式[20] # 梯度范数裁剪（Gradient Norm Clipping）: 这种方法涉及计算所有参数梯度的范数（例如L2范数），如果这个范数超过了设定的阈值，就将梯度缩放到这个阈值以内。在PyTorch中，这可以通过 torch.nn.utils.clip_grad_norm_ 函数实现。 梯度值裁剪（Gradient Value Clipping）: 这种方法对每个参数的梯度值进行独立裁剪，确保它们不会超过一个设定的最大值或最小值。在PyTorch中，这可以通过 torch.nn.utils.clip_grad_value_ 函数实现。 参考 # overview # Performance and Scalability: How To Fit a Bigger Model and Train It Faster ***\ngradient accumulation # 1xx. [LLMs 实践] 11 gradient accumulation 显存优化 trick v\n​\tgradient_accumulation.ipynb\n​\t[ LLMs 实践] 11 gradient accumulation 显存优化 trick 1xx. Pytorch入门（7）—— 梯度累加（Gradient Accumulation）\n1xx. 聊聊梯度累加(Gradient Accumulation)\n1xx. What is Gradient Accumulation in Deep Learning?\n1xx. Performing gradient accumulation with Accelerate\n​\t使用Accelerate进行梯度累积\ngradient checkpointing # [LLMs 实践] 13 gradient checkpointing 显存优化 trick v ​\tgradient_checkpointing.ipynb\n​\t[LLMs 实践] 13 gradient checkpointing 显存优化 trick ​\tFitting larger networks into memory. *** 看动图\n​\tBackprop and systolic arrays.\nGradient Clipping # 梯度裁剪（Gradient Clipping） ​\thttps://github.com/pytorch/pytorch/blob/main/torch/nn/utils/clip_grad.py\n深度炼丹之梯度裁剪 1xx. 【深度学习】第6.2节 梯度裁剪\n1xx. 【Pytorch】梯度裁剪——torch.nn.utils.clip_grad_norm_的原理及计算过程\n1xx. PyTorch使用Tricks：梯度裁剪-防止梯度爆炸或梯度消失 ！！\n"},{"id":41,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningOverfitting/","title":"(原理)过拟合","section":"basic","content":"\n(原理)过拟合 # (原理)过拟合\n"},{"id":42,"href":"/www6vAlgo/docs/DeepLearning/Transformer/TransformerCode/","title":"(实战)Transformer","section":"Transformer","content":"\n参考 # 1xx. transformer.ipynb git Transformer代码实现\n1xx. Transformer transformer.py git\n1xx. [译] Transformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019） V, github Transformers from scratch\n1xx. 从零实现Transformer的简易版与强大版：从300多行到3000多行\n1xx. Transformer源码详解（Pytorch版本）\n"},{"id":43,"href":"/www6vAlgo/docs/LLM/Core/ImpossibleTriangle/","title":"(原理)不可能三角","section":"Core","content":" 不可能三角[1] # 不可能三角 # 预训练模型之所以是划时代的进展，是它具备了中等尺寸（一张卡即可精调）和全任务SOTA的精调效果 而最近两年预训练模型都在往大尺寸发展，也就是具备了少样本效果，但他们的少样本效果依旧比不过中等模型的精调 弥补方法 # 优化size 对于减少模型尺寸，一条典型的故事线就是蒸馏。但其中仍存在两个问题：一是学生模型很难达到原始模型的效果，二是原始的大尺寸模型的推理效率太低 优化few-shot 对于提升少样本表现，数据增强是一个好办法，比如用无监督数据做自监督训练、或者基于其他模型生成一些伪样本，但这类方法依旧受限于现有标注样本的多样性，泛化性能提升有限 fine-tuning 对于提升精调表现和效率（其实也偏少样本），最近一个比较火的故事是prompt，但这种方式对prompt的设计非常敏感，同时效果也很难超过目前的有监督SOTA 其他 不可能三角 # 分布式系统 # CAP理论 C 一致性 A 可用性 P 分区 分布式存储 # RUM猜想 Read-overhead Update-overhead Memory-overhead 范式 # pretrain, finetune 范式[3] # 第三阶段范式\npretrain, prompt, predict 范式[3] # 第四阶段范式\n总结 # 根据不可能三角形， pretrain, finetune 范式[3] 向pretrain, prompt, predict 范式[3]的迁移是受大模型大小的影响\n参考 # 不可能三角 # 预训练模型的下一步？突破Impossible Triangle Impossible Triangle: What’s Next for Pre-trained Language Models? 微软朱晨光：预训练模型下一步怎么走？突破PLM的「不可能三角」 Go to Page self "},{"id":44,"href":"/www6vAlgo/docs/RL/framework/veRLConfig/","title":"(原理\u0026实战)veRL Config","section":"Framework","content":"\nveRL Config # (原理\u0026amp;实战)veRL Config\n"},{"id":45,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Embedding/Embedding/","title":"(原理)Embedding","section":"Embedding","content":" example [1] # 降维: t-SNE K-Means 聚类 文本搜索 相似度搜索 Embedding 价值 [2] # 降维 将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。 捕捉语义信息 Embedding不仅仅是降维，更重要的是，它能够捕捉到数据的语义信息。 泛化能力 由于Embedding能够捕捉到数据的一些内在规律，因此对于这些未见过的数据，Embedding仍然能够给出合理的表示 应用 [2] # 语义表示和语义相似度 词语关系和类比推理 上下文理解 文本分类和情感分析 机器翻译和生成模型 天梯榜 # mteb/leaderboard\nexample[3] # m3e模型 bge模型 参考 # embedding git\n《AI 大模型应用开发实战营》 03-大模型开发基础：Embedding\n一文通透Text Embedding模型：从text2vec、openai-ada-002到m3e、bge\n1xx. 如何选取RAG中的embedding模型 v ***\nhuggingface embedding模型排行榜\nSentence Bert Demo Repo git\n1xx. 引入任务Instruction指令的句子向量化方案：Instructor的实现思路及训练数据集构造方案\nRepo git\n1xx. 也看利用大模型进行RAG文本嵌入训练数据生成：兼看面向NLP任务的开源指令微调数据集 《Improving Text Embeddings with Large Language Models》\n1xx. 如何提高LLMs的文本表征(Text Embedding)能力?\n《Improving Text Embeddings with Large Language Models》\n1xx. 文本转向量教程s2——认识文本转向量方法（sbert本质和推理加速） V\n"},{"id":46,"href":"/www6vAlgo/docs/LLM/Core/Eval/","title":"测评 *","section":"Core","content":" 基础指标[1] # 分类任务\nAccuracy Precision Recall F1-score AUC-ROC 曲线 生成任务\nBLEU ROUGE METEOR 人工评估 回归任务\nMSE MAE R2 PPL 困惑度\n测评集 # MMLU C-EVAL Framework # OpenCompass 参考 # AI大模型面试题：5.模型微调怎么评估效果 1xx. 一些讨论：三张关于大模型微调方案的脑图及几点llama2等行业落地的问题思考 1xx. https://github.com/CLUEbenchmark/SuperCLUE-Llama2-Chinese\n1xx. 如何让自己的大模型榜单评分更高：也谈榜单评测评分的一些常见方案和典型案例 CEval # 1xx. 大模型落地的一些前沿观点：兼看知识图谱增强大模型问答的几个方案及CEVAL榜单评测启发 二、CEVAL榜单评测中能够得到一些启示\n1. C-Eval 数据集评测简明教程\n1xx. 大模型B端落地“牛刀杀鸡”的奇怪感觉：兼看CEVAl通用评测到金融、医疗两大垂域评测的转变 CEVAl\nFramework # https://opencompass.org.cn/home\n"},{"id":47,"href":"/www6vAlgo/docs/DeepLearning/Transformer/TrainTokenizer/","title":"Tokenizer","section":"Transformer","content":"\ntokenizer 分词 # 单词分词法 单字分词法 子词分词法 BPE [GPT系列], WordPiece 参考 # 1xx. 大模型词表扩充必备工具SentencePiece 1xx. NLP（二）：浅谈分词 1xx. https://www.bilibili.com/video/BV1vN411p7t2/ 1xx. 开源大模型如何更好地适应中文场景：LLAMA扩充词表、BLOOM裁剪词表基本原理与开源实现\n"},{"id":48,"href":"/www6vAlgo/docs/RL/Agentic-RL/Search/Kimi-Researcher/","title":"Kimi-Researcher","section":"Search","content":" 论文 # Kimi-Researcher - End-to-End RL Training for Emerging Agentic Capabilities\n参考 # 强化学习智能体新模板：深入解析Kimi‑Researcher\n1xx. Kimi-Researcher：端到端强化学习驱动的自主智能体\n1xx. up: 有个视频 "},{"id":49,"href":"/www6vAlgo/docs/RL/core/unified/","title":"unified paradigm","section":"Core","content":" RL unified paradigm # RL unified paradigm # DPO # PPO # GRPO # 参考 # DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n"},{"id":50,"href":"/www6vAlgo/docs/DeepLearning/basic/Pytorch/","title":"(实战)PyTorch","section":"basic","content":"\nPyTorch 实战 # (实战)PyTorch\n"},{"id":51,"href":"/www6vAlgo/docs/LLM/Survey/LeaderBoard/","title":"大模型 排行榜","section":"Survey","content":" 大模型 # 排行榜 # HuggingFaceH 大模型排行榜\nLLM Collection\n中国排行榜 # 中国大模型 通用 39 金融 25 司法 8 法律 6 医学 13 医疗 24 教育 13 科研 17 工业 23 政务 12 运维 7 "},{"id":52,"href":"/www6vAlgo/docs/LLM/Reasoning/Survey/ReasoningTTS/","title":"(Survey) Test-Time Scaling","section":"Survey","content":"\nTest-Time Scaling # (Survey) Test-Time Scaling\n"},{"id":53,"href":"/www6vAlgo/docs/LLM/Reasoning/Survey/ReasoningPostTraining/","title":"(Survey)Reasoning LLM Post-Training","section":"Survey","content":"\nReasoning LLM Post-Training # "},{"id":54,"href":"/www6vAlgo/docs/RL/core/GRPO-family/GRPO/","title":"(原理)GRPO","section":"GRPO Family","content":"\nGRPO # GRPO\n"},{"id":55,"href":"/www6vAlgo/docs/LLM/Reasoning/R1/DeepseekDistill/","title":"(实战)Deepseek 蒸馏","section":"R1","content":"\n(实战)Deepseek 蒸馏\n"},{"id":56,"href":"/www6vAlgo/docs/LLM/Reasoning/R1/DeepseekSFT/","title":"(实战)Deepseek R1 SFT","section":"R1","content":"\nDeepSeek R1 SFT # (实战)DeepSeek R1 SFT\n"},{"id":57,"href":"/www6vAlgo/docs/LLM/Reasoning/R1/DeepSeekExplain2/","title":"解读 DeepSeek[邱锡鹏]","section":"R1","content":"\n{% asset_img \u0026lsquo;ubqella3.bmp\u0026rsquo; %}\n{% asset_img \u0026lsquo;gelov7re.bmp\u0026rsquo; %}\n{% asset_img \u0026lsquo;Deepseek-R1实现过程.png\u0026rsquo; %}\n这基本上就是R1的技术路线。我简单列一些关于DeepSeek R1的思考和启发：\n1、R1/R1-zero的技术路线和社区对o1复现的差异\n此前社区对o1的复现基本都会涉及到蒸馏和搜索。 R1-Zero没有SFT，没有过程监督，没有搜索，也能训练出类似o1的效果。学术界之前也有很多实验，但在较小的模型上都没有成功。说明只有基模型足够强，Scaling RL才能取得比较好的效果。 虽然R1强调MCTS没有效果，但是简单的majority vote能大幅提升R1的效果，说明搜索仍然是重要的Scale的范式。 R1的成功还依赖DeepSeek强大的系统效率和RL调教能力。 2、策略初始化\nR1-zero是一个比较好的尝试，但是R1还是经过了先SFT（大概几干条）后再进行RL。 未来后训练的重心会逐步倾向于RL，但是少量训练用于SFT可能还是必须的。 3、奖励模型\nR1的奖励设计跟普通的后训练没特别大的区别（Qwen2，Tulu3），有ground truth用ground truth做EM，否则用RM。 RM的（训练数据量，模型大小，OOD问题，选代周期）的相关问题在整个训练的流程中还是比较关键。可能使用当前开源的比较强大的RM可以达到比较好的效果，也有可能基于内部的数据重新进行了偏好标注。 奖励设计（例如RPM的技巧）可能会在基于少量样本的强化学习微调上仍然起到显著作用。 4、PRM和MCIS\nDS给了两个PRM和MCTS的“不成功尝试”。但PRM部分说的比较笼统，并且DS的PRM只评估Correctness（与OAI的Lets verify step by step一致）。 R1给的是一个简单而且可规模化的可行解，这样做不一定是最优的。基于R1的Test-time search也继续优化它的效果。 PRM总归是一种比较稠密的监督信号，按照传统R1的理论，对OR进行shaping可以使训练更稳定或收敛得更快。 PRM不应该是一个被完全放弃的东西，可以让模型收敛得更快速或更稳定（Scaling曲线的斜率更大）。 5、写作能力提升\no1相比4o在写作等任务上的提升非常小，但R1的创作经常会令人眼前一亮，可能主要是强基模型在Scale RL后涌现的能力，也有人猜测是因为R1的安全对齐做的比较少，没有太约束模型的创作能力。 6、过度优化问题\nR1经常会使用一些高端词汇，典型的如量子纠缠和熵增熵减（会用在各个领域）。猜测是某种形式的reward hacking导致的。 R1在一些通用领域没有ground truth的任务上的推理效果还并不理想，强化学习的训练并不能保证泛化。 7、Test-Time Scaling\no1出来后大家讨论比较多的是Test-Time Scaling，但重要的还是Training-Time Scaling，包括数据和Training Step。蒸馏见效快，但上限不高，重要的还是高质量致据的缺失，蒸馏数据无法提供训练Scaling。RL是其中的关键，因为它可以保障有足够的数据和足够的训练步骤。 8、Agentic展望\nR1是目前唯一同时具有强推理能力和联网搜索的产品，效果很好，可以调研一些复杂的信息并进行回答。强推理模型最终的落脚点大概率是Agent，怎么用强推理模型帮助Agent更好更鲁棒是一个比较重要的问题。 参考 # DeepSeek最强专业拆解来了，清交复教授超硬核解读\n"},{"id":58,"href":"/www6vAlgo/docs/LLM/Reasoning/R1/DeepSeekExplain1/","title":"解读 DeepSeek[刘知远]","section":"R1","content":"\n{% asset_img \u0026lsquo;1ffwdizy.bmp\u0026rsquo; %}\n硬核解读 DeepSeek：大模型强化学习技术原理与大模型技术发展研判\n"},{"id":59,"href":"/www6vAlgo/docs/LLM/Reasoning/R1/ReasoningLLM/","title":"(原理)Reasoning LLM","section":"R1","content":"\nReasoning LLM # (原理)Reasoning LLM\n"},{"id":60,"href":"/www6vAlgo/docs/LLM/Reasoning/R1/DeepSeekR1/","title":"(原理) DeepSeek R1","section":"R1","content":"\n(原理) DeepSeek R1 # (原理)DeepSeek R1\n"},{"id":61,"href":"/www6vAlgo/docs/LLM/Reasoning/V3/DeepSeek/","title":"DeepSeek V3","section":"V3","content":"\nDeepSeek V3 # DeepSeek V3\n"},{"id":62,"href":"/www6vAlgo/docs/LLM/Dense/BERT/","title":"(原理)BERT","section":"Dense","content":"\nBERT # (原理)BERT\n"}]