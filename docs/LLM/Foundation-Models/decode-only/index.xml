<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>decode-only on LLM 算法</title>
    <link>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/decode-only/</link>
    <description>Recent content in decode-only on LLM 算法</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 04 Sep 2024 12:24:07 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/decode-only/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GPT 系列</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptFamily/gptFamily/</link>
      <pubDate>Sun, 11 Dec 2022 16:21:04 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptFamily/gptFamily/</guid>
      <description>&lt;h1 id=&#34;进化时间线&#34;&gt;&#xA;  进化时间线&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%bf%9b%e5%8c%96%e6%97%b6%e9%97%b4%e7%ba%bf&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img &amp;lsquo;family.jpg&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;h1 id=&#34;gpt1-1&#34;&gt;&#xA;  GPT1 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gpt1-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;它是最早一批提出在 NLP 任务上使用 &lt;strong&gt;pre-train + fine-tuning 范式&lt;/strong&gt;的工作。&lt;/li&gt;&#xA;&lt;li&gt;GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;预训练模型具有 zero-shot 的能力&lt;/strong&gt;，并且能随着预训练的进行不断增强&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;gpt2-1&#34;&gt;&#xA;  GPT2 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gpt2-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;核心思想&#34;&gt;&#xA;  核心思想&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，&lt;strong&gt;不需要在下游任务微调&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;h3 id=&#34;gpt-2-vs-gpt-1&#34;&gt;&#xA;  GPT-2 vs. GPT-1&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gpt-2-vs-gpt-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;主推 zero-shot&lt;/strong&gt;，而 GPT-1 为 pre-train + fine-tuning；&lt;/li&gt;&#xA;&lt;li&gt;训练数据规模更大，GPT-2 为 800w 文档 40G，GPT-1 为 5GB；&lt;/li&gt;&#xA;&lt;li&gt;模型大小，GPT-2 最大 15 亿参数，GPT-1为 1 亿参数；&lt;/li&gt;&#xA;&lt;li&gt;模型结构调整，层归一化和参数初始化方式；&lt;/li&gt;&#xA;&lt;li&gt;训练参数，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024，等等；&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;gpt3-1&#34;&gt;&#xA;  GPT3 [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gpt3-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;下游任务评估方法&#34;&gt;&#xA;  下游任务评估方法&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%8b%e6%b8%b8%e4%bb%bb%e5%8a%a1%e8%af%84%e4%bc%b0%e6%96%b9%e6%b3%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;GPT-3 在下游任务的评估与预测时，提供了三种不同的方法：&#xA;&lt;strong&gt;Zero-shot&lt;/strong&gt;：仅使用当前任务的自然语言描述，不进行任何梯度更新；&#xA;&lt;strong&gt;One-shot&lt;/strong&gt;：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新；&#xA;&lt;strong&gt;Few-shot&lt;/strong&gt;：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLaMA</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptLlama/gptLlama/</link>
      <pubDate>Sun, 01 Jan 2023 19:35:09 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptLlama/gptLlama/</guid>
      <description>&lt;h1 id=&#34;llama&#34;&gt;&#xA;  LLaMA&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llama&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/LLaMA-2b55a2a573b549df9f6fc4cc26c2292f?pvs=4&#34;&gt;LLaMA&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLaMA 家族</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptLlamaFamily/gptLlamaFamily/</link>
      <pubDate>Fri, 24 Feb 2023 22:14:52 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptLlamaFamily/gptLlamaFamily/</guid>
      <description>&lt;h1 id=&#34;llama-家族1&#34;&gt;&#xA;  LLaMA 家族[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llama-%e5%ae%b6%e6%97%8f1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;项目&lt;/th&gt;&#xA;          &lt;th&gt;描述&lt;/th&gt;&#xA;          &lt;th&gt;数据集&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;LLaMa&lt;/td&gt;&#xA;          &lt;td&gt;基座模型&lt;/td&gt;&#xA;          &lt;td&gt;公开可用的数据集(1T token)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Stanford Alpaca&lt;/td&gt;&#xA;          &lt;td&gt;结合英文语料通过Self Instruct方式微调LLaMA 7B&lt;/td&gt;&#xA;          &lt;td&gt;Self Instruct from davinci-003 API(52K)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Vicuna-13B&lt;/td&gt;&#xA;          &lt;td&gt;通过ShareGPT.com的7万条对话数据微调LLaMA(Alpaca基础之上, 多轮对话和长序列, full fine-tune)&lt;/td&gt;&#xA;          &lt;td&gt;用户共享对话(70K sample)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;BELLE&lt;/td&gt;&#xA;          &lt;td&gt;结合中文语料通过Self Instruct方式微调BLOOMZ-7B或LLaMA&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Chinese-LLaMA/Chinese-Alpaca&lt;/td&gt;&#xA;          &lt;td&gt;通过中文数据预训练/指令微调LLaMA&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;姜子牙系列模型Ziya-LLaMA-13B-v1&lt;/td&gt;&#xA;          &lt;td&gt;基于LLaMA-13B的中英文模型&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ChatLLaMA(英文版)&lt;/td&gt;&#xA;          &lt;td&gt;LLaMA的RLHF版&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;ColossalChat&lt;/td&gt;&#xA;          &lt;td&gt;通过self-instruct技术指令微调LLaMA且加上RLHF&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;{% asset_img &amp;rsquo;llama2-famaly.jpg&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;家族&#34;&gt;&#xA;  家族&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%b6%e6%97%8f&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/v_JULY_v/article/details/129709105&#34;&gt;LLaMA的解读与其微调：Alpaca-LoRA/Vicuna/BELLE/中文LLaMA/姜子牙/LLaMA 2&lt;/a&gt; ***&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzUyOTA5OTcwMg==&amp;amp;mid=2247485019&amp;amp;idx=1&amp;amp;sn=e3417472c0c1f98aede498fbe905e1a0&amp;amp;&#34;&gt;我想学大模型，应该从哪个模型开始？LLaMA生态家谱整理和分析 &lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/618695885&#34;&gt;NLP（九）：LLaMA, Alpaca, ColossalChat 系列模型研究&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &amp;laquo;千帆增强版 Llama 2-提升大模型对话指令遵循能力&amp;raquo;  v&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648402185&amp;amp;idx=2&amp;amp;sn=55901b89381e27aedee56c69041f6af8&#34;&gt;近期大模型动态：LLaMA-2-7B-32K的训练数据组织情况及面向儿童心理健康领域的微调模型推介 &lt;/a&gt;    llama-2-7b-32k -  LLaMA-2的上下文长度为4Ktoken。要将其扩展到32K上下文，该工作分成了三个部分：建模、数据和系统优化。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Llama3.1</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptLlama3-1/gptLlama3-1/</link>
      <pubDate>Wed, 04 Sep 2024 12:24:07 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptLlama3-1/gptLlama3-1/</guid>
      <description>&lt;h1 id=&#34;llama31&#34;&gt;&#xA;  Llama3.1&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llama31&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/LLaMA3-1-c2124d23077241afa9d92eb9a54a043a?pvs=4&#34;&gt;Llama3.1&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
