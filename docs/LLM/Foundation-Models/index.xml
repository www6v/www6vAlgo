<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Foundation Models on LLM 算法</title>
    <link>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/</link>
    <description>Recent content in Foundation Models on LLM 算法</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Fri, 17 Feb 2023 17:46:06 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(综述)大模型</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/gptLargeModelSurvey/gptLargeModelSurvey/</link>
      <pubDate>Sun, 30 Oct 2022 19:10:21 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/gptLargeModelSurvey/gptLargeModelSurvey/</guid>
      <description>&lt;h1 id=&#34;llms的背景1&#34;&gt;&#xA;  LLMs的背景[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llms%e7%9a%84%e8%83%8c%e6%99%af1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;scaling-law-of-llms&#34;&gt;&#xA;  Scaling law of LLMs&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#scaling-law-of-llms&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;KM scaling law&lt;/li&gt;&#xA;&lt;li&gt;Chinchilla Scaling law&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;llms的涌现能力&#34;&gt;&#xA;  LLMs的涌现能力&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llms%e7%9a%84%e6%b6%8c%e7%8e%b0%e8%83%bd%e5%8a%9b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;in-context learning&lt;/li&gt;&#xA;&lt;li&gt;instruction following&lt;/li&gt;&#xA;&lt;li&gt;step-by-step reasoning&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;大语言模型的关键技术-&#34;&gt;&#xA;  大语言模型的关键技术 ***&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af-&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Scaling&lt;/li&gt;&#xA;&lt;li&gt;Training&lt;/li&gt;&#xA;&lt;li&gt;Ability Eliciting&lt;/li&gt;&#xA;&lt;li&gt;Alignment Tuning&lt;/li&gt;&#xA;&lt;li&gt;Tool Manipulation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;pre-training1&#34;&gt;&#xA;  Pre-training[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pre-training1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;数据收集&#34;&gt;&#xA;  数据收集&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e6%94%b6%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;架构&#34;&gt;&#xA;  架构&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;模型训练-&#34;&gt;&#xA;  模型训练 ***&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83-&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;优化设置&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Batch Training&lt;/li&gt;&#xA;&lt;li&gt;Learning Rate&lt;/li&gt;&#xA;&lt;li&gt;Optimizer&lt;/li&gt;&#xA;&lt;li&gt;Stabilizing the Training&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;可扩展的训练技巧&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;3D并行&#xA;数据并行 +  流水线并行 + 张量并行&lt;/li&gt;&#xA;&lt;li&gt;ZeRO&lt;/li&gt;&#xA;&lt;li&gt;混合精度训练&lt;/li&gt;&#xA;&lt;li&gt;总体训练建议&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;adaptation-tuning-of-llms1&#34;&gt;&#xA;  Adaptation Tuning of LLMs[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#adaptation-tuning-of-llms1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;指令调优-&#34;&gt;&#xA;  指令调优 ***&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8c%87%e4%bb%a4%e8%b0%83%e4%bc%98-&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;本质上，指令微调是在&lt;strong&gt;自然语言格式的实例（instance）集合上&lt;/strong&gt;微调预训练后的 LLM 的方法 [62]。&lt;/p&gt;</description>
    </item>
    <item>
      <title>大模型</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/gptLargeModel/gptLargeModel/</link>
      <pubDate>Fri, 17 Feb 2023 17:46:06 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/gptLargeModel/gptLargeModel/</guid>
      <description>&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;http://arthurchiao.art/blog/llm-practical-guide-zh/&#34;&gt;[译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023）&lt;/a&gt;   实战&lt;br&gt;&#xA;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/597586623&#34;&gt;通向AGI之路：大型语言模型（LLM）技术精要&lt;/a&gt; ***&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648403847&amp;amp;idx=1&amp;amp;sn=9af731e9f8418a2d869f5464530c8bd6&#34;&gt;必看的十二个大模型前沿综述：兼论HALO大模型幻觉检测与缓解方案及Google小模型预测大模型训练不稳定的探索 &lt;/a&gt; 12个综述&lt;/p&gt;</description>
    </item>
    <item>
      <title>大模型 排行榜</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/gptLeaderBoard/gptLeaderBoard/</link>
      <pubDate>Wed, 04 Jan 2023 11:14:49 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Foundation-Models/gptLeaderBoard/gptLeaderBoard/</guid>
      <description>&lt;h1 id=&#34;大模型&#34;&gt;&#xA;  大模型&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%a7%e6%a8%a1%e5%9e%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;排行榜&#34;&gt;&#xA;  排行榜&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%8e%92%e8%a1%8c%e6%a6%9c&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34;&gt;HuggingFaceH 大模型排行榜&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.promptingguide.ai/models/collection&#34;&gt;LLM Collection&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;中国排行榜&#34;&gt;&#xA;  中国排行榜&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%ad%e5%9b%bd%e6%8e%92%e8%a1%8c%e6%a6%9c&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/www6v/awesome-LLMs-In-China&#34;&gt;中国大模型 &lt;/a&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;通用 39&lt;/li&gt;&#xA;&lt;li&gt;金融 25&lt;/li&gt;&#xA;&lt;li&gt;司法 8&lt;/li&gt;&#xA;&lt;li&gt;法律 6&lt;/li&gt;&#xA;&lt;li&gt;医学 13&lt;/li&gt;&#xA;&lt;li&gt;医疗 24&lt;/li&gt;&#xA;&lt;li&gt;教育 13&lt;/li&gt;&#xA;&lt;li&gt;科研 17&lt;/li&gt;&#xA;&lt;li&gt;工业 23&lt;/li&gt;&#xA;&lt;li&gt;政务 12&lt;/li&gt;&#xA;&lt;li&gt;运维 7&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
