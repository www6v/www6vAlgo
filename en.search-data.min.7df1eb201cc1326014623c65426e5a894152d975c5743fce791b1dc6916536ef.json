[{"id":0,"href":"/www6vAlgo/docs/RL/framework/verl/","title":"HybridFlow[veRL]","section":"Framework","content":" 论文 # HybridFlow: A Flexible and Efficient RLHF Framework\n解读 # HybridFlow[veRL]\n"},{"id":1,"href":"/www6vAlgo/docs/RL/Agentic-RL/survey/survey/","title":"(Survey)Agentic RL","section":"Survey","content":" 论文 # The Landscape of Agentic Reinforcement Learning for LLMs: A Survey git 解读 # (Survey)Agentic RL\n参考 # The Landscape of Agentic Reinforcement Learning for LLMs: A Survey\nThe Landscape of Agentic Reinforcement Learning for LLMs: A Survey\n"},{"id":2,"href":"/www6vAlgo/docs/RL/Agentic-RL/Tool/OTC/","title":"OTC","section":"Tool","content":" 参考 # 2025年大模型agent rl训练多轮planning技术TORL,ToolRL, RAGEN,OTC,SkyRL-v0, GiGPO,Tool-N1 ,ARTIST, ZeroTIR, GRPO\n"},{"id":3,"href":"/www6vAlgo/docs/RL/Agentic-RL/Tool/ReTool/","title":"ReTool","section":"Tool","content":" 论文 # 参考 # 通过工具增强 LLM Agent 能力：veRL+ReTool 的完整实践指南\n"},{"id":4,"href":"/www6vAlgo/docs/RL/Agentic-RL/Tool/ToolRL/","title":"ToolRL","section":"Tool","content":" 论文 # ToolRL: Reward is All Tool Learning Needs\n为了确定最佳奖励策略，探索了四个关键维度的各种奖励配置：(1) 奖励类型（奖励哪些方面），(2) 奖励尺度（奖励多少），(3) 奖励粒度（奖励信号的详细程度），以及 (4) 奖励动态（奖励如何随时间演变）。通过大量的实验确定了最符合主体工具使用情况的奖励设计，并揭示了奖励对于调用工具的 LLM 而言“有用”的原因。论文得出的核心见解总结如下：\n推理轨迹越长并不一定越好，而且过长的奖励可能会降低性能。 动态奖励尺度有助于模型从简单行为平稳过渡到复杂行为。 细粒度的奖励分解可实现更稳定、更有效的学习。 论文得出的核心见解总结如下：\n推理轨迹越长并不一定越好，而且过长的奖励可能会降低性能。 动态奖励尺度有助于模型从简单行为平稳过渡到复杂行为。 细粒度的奖励分解可实现更稳定、更有效的学习。 Format Reward Correctness Reward\n参考 # 2025年大模型agent rl训练多轮planning技术TORL,ToolRL, RAGEN,OTC,SkyRL-v0, GiGPO,Tool-N1 ,ARTIST, ZeroTIR, GRPO\n"},{"id":5,"href":"/www6vAlgo/docs/RL/Agentic-RL/Search/Search-R1/","title":"Search-R1","section":"Search","content":" 论文 # Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning https://github.com/PeterGriffinJin/Search-R1 Methods # 详细方法和步骤:\n将搜索引擎建模为环境的一部分： SEARCH-R1将搜索引起作为环境的一部分， 让模型与环境交互，从而得到 reward。 支持多轮检索和推理： SEARCH-R1通过特定的标签（\u0026lt;search\u0026gt;, \u0026lt;/search\u0026gt;, \u0026lt;information\u0026gt;, \u0026lt;/information\u0026gt;, \u0026lt;think\u0026gt;, \u0026lt;/think\u0026gt;, \u0026lt;answer\u0026gt;, \u0026lt;/answer\u0026gt;）来支持多轮检索和推理。 优化算法兼容性： SEARCH-R1 与各种 RL 算法兼容，包括 PPO 和 GRPO。 简单结果奖励函数： 避免复杂的基于过程的奖励, 采用简单的基于结果的奖励函数 （字符串匹配作为reward!!!）。 总结 # 结论1: SEARCH-R1 显著提升了LLM在需要实时外部知识的复杂推理任务中的能力。 通过强化学习，LLM可以自主生成查询并有效利用检索到的信息，优于传统的RAG方法。\n结论2: SEARCH-R1在不同LLM架构和训练方法上具有广泛的适用性。 实验结果表明，无论使用基础模型还是指令调整模型，SEARCH-R1都能带来显著的性能提升，且对不同的RL算法（如PPO和GRPO）具有兼容性。\n结论3: SEARCH-R1有很强的实用价值。 SEARCH-R1能够显著提高LLM在需要实时外部知识的复杂推理任务中的能力。 可以用于智能问答，智能助手等领域。\n参考 # Search-R1：让大模型学会“检索+推理”的新范式 1xx. 【论文解读】Search-R1：强化学习如何教会 LLM 自主搜索？\n1xx. 有个学术的会议\n1xx. Search-R1：让 LLM 学会 “边搜边想”，强化学习赋能检索增强推理\n"},{"id":6,"href":"/www6vAlgo/docs/LLM/MOE/gptModelMOE/gptModelMOE/","title":"(原理)Visual  MOE","section":"MOE","content":" Visual MOE # (原理)Visual MOE\n"},{"id":7,"href":"/www6vAlgo/docs/RL/core/GRPODeepseek/","title":"(实战)GRPO","section":"Core","content":"\nGRPO # (实战)GRPO\n"},{"id":8,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningLearningRate/","title":"(原理)学习率","section":"网络优化","content":"\n学习率 # (原理)学习率\n"},{"id":9,"href":"/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/WeightDecay/","title":"(原理\u0026实战)权重衰减","section":"正则化","content":"\n权重衰减 WeightDecay # (原理\u0026amp;实战)权重衰减\n"},{"id":10,"href":"/www6vAlgo/docs/LLM/core/gptScalingLaw/","title":"Scaling Law","section":"core","content":" Scaling Law[10] # Scaling Law # 参数量 vs 数据量 # 参数量 vs 数据量 # 参考 # Scaling Law # 解析大模型中的Scaling Law 1xx. 论文阅读，大模型的缩放定律，Scaling Laws for Neural Language Models 2xx. Training Compute-Optimal Large Language Models 简读 2xx. 【预训练模型】推翻OpenAI结论, DeepMind重新定义预训练的训练参数和训练规模的关系！ 《Scaling Laws for Neural Language Models》 《Training Compute-Optimal Large Language Models》\n"},{"id":11,"href":"/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLData/","title":"机器学习-数据","section":"机器学习","content":"\n机器学习-数据 # 机器学习-数据\n"},{"id":12,"href":"/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptFamily/gptFamily/","title":"GPT 系列","section":"decode-only","content":" 进化时间线 # {% asset_img \u0026lsquo;family.jpg\u0026rsquo; %}\nGPT1 [1] # 它是最早一批提出在 NLP 任务上使用 pre-train + fine-tuning 范式的工作。 GPT 的实验证明了模型的精度和泛化能力会随着解码器层数增加而不断提升，而且目前还有提升空间 预训练模型具有 zero-shot 的能力，并且能随着预训练的进行不断增强 GPT2 [1] # 核心思想 # 当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，不需要在下游任务微调。\nGPT-2 vs. GPT-1 # 主推 zero-shot，而 GPT-1 为 pre-train + fine-tuning； 训练数据规模更大，GPT-2 为 800w 文档 40G，GPT-1 为 5GB； 模型大小，GPT-2 最大 15 亿参数，GPT-1为 1 亿参数； 模型结构调整，层归一化和参数初始化方式； 训练参数，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024，等等； GPT3 [1] # 下游任务评估方法 # GPT-3 在下游任务的评估与预测时，提供了三种不同的方法： Zero-shot：仅使用当前任务的自然语言描述，不进行任何梯度更新； One-shot：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新； Few-shot：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；\nShot[2] One-shot Few-Shot Zero-Shot Few-shot vs fine-tuning # 其中 Few-shot 也被称为 in-context learning，虽然它与 fine-tuning 一样都需要一些有监督标注数据，但是两者的区别是： 【本质区别】 fine-tuning 基于标注数据对模型参数进行更新 而in-context learning使用标注数据时不做任何的梯度回传, 模型参数不更新\nGPT-3 vs. GPT-2 # 效果上，超出 GPT-2 非常多，能生成人类难以区分的新闻文章； 主推 few-shot，相比于 GPT-2 的 zero-shot，具有很强的创新性； 模型结构略微变化，采用 sparse attention 模块； 海量训练语料 45TB（清洗后 570GB），相比于 GPT-2 的 40GB； 海量模型参数，最大模型为 1750 亿，GPT-2 最大为 15 亿参数； InstructGPT [1] # 步骤 # 有监督微调， 奖励模型训练， 强化学习训练 技术方案 # 有监督微调（SFT） 本质上来说，SFT 可以理解为人工标注了一批数据，然后去微调 GPT-3。但是值得一提的是，这里标注的数据与 GPT-3 之前用来做下游任务使用的 few-shot 格式，有非常本质的区别。 InstructGPT 在 SFT 中标注的数据，正是为了消除这种模型预测与用户表达习惯之间的 gap。在标注过程中，他们从 GPT-3 的用户真实请求中采样大量下游任务的描述，然后让标注人员对任务描述进行续写，从而得到该问题的高质量回答。\n基于人类反馈的强化学习（RLHF） {% asset_img \u0026lsquo;instructGPT.jpg\u0026rsquo; %}\n总结 # 解决 GPT-3 的输出与人类意图之间的Align问题； 让具备丰富世界知识的大模型，学习“人类偏好”； 标注人员明显感觉 InstructGPT 的输出比 GPT-3 的输出更好，更可靠； InstructGPT 在真实性，丰富度上表现更好； InstructGPT 对有害结果的生成控制的更好，但是对于**“偏见”没有明显改善**； ChatGPT 训练 [3] # 基于人类反馈的强化学习微调技术 RLHF 使用有监督微调 Supervised Fine-tuning（SFT）预训练语言模型 Supervised fine-tuning (SFT) = Instruction Tuning 训练奖励模型 Reward Model（RM） 使用强化学习算法微调语言模型 RLHF [本质 基于强化学习, 强化学习算法] 参考 # GPT / GPT-2 / GPT-3 / InstructGPT 进化之路 ***\nFew-Shot, Zero-Shot \u0026amp; One-shot 的通俗理解\nAI 大模型微调训练营大纲\n1xx. 万字拆解！追溯ChatGPT各项能力的起源 符尧\n1xx. [Transformer 101系列] ChatGPT是怎么炼成的? 未\n"},{"id":13,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Transformer/","title":"(原理)Transformer","section":"Transformer","content":"\n(原理)Transformer # (原理)Transformer\n"},{"id":14,"href":"/www6vAlgo/docs/LLM/Foundation-Models/gptLargeModelSurvey/gptLargeModelSurvey/","title":"(综述)大模型","section":"Foundation Models","content":" LLMs的背景[1] # Scaling law of LLMs # KM scaling law Chinchilla Scaling law LLMs的涌现能力 # in-context learning instruction following step-by-step reasoning 大语言模型的关键技术 *** # Scaling Training Ability Eliciting Alignment Tuning Tool Manipulation Pre-training[1] # 数据收集 # 架构 # 模型训练 *** # 优化设置\nBatch Training Learning Rate Optimizer Stabilizing the Training 可扩展的训练技巧\n3D并行 数据并行 + 流水线并行 + 张量并行 ZeRO 混合精度训练 总体训练建议 Adaptation Tuning of LLMs[1] # 指令调优 *** # 本质上，指令微调是在自然语言格式的实例（instance）集合上微调预训练后的 LLM 的方法 [62]。\n指令微调后，LLM 可以展现出泛化到未见过任务的卓越能力 [28, 62, 64]，即使在多语言场景下也能有不错表现 [98]。\n格式化实例的构建 # 格式化已有数据集 格式化人类需求 构建实例的关键因素 增加指令 设计格式 总的来说，指令多样性似乎比实例数量更重要\n指令微调策略 # 平衡数据分布 一种广泛使用的方法是实例比例混合策略 [87]，即将所有数据集合并，然后从混合数据集中按比例采样每种实例。 此外，根据最近的研究发现 [64, 99]，提高高质量数据集（例如 FLAN [62] 和 P3 [209]）的采样比例通常可以带来性能提升。\n结合指令微调和预训练 为了使微调过程更加有效和稳定，OPT-IML [99] 在指令微调期间加入了预训练数据，这可以看作是对模型的正则化（regularization）。\n具体而言，GLM-130B [97] 和 Galactica [34] 将指令格式数据集作为预训练语料库的一小部分来预训练 LLM，这有可能同时获得预训练和指令微调的优势。\n指令微调的效果 # 性能改进 最近的研究在多个规模上（从 7700 百万到 5400 亿不等）对 LM 进行了实验，表明不同规模的模型都可以从指令微调中受益 [64, 216]，随着参数规模的增加，性能也得到了提升 [98]。 【普适性】 此外，经过指令微调的较小模型甚至可以比未经微调的较大模型表现更好 [28, 64]。\n任务泛化性 todo 对齐调优 # 高效调优 # 参考 # 大语言模型综述 中文 v10\n大语言模型综述 中文\nLLMSurvey Repo git\n[论文]大语言模型综述\n详谈大模型训练中的数据收集、处理与模型影响：A Survey of Large Language Models工作中的数据总结\n大模型综述-A Survey of Large Language Models 1xx. 值得一看的大模型最新综述：兼看多语种大模型微调数据集Aya 1xx. 43页预训练模型综述（清华、复旦、人大）\n"},{"id":15,"href":"/www6vAlgo/docs/DeepLearning/basic/DeepLearning/","title":"Deep Learning","section":"basic","content":"\nDeep Learning # Deep Learning\n"},{"id":16,"href":"/www6vAlgo/docs/LLM/MOE/gptModelMOECode/gptModelMOECode/","title":"(代码)MOE","section":"MOE","content":" import torch import torch.nn as nn import torch.nn.functional as F import torch_npu from torch_npu.contrib import transfer_to_npu class Expert(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super().__init__() self.net = nn.Sequential( nn.Linear(input_dim, hidden_dim), nn.GELU(), nn.Linear(hidden_dim, output_dim)) def forward(self, x): return self.net(x) class MoE(nn.Module): def __init__(self, input_dim, num_experts, top_k, expert_capacity, hidden_dim, output_dim): super().__init__() self.num_experts = num_experts self.top_k = top_k self.expert_capacity = expert_capacity # 路由网络 self.gate = nn.Linear(input_dim, num_experts) # 专家集合 self.experts = nn.ModuleList( [Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)]) def forward(self, x): batch_size, input_dim = x.shape device = x.device # 路由计算 logits = self.gate(x) probs = torch.softmax(logits, dim=-1) topk_probs, topk_indices = torch.topk(probs, self.top_k, dim=-1) # 辅助损失计算 if self.training: # 重要性损失（专家利用率均衡） importance = probs.sum(0) importance_loss = torch.var(importance) / (self.num_experts ** 2) # 负载均衡损失（样本分配均衡） mask = torch.zeros_like(probs, dtype=torch.bool) mask.scatter_(1, topk_indices, True) routing_probs = probs * mask expert_usage = mask.float().mean(0) routing_weights = routing_probs.mean(0) load_balance_loss = self.num_experts * (expert_usage * routing_weights).sum() aux_loss = importance_loss + load_balance_loss else: aux_loss = 0.0 # 专家分配逻辑 flat_indices = topk_indices.view(-1) flat_probs = topk_probs.view(-1) sample_indices = torch.arange(batch_size, device=device)[:, None]\\ .expand(-1, self.top_k).flatten() # 初始化输出 outputs = torch.zeros(batch_size, self.experts[0].net[-1].out_features, device=device) # 处理每个专家 for expert_idx in range(self.num_experts): # 获取分配给当前专家的样本 expert_mask = flat_indices == expert_idx expert_samples = sample_indices[expert_mask] expert_weights = flat_probs[expert_mask] # 容量控制 if len(expert_samples) \u0026gt; self.expert_capacity: expert_samples = expert_samples[:self.expert_capacity] expert_weights = expert_weights[:self.expert_capacity] if len(expert_samples) == 0: continue # 处理专家计算 expert_input = x[expert_samples] expert_output = self.experts[expert_idx](expert_input) weighted_output = expert_output * expert_weights.unsqueeze(-1) # 累加输出 outputs.index_add_(0, expert_samples, weighted_output) return outputs, aux_loss # 测试示例 if __name__ == \u0026#34;__main__\u0026#34;: input_dim = 128 output_dim = 256 num_experts = 8 top_k = 2 expert_capacity = 32 hidden_dim = 512 batch_size = 64 # add device = torch.device(\u0026#34;npu:4\u0026#34; if torch.npu.is_available() else \u0026#34;cpu\u0026#34;) moe = MoE(input_dim, num_experts, top_k, expert_capacity, hidden_dim, output_dim).to(device) x = torch.randn(batch_size, input_dim).to(device) experimental_config = torch_npu.profiler._ExperimentalConfig( export_type=torch_npu.profiler.ExportType.Text, profiler_level=torch_npu.profiler.ProfilerLevel.Level0, msprof_tx=False, aic_metrics=torch_npu.profiler.AiCMetrics.AiCoreNone, l2_cache=False, op_attr=False, data_simplification=False, record_op_args=False, gc_detect_threshold=None ) with torch_npu.profiler.profile( activities=[ torch_npu.profiler.ProfilerActivity.CPU, torch_npu.profiler.ProfilerActivity.NPU ], schedule=torch_npu.profiler.schedule(wait=0, warmup=0, active=1, repeat=1, skip_first=1), on_trace_ready=torch_npu.profiler.tensorboard_trace_handler(\u0026#34;./moe_stand_npu_result\u0026#34;), record_shapes=False, profile_memory=False, with_stack=False, with_modules=False, with_flops=False, experimental_config=experimental_config) as prof: # 训练模式 for _ in range(10): moe.train() output, loss = moe(x) print(f\u0026#34;Using device: {x.device}\u0026#34;) print(f\u0026#34;Training output shape: {output.shape}\u0026#34;) # torch.Size([64, 256]) print(f\u0026#34;Training auxiliary loss: {loss.item():.4f}\u0026#34;) # 示例值，如0.1234 prof.step() print(\u0026#34;=\u0026#34; * 80) # 推理模式 moe.eval() output, _ = moe(x) print(f\u0026#34;Eval output shape: {output.shape}\u0026#34;) # torch.Size([64, 256]) 参考 # MoE git\n"},{"id":17,"href":"/www6vAlgo/docs/RL/core/PPO/","title":"(原理)PPO","section":"Core","content":"\nPPO # (原理)PPO\n"},{"id":18,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningBatchsize/","title":"(原理)Batchsize","section":"网络优化","content":"\n最佳实践 # batchsize # batchsize 下限 [1] 别太小的限制在于，batch size太小，会来不及收敛。\n所以在常见的setting（～100 epochs），batch size一般不会低于16。\nbatchsize 上限 [1] batch size别太大的限制在于两个点，\n1）batch size太大，memory容易不够用。这个很显然，就不多说了。\n2）batch size太大，深度学习的优化（training loss降不下去）和泛化（generalization gap很大）都会出问题。\nlearning rate \u0026amp; batch size # 总之，可以证明，learning rate/batch size的比值对深度学习是有指数级的影响[3]，所以非常重要，没事别瞎调。[1]\n这也是为什么大的batch_size往往建议可以相应取大点learning_rate, 因为梯度震荡小，大learning_rate可以加速收敛过程，也可以防止陷入到局部最小值，而小batch_size用小learning_rate迭代，防止错过最优点，一直上下震荡没法收敛（这也是一个小trick）。[2]\n参考 # 怎么选取训练神经网络时的Batch size? Summer Clover 训练神经网络时batchsize扩大一倍的同时需要增加epoch数量吗? 新一 7.1 批大小调整实验 百度邱\n7.1 批大小调整实验\n设置BatchSize\n深度学习中的batch的大小对学习效果有何影响？\n"},{"id":19,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningForwardBackward/","title":"(原理\u0026实战)前向/反向传播","section":"basic","content":"\n前向/反向传播 # (原理\u0026amp;实战)前向/反向传播\n"},{"id":20,"href":"/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/Dropout/","title":"(原理\u0026实战)Dropout","section":"正则化","content":"\nDropout # (原理\u0026amp;实战)Dropout\n"},{"id":21,"href":"/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLModel/","title":"机器学习-模型","section":"机器学习","content":"\n机器学习-模型 # 机器学习-模型\n"},{"id":22,"href":"/www6vAlgo/docs/DeepLearning/Transformer/SelfAttention/","title":"(原理)Self-Attention","section":"Transformer","content":"\nSelf-Attention # (原理)Self-Attention\n"},{"id":23,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Embedding/survey/","title":"(Survey)Embedding","section":"Embedding","content":" 论文 # 论文地址\nOn The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey 通用文本嵌入（GPTE）模型的典型架构和训练方式 # 通用文本embedding的代表模型及参数 # 参考 # Embedding的9点总结-从架构、数据到代表模型\n"},{"id":24,"href":"/www6vAlgo/docs/LLM/Foundation-Models/gptLargeModel/gptLargeModel/","title":"大模型","section":"Foundation Models","content":" 参考 # 1xx. [译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023） 实战\n1xx. 通向AGI之路：大型语言模型（LLM）技术精要 ***\n1xx. 必看的十二个大模型前沿综述：兼论HALO大模型幻觉检测与缓解方案及Google小模型预测大模型训练不稳定的探索 12个综述\n"},{"id":25,"href":"/www6vAlgo/docs/LLM/core/gptEmergent/","title":"(原理)涌现现象","section":"core","content":" Emergent Abilities # 🔗 文章：Emergent Abilities of Large Language Models (2022.10) (arxiv.org) 🔑关键词和摘要 Keywords: LLMs, Emergent Ability, Scaling abstract 不可预测 不能从小模型的的性能外推 是否能通过继续扩大模型规模来获得更多涌现能力 ⚙️研究设计和结论 定义 通常的涌现现象 大模型的涌现现象 小模型接近随机 大模型突然出现 相变 实验框架 performance vs 1. FLOPs, model parameters Training datasets 叠甲：emergent 与很多因素都有关，本文并不是说到哪个 scale 就会出现 emergent，而是说 emergent 现象普遍存在。 实验1 Few-shot Prompting 测试数据说明: A: 三位数加法，两位数乘法 B: [dɪfərənt], 复原 \u0026ldquo;different,\u0026rdquo; C: 从 e l h l o 复原 hello D: 波斯语问答 E: 针对GPT-3 对抗标的问答 \u0026hellip; 结果 这些 task，以 few-shot 形式展示过以后，都有 emergent 不同模型 emergent scale 不一样 有的 task，只有 540B 的 PaLM emerge了 实验2 增强语言模型能力的 emerge 现象 已知的一些大模型技巧在何种规模下发挥作用？ 大模型技巧 思维链 Chain-of-thought: Let\u0026rsquo;s think step by step. 指令微调 请写一段XXX的描述 草稿本方法： 计算 15+16, 让模型在草稿本上写“5+6=11，进位1” 这些增强语言模型能力的方法都有一定程度的涌现 联想：之前的 prompt tuning，parameter efficient tuning，都是某种随着模型规模扩大的涌现？ 讨论 Emergent 现象的解释 多步能力说 每个子能力达到 90% -\u0026gt; 一无是处 每个子能力达到 95% -\u0026gt; 能完成一些任务了 指标缺陷说 奇怪的现象：交叉熵损失不是 emergent 的，而是在逐步下降 Emergent 的阈值可能会越来越小 更干净的数据，更好的训练技巧，更优秀的模型结构都可以是 Emergent阈值变小 未来方向： 继续扩大 model scale，远未达到上限 一些新结构的 scaling 数据的 scaling 理解 prompt 机制 更前沿的 task，用来指导 emergent 理解 emergence 📚论文贡献 优点 第一次正式提出 emergent 实验 做了充分的实验表明该现象在各种数据集上广泛存在 甚至验证了一些“方法”的涌现 提出了一些解释该现象的观点，并提出质疑 改进点 还是不知道为啥 emerge 实验采用各种不同模型，无法得出哪个计算量级对哪种能力有 emerge 参考 # 清华博士带你思考大语言模型LLM的涌现现象（Emergent） 有脑图 Emergent Abilities of Large Language Models （https://arxiv.org/abs/2206.07682）\n再谈ChatGPT等大模型的涌现能力：关于涌现能力的定义、测试方法及分析工作总结 "},{"id":26,"href":"/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptLlama/gptLlama/","title":"LLaMA","section":"decode-only","content":" LLaMA # LLaMA\n"},{"id":27,"href":"/www6vAlgo/docs/RL/Agentic-RL/Search/websailer/","title":"WebSailor","section":"Search","content":" 论文 # 《WebSailor: Navigating Super-human Reasoning for Web Agent》阿里巴巴集团通义实验室\n主要介绍了一种名为WebSailor的新型网页智能体（Web Agent），其在复杂信息寻求任务中展现了超越人类的推理能力。 以下是对文档的深度阅读总结，涵盖其核心问题、方法、实验及贡献。\n一、研究背景与问题定义 # 1.1 信息寻求的挑战 # 互联网时代的信息爆炸超越了人类认知极限（有限记忆、脆弱注意力、无法并行探索）。 专有智能体系统（如OpenAI的Deep Research）已展现出超越人类的性能，但开源智能体仍存在巨大性能差距。 1.2 任务分级 # 论文将信息寻求任务分为三个级别：\nLevel 1：低不确定性任务（如单次搜索即可解决）。 Level 2：高初始不确定性但解决路径清晰（如标准多跳问答）。 Level 3（本文焦点）：高不确定性且解决路径复杂、无预定义路径（如BrowseComp基准中的任务）。 1.3 性能差距根源 # 现有训练范式集中于Level 1–2任务，缺乏对Level 3复杂推理模式的暴露，导致模型无法发展出多步推理能力。\n二、核心方法：WebSailor框架 # 2.1 训练数据合成（SailorFog-QA） # a) 构建复杂知识图 # 通过随机游走从真实网站中提取互联的知识结构，生成具有涌现式非线性结构的图。 b) 生成高不确定性问题 # 采样多样化拓扑的子图（包含新颖的实体与关系组合）。 信息模糊化处理（如将精确日期转为“2010年代初”，名称部分掩码等），强制智能体进行推理而非简单查找。 c) 优势 # 数据基于真实互联网，贴合实际挑战。 子图拓扑多样性自然产生需复杂推理模式（多步演绎、组合与比较分析）的问题。 高度可扩展（子图数量随图规模非线性增长）。 2.2 推理轨迹重建 # a) 问题 # 开源大型推理模型（LRM，如QwQ、DeepSeek-R1）能生成正确轨迹，但其原生推理输出冗长、风格化，直接用于微调会限制智能体的探索策略泛化能力，且长轨迹易超出上下文窗口限制。\nb) 解决方案 # 用LRM生成完整动作-观察序列（丢弃原生冗长思考）。 用另一指令遵循模型（如Qwen-2.5-72B）为每一步动作重建简洁、目标导向的思考（“短链思维”风格），形成高质量监督信号。 2.3 训练流程优化 # a) 冷启动（RFT） # 必要性：RL奖励稀疏（初始近零反馈），且蒸馏依赖低（仅需2k+高质量样本）。 过滤：保留正确轨迹、长度＜32k token、工具调用＞5次（确保复杂性）。 训练目标：增强决策能力（掩码环境观察的损失计算）。 b) 强化学习（DUPO算法） # 挑战：多轮推理与工具使用导致训练缓慢。 创新：复制采样策略优化（训练前过滤全正确样本，训练中复制同一批次内标准差非零的样本），提速2–3倍。 奖励设计：结合格式验证（0.1权重）和答案验证（0.9权重），避免奖励黑客。 三、实验结果 # 3.1 基准测试 # BrowseComp-en/zh：最具挑战性的网页浏览基准，需复杂策略。 GAIA：需多模态与工具使用（仅用文本子集）。 Xbench-DeepSearch：动态深度搜索基准。 3.2 性能对比 # WebSailor（3B/7B/32B/72B）在所有开源模型与智能体方法中领先，且超越部分结合浏览能力的专有LRM（如Grok-3、Doubao）。 WebSailor-72B在BrowseComp-zh上与Doubao持平，虽仍落后于DeepResearch（SOTA），但显著缩小了开源与专有系统的差距。 向下兼容性：在简单任务（如SimpleQA）上也表现优异。 3.3 关键分析 # 数据复杂性：SailorFog-QA的工具调用分布与BrowseComp-en高度相似（长尾、多＞5次调用），而WebDancer数据集中＞50%仅需2次调用。 RL有效性：RL训练显著提升Pass@1性能（尤其BrowseComp），增强样本效率与稳定性。 冷启动必要性：无冷启动的RL模型工具调用数更低，无法掌握长视距推理，性能差距大。 四、局限与未来工作 # 上下文长度限制（32k token）可能制约更复杂问题的解决。 过度思考倾向：对简单问题也进行多步工具调用（但常为交叉验证，非无意义探索）。 训练效率：同步RL框架低效（仅50步），未来将转向异步训练。 五、结论 # WebSailor通过合成高不确定性数据、重建简洁推理轨迹、以及RFT冷启动与DUPO算法，实现了开源智能体在复杂信息寻求任务上的突破性进展，证明了开源模型可达到接近专有系统的性能。未来将继续探索更复杂任务与高效RL训练，以追求更广泛的“超人类”性能。\n附录与案例 # 文档还提供了：\n工具细节（search、visit）； QA构建流程； 训练超参数； 完整轨迹案例（如BrowseComp-en中关于Joey Hess的查询），展示多步推理与工具调用的实际应用。 总结 # 该研究在数据合成、推理重建和训练优化方面均有显著创新，为开源社区提供了可复现的高性能网页智能体方案，推动了对“超人类推理”能力的探索。\n参考 # 本文由元宝生成\nAgent智能体 | 深入解读阿里开源Web Agent新王者：WebSailor\n"},{"id":28,"href":"/www6vAlgo/docs/RL/core/RewardModel/","title":"(原理|实现)PPO-RewardModel","section":"Core","content":"\nPPO-RewardModel # (原理|实现)PPO-RewardModel\n"},{"id":29,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningLoss/","title":"(原理\u0026实战)交叉熵损失","section":"basic","content":"\n交叉熵损失 # (原理\u0026amp;实战)交叉熵损失\n"},{"id":30,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningNorm/","title":"规范化 Norm","section":"网络优化","content":"\nNorm 作用[1] # dnn 的标准组件，稳定和加速训练过程\nBatch Norm[1] # reduce cross batch size mini-batch dimension 一般用于图像，不涉及到padding的问题；\nLayer Norm[1] # reduce cross hidden dim reduce across the feature dimension. 一般用于序列，一个 batch size 内存在 padding；\nRMSNorm: 对 LN 的一种变体，llama 💡 https://spaces.ac.cn/archives/9009 Pre LN: llama Post LN: attention is all you need llama在工程上使用Pre LN\n[pytorch] BN、LN、RMSNorm 及 pre LN vs. post LN 对比，标准化 v *** ​\tnormalization.ipynb\n​\t[pytorch] BN、LN、RMSNorm 及 pre LN vs. post LN 对比，标准化 1xx. Batch Normalization, Layer Normalization and Root Mean Square Layer Normalization: A Comprehensive Guide with Python Implementations\ntodo\n7.5 逐层规范化 百度邱 有代码\nhttps://aistudio.baidu.com/education/lessonvideo/3048901\n"},{"id":31,"href":"/www6vAlgo/docs/DeepLearning/Transformer/ModelGQA/","title":"(原理)GQA","section":"Transformer","content":"\n论文 # GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\nMHA vs. MQA vs. GQA [1] # MHA # 首先是原始的 MHA(Multi-Head Attention)，QKV 三部分有相同数量的头，且一一对应。每次做 Attention，head1 的 QKV 就做好自己运算就可以，输出时各个头加起来就行。\nMQA # 而 MQA 则是，让 Q 仍然保持原来的头数，但 K 和 V 只有一个头，相当于所有的 Q 头共享一组 K 和 V 头，所以叫做 Multi-Query 了。实现改变了会不会影响效果呢？确实会影响但相对它能带来的收益，性能的些微降低是可以接受的。\n能带来多大的收益呢，实验发现一般能提高 30%-40% 的吞吐。\n收益主要就是由降低了 KV cache 带来的。实际上 MQA 运算量和 MHA 是差不多的，可理解为读取一组 KV 头之后，给所有 Q 头用，但因为之前提到的内存和计算的不对称，所以是有利的。\nGQA # 而 GQA 呢，是 MHA 和 MQA 的折衷方案，既不想损失性能太多，又想获得 MQA 带来的推理加速好处。具体思想是，不是所有 Q 头共享一组 KV，而是分组一定头数 Q 共享一组 KV，比如上面图片就是两组 Q 共享一组 KV。\nMQA 和 GQA 形式在推理加速方面，主要是通过两方面来完成：\n降低了从内存中读取的数据量，所以也就减少了计算单元等待时间，提高了计算利用率； KV cache 变小了 head_num 倍，也就是显存中需要保存的 tensor 变小了，空出来空间就可以加大 batch size，从而又能提高利用率。 如果要用 MQA 和 GQA，可以是从头训练的时候就加上，也可以像 GQA 论文里面一样，用已有的开源模型，挑一些头取个 mean 用来初始化 MQA 或 GQA 继续训练一段时间。\nGQA \u0026amp; MQA [2] # 图 4.1 Multi-head attention 拥有 H 个查询、键和值头。Multi-query attention 在所有 查询头之间共享单个键和值头。Grouped-query attention 则在每个查询头组之间共享单 个键和值头，从而在多头和多查询注意力之间进行插值。\nFig. 4.1 Multi-head attention has H query, key, and value heads. Multi-query attention shares single key and value heads across all query heads. Grouped-query attention instead shares single key and value heads for each group of query heads, interpolating between multi-head and multi-query attention.\n参考 # 为什么现在大家都在用 MQA 和 GQA？ ***\nLLM学习系列1：大模型架构要点总结 主流大语言模型的技术原理细节 *** 腾讯 [架构] + 训练 + 微调\n1xx. 理解Attention:从起源到MHA,MQA和GQA *** 1xx. 深度解析Group Query Attention(GQA)为什么能给LLM decoder带来极大推理加速 1xx. 深度学习中的注意力机制：MHA、MQA和GQA\n1xx. 【研1基本功 （真的很简单）Group Query-Attention】大模型训练必备方法——bonus(位置编码讲解) v *** Nomolization[post, pre, sandwich] + Position Encoding[RoPE] + GQA代码 1xx. 一文通透各种注意力：从多头注意力MHA到分组查询注意力GQA、多查询注意力MQA 删除\n手写大模型组件之Group Query Attention，从 MHA，MQA 到 GQA\n"},{"id":32,"href":"/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptLlamaFamily/gptLlamaFamily/","title":"LLaMA 家族","section":"decode-only","content":" LLaMA 家族[1] # 项目 描述 数据集 LLaMa 基座模型 公开可用的数据集(1T token) Stanford Alpaca 结合英文语料通过Self Instruct方式微调LLaMA 7B Self Instruct from davinci-003 API(52K) Vicuna-13B 通过ShareGPT.com的7万条对话数据微调LLaMA(Alpaca基础之上, 多轮对话和长序列, full fine-tune) 用户共享对话(70K sample) BELLE 结合中文语料通过Self Instruct方式微调BLOOMZ-7B或LLaMA Chinese-LLaMA/Chinese-Alpaca 通过中文数据预训练/指令微调LLaMA 姜子牙系列模型Ziya-LLaMA-13B-v1 基于LLaMA-13B的中英文模型 ChatLLaMA(英文版) LLaMA的RLHF版 ColossalChat 通过self-instruct技术指令微调LLaMA且加上RLHF {% asset_img \u0026rsquo;llama2-famaly.jpg\u0026rsquo; %}\n参考 # 家族 # LLaMA的解读与其微调：Alpaca-LoRA/Vicuna/BELLE/中文LLaMA/姜子牙/LLaMA 2 *** 1xx. 我想学大模型，应该从哪个模型开始？LLaMA生态家谱整理和分析 1xx. NLP（九）：LLaMA, Alpaca, ColossalChat 系列模型研究\n1xx. \u0026laquo;千帆增强版 Llama 2-提升大模型对话指令遵循能力\u0026raquo; v\n1xx. 近期大模型动态：LLaMA-2-7B-32K的训练数据组织情况及面向儿童心理健康领域的微调模型推介 llama-2-7b-32k - LLaMA-2的上下文长度为4Ktoken。要将其扩展到32K上下文，该工作分成了三个部分：建模、数据和系统优化。\n实战 # 1xx. 从0到1复现斯坦福羊驼（Stanford Alpaca 7B） GPUs: 8 卡 A800 80GB GPUs\n汉化 # 1xx. 掘力计划 23 期-Linly-Chinese-LLaMA2 中文开源大模型方案分享 v\n"},{"id":33,"href":"/www6vAlgo/docs/LLM/core/gptHallucination/","title":"(原理)幻觉问题","section":"core","content":" 幻觉[3] # Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge. 大型语言模型中的幻觉通常是指模型生成不忠实、捏造、不一致或无意义的内容。作为一个术语，幻觉在某种程度上被推广到模型犯错的情况。在这里，我想将幻觉问题缩小到模型输出是捏造的， 而不是基于所提供的上下文或世界知识的情况。\nThere are two types of hallucination: 幻觉有两种类型：\nIn-context hallucination: The model output should be consistent with the source content in context. 上下文幻觉：模型输出应与上下文中的源内容一致。 Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so. 外在幻觉：模型输出应以训练前数据集为基础。但是，考虑到预训练数据集的大小，检索和识别每代冲突的成本太高。如果我们将预训练数据语料库视为世界知识的代理，我们基本上会尝试确保模型输出是真实的，并且可以通过外部世界知识进行验证。同样重要的是，当模型不知道某个事实时，它应该这么说。 Anti-Hallucination Methods[3] # RAG → Edits and Attribution # Self-RAG (“Self-reflective retrieval-augmented generation”; Asai et al. 2024) trains a LM end-to-end to learn to reflect on its own generation by outputting both task output and intermittent special reflection tokens. They created a supervision dataset for a critic model and a generator model by prompting GPT-4 and then distilled that into an in-house model to reduce inference cost. Self-RAG（“自反射检索增强一代”;Asai 等人，2024 年）通过输出任务输出和间歇性特殊反射令牌 ，端到端训练 LM 以学习反射自己的生成。他们通过提示 GPT-4 为 critic 模型和生成器模型创建了一个监督数据集，然后将其提炼成内部模型以降低推理成本。\nChain of Actions # Without grounding by external retrieved knowledge, we can design a process for using the model itself to do verification and revision to reduce hallucination. 在没有外部检索知识的基础的情况下，我们可以设计一个流程，使用模型本身进行验证和修改，以减少幻觉。\nDhuliawala et al. (2023) proposed a method named Chain-of-Verification (CoVe) based on a chain of actions to plan and execute verification. [10] Dhuliawala 等人（2023 年） 提出了一种名为验证链 （CoVe） 的方法，该方法基于一系列行动来计划和执行验证。\nRECITE (“Recitation-augmented generation”; Sun et al. 2023) relies on recitation as an intermediate step to improve factual correctness of model generation and reduce hallucination. The motivation is to utilize Transformer memory as an information retrieval mechanism. Within RECITE’s recite-and-answer scheme, the LLM is asked to first recite relevant information and then generate the output. Precisely, we can use few-shot in-context prompting to teach the model to generate recitation and then generate answers conditioned on recitation. Further it can be combined with self-consistency ensemble consuming multiple samples and extended to support multi-hop QA. RECITE （“朗诵增强生成”;Sun 等人，2023 年）依靠背诵作为中间步骤来提高模型生成的事实正确性并减少幻觉。动机是利用 Transformer 内存作为信息检索机制。在 RECITE 的背诵和回答方案中，要求 LLM 首先背诵相关信息，然后生成输出。准确地说，我们可以使用小镜头上下文提示来教模型生成背诵，然后生成以背诵为条件的答案。此外，它可以与使用多个样本的自一致性集成相结合，并扩展以支持多跳 QA。\ntodo\nSurvey # 论文 # 论文地址 A Survey of Hallucination in Large Foundation Models Paper 1xx. 大模型前沿热点最新综述：大模型微调遗忘、Agent智能体、幻觉及RAG检索增强模型推介 大模型微调遗忘 幻觉\n论文 # 论文地址 Siren\u0026rsquo;s Song in the AI Ocean: A Survey on Hallucination in Large Language Models Paper 1xx. 人工智能海洋中的塞壬之歌：大型语言模型LLM中的幻觉研究综述（一） 1xx. 大型语言模型的幻觉研究｜减轻及避免大模型LLM幻觉（二） 1xx. 值得一读的大模型生成幻觉研究综述：大模型幻觉的起因、评估以及减轻策略总结 幻觉 vs 事实性[1] # 幻觉主要是指LLM生成毫无根据或毫无根据的内容，幻觉可以理解为模型倾向于\u0026quot;生成与某些来源相关的无意义或不真实的内容\u0026quot;。这与事实性问题不同，后者强调模型学习、获取和利用事实性知识的能力。\n举例说明两者的区别：\n如果一个LLM在被要求创作\u0026quot;一个关于兔子和狼交朋友的童话故事\u0026quot;时，创作出了一个关于\u0026quot;兔子和狗交朋友\u0026quot;的故事，那么它就表现出了幻觉。不过，这并不一定是事实性错误。 如果生成的内容包含准确的信息，但与提示的具体内容有出入，那就是幻觉，而不是事实性问题。 例如，如果LLM的输出包含了比提示指定更多的细节或不同的元素，但事实仍然正确，这就是幻觉。\n相反，如果LLM避免给出直接答案，而是说\u0026quot;我不知道\u0026quot;，或者给出了一个准确的答案，但遗漏了一些正确的细节，那么这就是事实性问题，而不是幻觉。\n此外，值得注意的是，幻觉有时会产生一些内容，虽然与原始输入内容有偏差，但在事实方面仍然是准确的。\n解决方案[2] # Prompt 工程 *\nFew-shot 外部知识 *\nRAG 后处理 *\n实事检查 * 人工检查 * 提升数据质量\nPretraining的数据质量 SFT的数据质量 模型能力提升 *\n微调 参考 # 再看大模型事实性的界定、错误的起因、评估及前沿缓解方案：Survey on Factuality in LLMS\n降低大模型幻觉的5种方案 v\n减少大模型幻觉，你必须要掌握的 6 个方法！ v\nExtrinsic Hallucinations in LLMs ***\n【译】LLM中的外部幻觉\nWork # 再看大模型幻觉问题如何缓解 ：Chain-of-Verification-一种基于链式验证思想的自我修正工作解读 1xx. 也看缓解大模型幻觉的多阶段RAG框架：加入混合检索、过程理由生成与验证的方案 survey # 1xx. 大模型的幻觉问题调研: LLM Hallucination Survey\n1xx. 网络安全领域微调模型SecGPT：兼看大模型幻觉的度量方式、评估benchmark及RAG增强不同方式 大模型幻觉综述\n1xx. LLM之幻觉（一）：大语言模型幻觉解决方案综述\n"},{"id":34,"href":"/www6vAlgo/docs/LLM/Foundation-Models/decode-only/gptLlama3-1/gptLlama3-1/","title":"Llama3.1","section":"decode-only","content":" Llama3.1 # Llama3.1\n"},{"id":35,"href":"/www6vAlgo/docs/RL/core/DPO/","title":"(原理|实现)DPO","section":"Core","content":"\nDPO # (原理|实现)DPO\n"},{"id":36,"href":"/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningGradOpt/","title":"(原理)梯度优化","section":"网络优化","content":"\n梯度优化 # Gradient accumulation # Gradient checkpointing [10] # 显存占用优化算法\nmemory usage 与 computation time 之间的 tradeoff ； gradient checkpointing\nIn deep neural networks, backpropagation requires storing intermediate activations for computing gradients during the backward pass.\n但是当层数变多时，存储所有的中间层的激活值（intermediate activations）非常地占用显存；\ngradient checkpointing\n选择性地重新计算（recompute）一部分的 intermediate activations 在反向传播过程中来缓解显存的压力；\nGradient Clipping (梯度裁剪) # 目的[21] # 梯度爆炸问题的常见应对方式为“梯度裁剪”，也就是通过“clip”方式来防止迭代中梯度值过大。\n两种常见形式[20] # 梯度范数裁剪（Gradient Norm Clipping）: 这种方法涉及计算所有参数梯度的范数（例如L2范数），如果这个范数超过了设定的阈值，就将梯度缩放到这个阈值以内。在PyTorch中，这可以通过 torch.nn.utils.clip_grad_norm_ 函数实现。 梯度值裁剪（Gradient Value Clipping）: 这种方法对每个参数的梯度值进行独立裁剪，确保它们不会超过一个设定的最大值或最小值。在PyTorch中，这可以通过 torch.nn.utils.clip_grad_value_ 函数实现。 参考 # overview # Performance and Scalability: How To Fit a Bigger Model and Train It Faster ***\ngradient accumulation # 1xx. [LLMs 实践] 11 gradient accumulation 显存优化 trick v\n​\tgradient_accumulation.ipynb\n​\t[ LLMs 实践] 11 gradient accumulation 显存优化 trick 1xx. Pytorch入门（7）—— 梯度累加（Gradient Accumulation）\n1xx. 聊聊梯度累加(Gradient Accumulation)\n1xx. What is Gradient Accumulation in Deep Learning?\n1xx. Performing gradient accumulation with Accelerate\n​\t使用Accelerate进行梯度累积\ngradient checkpointing # [LLMs 实践] 13 gradient checkpointing 显存优化 trick v ​\tgradient_checkpointing.ipynb\n​\t[LLMs 实践] 13 gradient checkpointing 显存优化 trick ​\tFitting larger networks into memory. *** 看动图\n​\tBackprop and systolic arrays.\nGradient Clipping # 梯度裁剪（Gradient Clipping） ​\thttps://github.com/pytorch/pytorch/blob/main/torch/nn/utils/clip_grad.py\n深度炼丹之梯度裁剪 1xx. 【深度学习】第6.2节 梯度裁剪\n1xx. 【Pytorch】梯度裁剪——torch.nn.utils.clip_grad_norm_的原理及计算过程\n1xx. PyTorch使用Tricks：梯度裁剪-防止梯度爆炸或梯度消失 ！！\n"},{"id":37,"href":"/www6vAlgo/docs/DeepLearning/basic/DeeplearningOverfitting/","title":"(原理)过拟合","section":"basic","content":"\n(原理)过拟合 # (原理)过拟合\n"},{"id":38,"href":"/www6vAlgo/docs/DeepLearning/Transformer/TransformerCode/","title":"(实战)Transformer","section":"Transformer","content":"\n参考 # 1xx. transformer.ipynb git Transformer代码实现\n1xx. Transformer transformer.py git\n1xx. [译] Transformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019） V, github Transformers from scratch\n1xx. 从零实现Transformer的简易版与强大版：从300多行到3000多行\n1xx. Transformer源码详解（Pytorch版本）\n"},{"id":39,"href":"/www6vAlgo/docs/LLM/core/gptImpossibleTriangle/","title":"(原理)不可能三角","section":"core","content":" 不可能三角[1] # 不可能三角 # 预训练模型之所以是划时代的进展，是它具备了中等尺寸（一张卡即可精调）和全任务SOTA的精调效果 而最近两年预训练模型都在往大尺寸发展，也就是具备了少样本效果，但他们的少样本效果依旧比不过中等模型的精调 弥补方法 # 优化size 对于减少模型尺寸，一条典型的故事线就是蒸馏。但其中仍存在两个问题：一是学生模型很难达到原始模型的效果，二是原始的大尺寸模型的推理效率太低 优化few-shot 对于提升少样本表现，数据增强是一个好办法，比如用无监督数据做自监督训练、或者基于其他模型生成一些伪样本，但这类方法依旧受限于现有标注样本的多样性，泛化性能提升有限 fine-tuning 对于提升精调表现和效率（其实也偏少样本），最近一个比较火的故事是prompt，但这种方式对prompt的设计非常敏感，同时效果也很难超过目前的有监督SOTA 其他 不可能三角 # 分布式系统 # CAP理论 C 一致性 A 可用性 P 分区 分布式存储 # RUM猜想 Read-overhead Update-overhead Memory-overhead 范式 # pretrain, finetune 范式[3] # 第三阶段范式\npretrain, prompt, predict 范式[3] # 第四阶段范式\n总结 # 根据不可能三角形， pretrain, finetune 范式[3] 向pretrain, prompt, predict 范式[3]的迁移是受大模型大小的影响\n参考 # 不可能三角 # 预训练模型的下一步？突破Impossible Triangle Impossible Triangle: What’s Next for Pre-trained Language Models? 微软朱晨光：预训练模型下一步怎么走？突破PLM的「不可能三角」 Go to Page self "},{"id":40,"href":"/www6vAlgo/docs/RL/framework/veRLConfig/","title":"(原理\u0026实战)veRL Config","section":"Framework","content":"\nveRL Config # (原理\u0026amp;实战)veRL Config\n"},{"id":41,"href":"/www6vAlgo/docs/DeepLearning/Transformer/Embedding/Embedding/","title":"(原理)Embedding","section":"Embedding","content":" example [1] # 降维: t-SNE K-Means 聚类 文本搜索 相似度搜索 Embedding 价值 [2] # 降维 将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。 捕捉语义信息 Embedding不仅仅是降维，更重要的是，它能够捕捉到数据的语义信息。 泛化能力 由于Embedding能够捕捉到数据的一些内在规律，因此对于这些未见过的数据，Embedding仍然能够给出合理的表示 应用 [2] # 语义表示和语义相似度 词语关系和类比推理 上下文理解 文本分类和情感分析 机器翻译和生成模型 天梯榜 # mteb/leaderboard\nexample[3] # m3e模型 bge模型 参考 # embedding git\n《AI 大模型应用开发实战营》 03-大模型开发基础：Embedding\n一文通透Text Embedding模型：从text2vec、openai-ada-002到m3e、bge\n1xx. 如何选取RAG中的embedding模型 v ***\nhuggingface embedding模型排行榜\nSentence Bert Demo Repo git\n1xx. 引入任务Instruction指令的句子向量化方案：Instructor的实现思路及训练数据集构造方案\nRepo git\n1xx. 也看利用大模型进行RAG文本嵌入训练数据生成：兼看面向NLP任务的开源指令微调数据集 《Improving Text Embeddings with Large Language Models》\n1xx. 如何提高LLMs的文本表征(Text Embedding)能力?\n《Improving Text Embeddings with Large Language Models》\n1xx. 文本转向量教程s2——认识文本转向量方法（sbert本质和推理加速） V\n"},{"id":42,"href":"/www6vAlgo/docs/LLM/core/gptEval/","title":"测评 *","section":"core","content":" 基础指标[1] # 分类任务\nAccuracy Precision Recall F1-score AUC-ROC 曲线 生成任务\nBLEU ROUGE METEOR 人工评估 回归任务\nMSE MAE R2 PPL 困惑度\n测评集 # MMLU C-EVAL Framework # OpenCompass 参考 # AI大模型面试题：5.模型微调怎么评估效果 1xx. 一些讨论：三张关于大模型微调方案的脑图及几点llama2等行业落地的问题思考 1xx. https://github.com/CLUEbenchmark/SuperCLUE-Llama2-Chinese\n1xx. 如何让自己的大模型榜单评分更高：也谈榜单评测评分的一些常见方案和典型案例 CEval # 1xx. 大模型落地的一些前沿观点：兼看知识图谱增强大模型问答的几个方案及CEVAL榜单评测启发 二、CEVAL榜单评测中能够得到一些启示\n1. C-Eval 数据集评测简明教程\n1xx. 大模型B端落地“牛刀杀鸡”的奇怪感觉：兼看CEVAl通用评测到金融、医疗两大垂域评测的转变 CEVAl\nFramework # https://opencompass.org.cn/home\n"},{"id":43,"href":"/www6vAlgo/docs/DeepLearning/Transformer/TrainTokenizer/","title":"Tokenizer","section":"Transformer","content":"\ntokenizer 分词 # 单词分词法 单字分词法 子词分词法 BPE [GPT系列], WordPiece 参考 # 1xx. 大模型词表扩充必备工具SentencePiece 1xx. NLP（二）：浅谈分词 1xx. https://www.bilibili.com/video/BV1vN411p7t2/ 1xx. 开源大模型如何更好地适应中文场景：LLAMA扩充词表、BLOOM裁剪词表基本原理与开源实现\n"},{"id":44,"href":"/www6vAlgo/docs/RL/Agentic-RL/Search/Kimi-Researcher/","title":"Kimi-Researcher","section":"Search","content":" 论文 # Kimi-Researcher - End-to-End RL Training for Emerging Agentic Capabilities\n参考 # 强化学习智能体新模板：深入解析Kimi‑Researcher\n1xx. Kimi-Researcher：端到端强化学习驱动的自主智能体\n1xx. up: 有个视频 "},{"id":45,"href":"/www6vAlgo/docs/RL/core/unified/","title":"unified paradigm","section":"Core","content":" RL unified paradigm # RL unified paradigm # DPO # PPO # GRPO # 参考 # DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n"},{"id":46,"href":"/www6vAlgo/docs/DeepLearning/basic/Pytorch/","title":"(实战)PyTorch","section":"basic","content":"\nPyTorch 实战 # (实战)PyTorch\n"},{"id":47,"href":"/www6vAlgo/docs/LLM/Foundation-Models/gptLeaderBoard/gptLeaderBoard/","title":"大模型 排行榜","section":"Foundation Models","content":" 大模型 # 排行榜 # HuggingFaceH 大模型排行榜\nLLM Collection\n中国排行榜 # 中国大模型 通用 39 金融 25 司法 8 法律 6 医学 13 医疗 24 教育 13 科研 17 工业 23 政务 12 运维 7 "},{"id":48,"href":"/www6vAlgo/docs/LLM/Foundation-Models/encode-only/gptBERT/gptBERT/","title":"(原理)BERT","section":"encode-only","content":"\nBERT # (原理)BERT\n"}]