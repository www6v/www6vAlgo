<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deepresearch on LLM 算法</title>
    <link>https://www6v.github.io/www6vAlgo/categories/Deepresearch/</link>
    <description>Recent content in Deepresearch on LLM 算法</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 26 Mar 2024 12:13:50 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAlgo/categories/Deepresearch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Search-R1</title>
      <link>https://www6v.github.io/www6vAlgo/docs/RL/Agentic-RL/Search/Search-R1/</link>
      <pubDate>Tue, 26 Mar 2024 12:13:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/RL/Agentic-RL/Search/Search-R1/</guid>
      <description>&lt;h1 id=&#34;论文&#34;&gt;&#xA;  论文&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2503.09516v1&#34;&gt;Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/PeterGriffinJin/Search-R1&#34;&gt;https://github.com/PeterGriffinJin/Search-R1&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;methods&#34;&gt;&#xA;  &lt;strong&gt;Methods&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#methods&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;详细方法和步骤:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/s-r1.png&#34; alt=&#34;s-r1.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;将搜索引擎建模为环境的一部分：&lt;/strong&gt; SEARCH-R1将搜索引起作为环境的一部分， 让模型与环境交互，从而得到 reward。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/s-r1-1.png&#34; alt=&#34;s-r1-1.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;支持多轮检索和推理：&lt;/strong&gt; SEARCH-R1通过特定的标签（&lt;code&gt;&amp;lt;search&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/search&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;information&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/information&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;answer&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/answer&amp;gt;&lt;/code&gt;）来支持多轮检索和推理。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/l6gmzlwi.png&#34; alt=&#34;l6gmzlwi.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;优化算法兼容性：&lt;/strong&gt; SEARCH-R1 与各种 RL 算法兼容，包括 PPO 和 GRPO。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;简单结果奖励函数：&lt;/strong&gt; 避免复杂的基于过程的奖励, 采用简单的基于结果的奖励函数 &lt;strong&gt;（字符串匹配作为reward!!!）。&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/g0ew6gx4.png&#34; alt=&#34;g0ew6gx4.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;总结&#34;&gt;&#xA;  &lt;strong&gt;总结&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;结论1: SEARCH-R1 显著提升了LLM在需要实时外部知识的复杂推理任务中的能力。&lt;/strong&gt; 通过强化学习，LLM可以自主生成查询并有效利用检索到的信息，优于传统的RAG方法。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;结论2: SEARCH-R1在不同LLM架构和训练方法上具有广泛的适用性。&lt;/strong&gt; 实验结果表明，无论使用基础模型还是指令调整模型，SEARCH-R1都能带来显著的性能提升，且对不同的RL算法（如PPO和GRPO）具有兼容性。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;结论3: SEARCH-R1有很强的实用价值。&lt;/strong&gt; SEARCH-R1能够显著提高LLM在需要实时外部知识的复杂推理任务中的能力。 可以用于智能问答，智能助手等领域。&lt;/p&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/rPza0-KB4Rpc_vXyYa8xSw&#34;&gt;Search-R1：让大模型学会“检索+推理”的新范式&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/30784344002&#34;&gt;【论文解读】Search-R1：强化学习如何教会 LLM 自主搜索？&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
