<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AIGC on LLM ç®—æ³•</title>
    <link>https://www6v.github.io/www6vAlgo/categories/AIGC/</link>
    <description>Recent content in AIGC on LLM ç®—æ³•</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 03 Aug 2024 14:22:39 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vAlgo/categories/AIGC/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Search-R1</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Agent/Deep-Research/Search/Search-R1/</link>
      <pubDate>Tue, 26 Mar 2024 12:13:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Agent/Deep-Research/Search/Search-R1/</guid>
      <description>&lt;h1 id=&#34;è®ºæ–‡&#34;&gt;&#xA;  è®ºæ–‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2503.09516v1&#34;&gt;Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/PeterGriffinJin/Search-R1&#34;&gt;https://github.com/PeterGriffinJin/Search-R1&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;methods&#34;&gt;&#xA;  &lt;strong&gt;Methods&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#methods&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;è¯¦ç»†æ–¹æ³•å’Œæ­¥éª¤:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/s-r1.png&#34; alt=&#34;s-r1.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;å°†æœç´¢å¼•æ“å»ºæ¨¡ä¸ºç¯å¢ƒçš„ä¸€éƒ¨åˆ†ï¼š&lt;/strong&gt;Â SEARCH-R1å°†æœç´¢å¼•èµ·ä½œä¸ºç¯å¢ƒçš„ä¸€éƒ¨åˆ†ï¼Œ è®©æ¨¡å‹ä¸ç¯å¢ƒäº¤äº’ï¼Œä»è€Œå¾—åˆ° rewardã€‚&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/s-r1-1.png&#34; alt=&#34;s-r1-1.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;æ”¯æŒå¤šè½®æ£€ç´¢å’Œæ¨ç†ï¼š&lt;/strong&gt;Â SEARCH-R1é€šè¿‡ç‰¹å®šçš„æ ‡ç­¾ï¼ˆ&lt;code&gt;&amp;lt;search&amp;gt;&lt;/code&gt;,Â &lt;code&gt;&amp;lt;/search&amp;gt;&lt;/code&gt;,Â &lt;code&gt;&amp;lt;information&amp;gt;&lt;/code&gt;,Â &lt;code&gt;&amp;lt;/information&amp;gt;&lt;/code&gt;,Â &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;,Â &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt;,Â &lt;code&gt;&amp;lt;answer&amp;gt;&lt;/code&gt;,Â &lt;code&gt;&amp;lt;/answer&amp;gt;&lt;/code&gt;ï¼‰æ¥æ”¯æŒå¤šè½®æ£€ç´¢å’Œæ¨ç†ã€‚&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/l6gmzlwi.png&#34; alt=&#34;l6gmzlwi.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ä¼˜åŒ–ç®—æ³•å…¼å®¹æ€§ï¼š&lt;/strong&gt;Â SEARCH-R1 ä¸å„ç§ RL ç®—æ³•å…¼å®¹ï¼ŒåŒ…æ‹¬ PPO å’Œ GRPOã€‚&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ç®€å•ç»“æœå¥–åŠ±å‡½æ•°ï¼š&lt;/strong&gt;Â é¿å…å¤æ‚çš„åŸºäºè¿‡ç¨‹çš„å¥–åŠ±, é‡‡ç”¨ç®€å•çš„åŸºäºç»“æœçš„å¥–åŠ±å‡½æ•°Â &lt;strong&gt;ï¼ˆå­—ç¬¦ä¸²åŒ¹é…ä½œä¸ºreward!!!ï¼‰ã€‚&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/g0ew6gx4.png&#34; alt=&#34;g0ew6gx4.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;æ€»ç»“&#34;&gt;&#xA;  &lt;strong&gt;æ€»ç»“&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;ç»“è®º1: SEARCH-R1 æ˜¾è‘—æå‡äº†LLMåœ¨éœ€è¦å®æ—¶å¤–éƒ¨çŸ¥è¯†çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚&lt;/strong&gt;Â é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ŒLLMå¯ä»¥è‡ªä¸»ç”ŸæˆæŸ¥è¯¢å¹¶æœ‰æ•ˆåˆ©ç”¨æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œä¼˜äºä¼ ç»Ÿçš„RAGæ–¹æ³•ã€‚&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;ç»“è®º2: SEARCH-R1åœ¨ä¸åŒLLMæ¶æ„å’Œè®­ç»ƒæ–¹æ³•ä¸Šå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚&lt;/strong&gt;Â å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºä½¿ç”¨åŸºç¡€æ¨¡å‹è¿˜æ˜¯æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼ŒSEARCH-R1éƒ½èƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”å¯¹ä¸åŒçš„RLç®—æ³•ï¼ˆå¦‚PPOå’ŒGRPOï¼‰å…·æœ‰å…¼å®¹æ€§ã€‚&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;ç»“è®º3: SEARCH-R1æœ‰å¾ˆå¼ºçš„å®ç”¨ä»·å€¼ã€‚&lt;/strong&gt;Â SEARCH-R1èƒ½å¤Ÿæ˜¾è‘—æé«˜LLMåœ¨éœ€è¦å®æ—¶å¤–éƒ¨çŸ¥è¯†çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ å¯ä»¥ç”¨äºæ™ºèƒ½é—®ç­”ï¼Œæ™ºèƒ½åŠ©æ‰‹ç­‰é¢†åŸŸã€‚&lt;/p&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/rPza0-KB4Rpc_vXyYa8xSw&#34;&gt;Search-R1ï¼šè®©å¤§æ¨¡å‹å­¦ä¼šâ€œæ£€ç´¢+æ¨ç†â€çš„æ–°èŒƒå¼&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/30784344002&#34;&gt;ã€è®ºæ–‡è§£è¯»ã€‘Search-R1ï¼šå¼ºåŒ–å­¦ä¹ å¦‚ä½•æ•™ä¼š LLM è‡ªä¸»æœç´¢ï¼Ÿ&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(å®æˆ˜)GRPO</title>
      <link>https://www6v.github.io/www6vAlgo/docs/RL/core/GRPO-family/GRPODeepseek/</link>
      <pubDate>Wed, 28 Feb 2024 01:29:48 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/RL/core/GRPO-family/GRPODeepseek/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;grpo&#34;&gt;&#xA;  GRPO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#grpo&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/GRPO-1a7bfe211084808086d9df62f186af6d?pvs=4&#34;&gt;(å®æˆ˜)GRPO&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)å­¦ä¹ ç‡</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningLearningRate/</link>
      <pubDate>Mon, 08 Jan 2024 19:28:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningLearningRate/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;å­¦ä¹ ç‡&#34;&gt;&#xA;  å­¦ä¹ ç‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ad%a6%e4%b9%a0%e7%8e%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/172bfe211084806f8de8cdd5a9b249ad?pvs=4&#34;&gt;(åŸç†)å­¦ä¹ ç‡&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†&amp;å®æˆ˜)æƒé‡è¡°å‡</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/WeightDecay/</link>
      <pubDate>Mon, 08 Jan 2024 19:13:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/WeightDecay/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;æƒé‡è¡°å‡-weightdecay&#34;&gt;&#xA;  æƒé‡è¡°å‡ WeightDecay&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9d%83%e9%87%8d%e8%a1%b0%e5%87%8f-weightdecay&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/16fbfe21108480ea8682d89d64f5b00c?pvs=4&#34;&gt;(åŸç†&amp;amp;å®æˆ˜)æƒé‡è¡°å‡&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>æœºå™¨å­¦ä¹ -æ•°æ®</title>
      <link>https://www6v.github.io/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLData/</link>
      <pubDate>Tue, 21 Nov 2023 22:44:57 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLData/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;æœºå™¨å­¦ä¹ -æ•°æ®&#34;&gt;&#xA;  æœºå™¨å­¦ä¹ -æ•°æ®&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e6%95%b0%e6%8d%ae&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/ML-216bfe2110848076a892e11e7144828c?source=copy_link&#34;&gt;æœºå™¨å­¦ä¹ -æ•°æ®&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)Transformer</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/Transformer/</link>
      <pubDate>Wed, 30 Nov 2022 16:44:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/Transformer/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;åŸç†transformer&#34;&gt;&#xA;  (åŸç†)Transformer&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8e%9f%e7%90%86transformer&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Transformer-b1b9836f9c244db3acda7869f64ff860?pvs=4&#34;&gt;(åŸç†)Transformer&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep Learning</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/basic/DeepLearning/</link>
      <pubDate>Sat, 11 Jun 2022 15:57:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/basic/DeepLearning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;deep-learning&#34;&gt;&#xA;  Deep Learning&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#deep-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Deep-Learning-10dbfe21108480b9affbf52a3b5bb13e?pvs=4&#34;&gt;Deep Learning&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)PPO</title>
      <link>https://www6v.github.io/www6vAlgo/docs/RL/core/PPO-family/PPO/</link>
      <pubDate>Sat, 01 Jun 2024 22:12:32 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/RL/core/PPO-family/PPO/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;ppo&#34;&gt;&#xA;  PPO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ppo&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/PPO-204bfe21108480b2aba0ce27d7d5e761?pvs=4&#34;&gt;(åŸç†)PPO&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)Batchsize</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningBatchsize/</link>
      <pubDate>Wed, 17 Jan 2024 18:45:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningBatchsize/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;æœ€ä½³å®è·µ&#34;&gt;&#xA;  æœ€ä½³å®è·µ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%80%e4%bd%b3%e5%ae%9e%e8%b7%b5&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;batchsize&#34;&gt;&#xA;  batchsize&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#batchsize&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;batchsize  ä¸‹é™ [1]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;åˆ«å¤ªå°çš„é™åˆ¶åœ¨äºï¼Œ&lt;strong&gt;batch sizeå¤ªå°ï¼Œä¼šæ¥ä¸åŠæ”¶æ•›ã€‚&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;æ‰€ä»¥åœ¨å¸¸è§çš„settingï¼ˆï½100 epochsï¼‰ï¼Œbatch sizeä¸€èˆ¬ä¸ä¼šä½äº16ã€‚&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;batchsize ä¸Šé™   [1]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;batch sizeåˆ«å¤ªå¤§çš„é™åˆ¶åœ¨äºä¸¤ä¸ªç‚¹ï¼Œ&lt;/p&gt;&#xA;&lt;p&gt;1ï¼‰batch sizeå¤ªå¤§ï¼Œmemoryå®¹æ˜“ä¸å¤Ÿç”¨ã€‚è¿™ä¸ªå¾ˆæ˜¾ç„¶ï¼Œå°±ä¸å¤šè¯´äº†ã€‚&lt;/p&gt;&#xA;&lt;p&gt;2ï¼‰&lt;strong&gt;batch sizeå¤ªå¤§ï¼Œæ·±åº¦å­¦ä¹ çš„ä¼˜åŒ–ï¼ˆtraining lossé™ä¸ä¸‹å»ï¼‰å’Œæ³›åŒ–ï¼ˆgeneralization gapå¾ˆå¤§ï¼‰éƒ½ä¼šå‡ºé—®é¢˜ã€‚&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;learning-rate--batch-size&#34;&gt;&#xA;  &lt;strong&gt;learning rate &amp;amp; batch size&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#learning-rate--batch-size&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;æ€»ä¹‹ï¼Œå¯ä»¥è¯æ˜ï¼Œ&lt;strong&gt;learning rate/batch sizeçš„æ¯”å€¼å¯¹æ·±åº¦å­¦ä¹ æ˜¯æœ‰æŒ‡æ•°çº§çš„å½±å“&lt;/strong&gt;[3]ï¼Œæ‰€ä»¥éå¸¸é‡è¦ï¼Œæ²¡äº‹åˆ«çè°ƒã€‚[1]&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆå¤§çš„batch_sizeå¾€å¾€å»ºè®®å¯ä»¥ç›¸åº”å–å¤§ç‚¹&lt;a href=&#34;https://zhida.zhihu.com/search?content_id=462989051&amp;amp;content_type=Answer&amp;amp;match_order=1&amp;amp;q=learning_rate&amp;amp;zhida_source=entity&#34;&gt;learning_rate&lt;/a&gt;, å› ä¸ºæ¢¯åº¦éœ‡è¡å°ï¼Œå¤§&lt;/strong&gt;learning_rate&lt;strong&gt;å¯ä»¥åŠ é€Ÿæ”¶æ•›è¿‡ç¨‹ï¼Œä¹Ÿå¯ä»¥é˜²æ­¢é™·å…¥åˆ°å±€éƒ¨æœ€å°å€¼ï¼Œè€Œå°batch_sizeç”¨å°learning_rateè¿­ä»£ï¼Œé˜²æ­¢é”™è¿‡æœ€ä¼˜ç‚¹ï¼Œä¸€ç›´ä¸Šä¸‹éœ‡è¡æ²¡æ³•æ”¶æ•›ï¼ˆè¿™ä¹Ÿæ˜¯ä¸€ä¸ªå°trickï¼‰&lt;/strong&gt;ã€‚[2]&lt;/p&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/61607442/answer/1875700191&#34;&gt;æ€ä¹ˆé€‰å–è®­ç»ƒç¥ç»ç½‘ç»œæ—¶çš„Batch size?&lt;/a&gt;  Summer Clover&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/456600260/answer/2380983385&#34;&gt;è®­ç»ƒç¥ç»ç½‘ç»œæ—¶batchsizeæ‰©å¤§ä¸€å€çš„åŒæ—¶éœ€è¦å¢åŠ epochæ•°é‡å—?&lt;/a&gt; æ–°ä¸€&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.notion.so/7-1-174bfe21108480a7a702e4ebed99f68f?pvs=21&#34;&gt;7.1 æ‰¹å¤§å°è°ƒæ•´å®éªŒ&lt;/a&gt; ç™¾åº¦é‚±&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://aistudio.baidu.com/education/lessonvideo/3048883&#34;&gt;7.1 æ‰¹å¤§å°è°ƒæ•´å®éªŒ&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/148267858&#34;&gt;è®¾ç½®BatchSize&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/32673260/answer/3356342576&#34;&gt;æ·±åº¦å­¦ä¹ ä¸­çš„batchçš„å¤§å°å¯¹å­¦ä¹ æ•ˆæœæœ‰ä½•å½±å“ï¼Ÿ&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>(åŸç†&amp;å®æˆ˜)å‰å‘/åå‘ä¼ æ’­</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/basic/DeeplearningForwardBackward/</link>
      <pubDate>Tue, 09 Jan 2024 12:22:12 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/basic/DeeplearningForwardBackward/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;å‰å‘åå‘ä¼ æ’­&#34;&gt;&#xA;  å‰å‘/åå‘ä¼ æ’­&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%89%8d%e5%90%91%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/164bfe211084800d80dbe868719ce79a?pvs=4&#34;&gt;(åŸç†&amp;amp;å®æˆ˜)å‰å‘/åå‘ä¼ æ’­&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†&amp;å®æˆ˜)Dropout</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/Dropout/</link>
      <pubDate>Mon, 08 Jan 2024 19:19:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E6%AD%A3%E5%88%99%E5%8C%96/Dropout/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;dropout&#34;&gt;&#xA;  Dropout&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dropout&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Dropout-16fbfe21108480259446f7c7feb39aca?pvs=4&#34;&gt;(åŸç†&amp;amp;å®æˆ˜)Dropout&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>æœºå™¨å­¦ä¹ -æ¨¡å‹</title>
      <link>https://www6v.github.io/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLModel/</link>
      <pubDate>Tue, 21 Nov 2023 22:44:57 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MLModel/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;æœºå™¨å­¦ä¹ -æ¨¡å‹&#34;&gt;&#xA;  æœºå™¨å­¦ä¹ -æ¨¡å‹&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0-%e6%a8%a1%e5%9e%8b&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/216bfe21108480868a4cf3a485dd48e6?source=copy_link&#34;&gt;æœºå™¨å­¦ä¹ -æ¨¡å‹&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)Self-Attention</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/SelfAttention/</link>
      <pubDate>Sun, 19 Nov 2023 14:19:46 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/SelfAttention/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;self-attention&#34;&gt;&#xA;  Self-Attention&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#self-attention&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Self-Attention-142bfe21108480f6a0c0cdbf9262a7e3?pvs=4&#34;&gt;(åŸç†)Self-Attention&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†|å®ç°)PPO-RewardModel</title>
      <link>https://www6v.github.io/www6vAlgo/docs/RL/core/PPO-family/RewardModel/</link>
      <pubDate>Sat, 01 Jun 2024 22:12:51 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/RL/core/PPO-family/RewardModel/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;ppo-rewardmodel&#34;&gt;&#xA;  PPO-RewardModel&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ppo-rewardmodel&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/PPO-RM-205bfe21108480a39b78d82696d1ede4?pvs=4&#34;&gt;(åŸç†|å®ç°)PPO-RewardModel&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†&amp;å®æˆ˜)äº¤å‰ç†µæŸå¤±</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/basic/DeeplearningLoss/</link>
      <pubDate>Fri, 12 Jan 2024 19:06:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/basic/DeeplearningLoss/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;äº¤å‰ç†µæŸå¤±&#34;&gt;&#xA;  äº¤å‰ç†µæŸå¤±&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%ba%a4%e5%8f%89%e7%86%b5%e6%8d%9f%e5%a4%b1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/176bfe21108480e19198ec69bf135e38?pvs=4&#34;&gt;(åŸç†&amp;amp;å®æˆ˜)äº¤å‰ç†µæŸå¤±&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>è§„èŒƒåŒ– Norm</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningNorm/</link>
      <pubDate>Mon, 08 Jan 2024 19:40:39 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningNorm/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;norm-ä½œç”¨1&#34;&gt;&#xA;  Norm ä½œç”¨[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#norm-%e4%bd%9c%e7%94%a81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;dnn çš„æ ‡å‡†ç»„ä»¶ï¼Œç¨³å®šå’ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹&lt;/p&gt;&#xA;&lt;h1 id=&#34;batch-norm1&#34;&gt;&#xA;  Batch Norm[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#batch-norm1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;reduce cross &lt;strong&gt;batch size&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;mini-batch dimension&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;ä¸€èˆ¬ç”¨äºå›¾åƒï¼Œä¸æ¶‰åŠåˆ°paddingçš„é—®é¢˜ï¼›&lt;/p&gt;&#xA;&lt;h1 id=&#34;layer-norm1&#34;&gt;&#xA;  Layer Norm[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#layer-norm1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;reduce cross &lt;strong&gt;hidden dim&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;reduce across the &lt;strong&gt;feature dimension&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;ä¸€èˆ¬ç”¨äºåºåˆ—ï¼Œä¸€ä¸ª batch size å†…å­˜åœ¨ paddingï¼›&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RMSNorm: å¯¹ LN çš„ä¸€ç§å˜ä½“ï¼Œllama&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;aside&gt; ğŸ’¡&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://spaces.ac.cn/archives/9009&#34;&gt;https://spaces.ac.cn/archives/9009&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Pre LN: &lt;code&gt;llama&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Post LN: &lt;code&gt;attention is all you need&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;llamaåœ¨å·¥ç¨‹ä¸Šä½¿ç”¨Pre LN&lt;/p&gt;&#xA;&lt;/aside&gt;&#xA;&lt;hr&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;[&lt;a href=&#34;https://www.bilibili.com/video/BV13q49eaERj/&#34;&gt;pytorch] BNã€LNã€RMSNorm åŠ pre LN vs. post LN å¯¹æ¯”ï¼Œæ ‡å‡†åŒ–&lt;/a&gt;  v ***&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;â€‹&#x9;&lt;a href=&#34;https://github.com/chunhuizhang/llm_aigc/blob/main/tutorials/nn_basics/tricks_norms/normalization.ipynb&#34;&gt;normalization.ipynb&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;â€‹&#x9;[&lt;a href=&#34;https://www.notion.so/pytorch-BN-LN-RMSNorm-pre-LN-vs-post-LN-177bfe2110848088830cfea3d5a33d3e?pvs=21&#34;&gt;pytorch] BNã€LNã€RMSNorm åŠ pre LN vs. post LN å¯¹æ¯”ï¼Œæ ‡å‡†åŒ– &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)GQA</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/ModelGQA/</link>
      <pubDate>Sun, 19 Nov 2023 14:29:40 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/ModelGQA/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡&#34;&gt;&#xA;  è®ºæ–‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.13245&#34;&gt;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;mha-vs-mqa-vs-gqa-1&#34;&gt;&#xA;  &lt;strong&gt;MHA vs. MQA vs.&lt;/strong&gt; GQA [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mha-vs-mqa-vs-gqa-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;mha&#34;&gt;&#xA;  &lt;strong&gt;MHA&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mha&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;é¦–å…ˆæ˜¯åŸå§‹çš„ &lt;strong&gt;MHA(Multi-Head Attention)&lt;/strong&gt;ï¼ŒQKV ä¸‰éƒ¨åˆ†æœ‰ç›¸åŒæ•°é‡çš„å¤´ï¼Œä¸”ä¸€ä¸€å¯¹åº”ã€‚æ¯æ¬¡åš Attentionï¼Œhead1 çš„ QKV å°±åšå¥½è‡ªå·±è¿ç®—å°±å¯ä»¥ï¼Œè¾“å‡ºæ—¶å„ä¸ªå¤´åŠ èµ·æ¥å°±è¡Œã€‚&lt;/p&gt;&#xA;&lt;h3 id=&#34;mqa&#34;&gt;&#xA;  &lt;strong&gt;MQA&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mqa&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;è€Œ &lt;strong&gt;MQA&lt;/strong&gt; åˆ™æ˜¯ï¼Œè®© &lt;strong&gt;Q ä»ç„¶ä¿æŒåŸæ¥çš„å¤´æ•°&lt;/strong&gt;ï¼Œä½† &lt;strong&gt;K å’Œ V åªæœ‰ä¸€ä¸ªå¤´&lt;/strong&gt;ï¼Œç›¸å½“äºæ‰€æœ‰çš„ Q å¤´å…±äº«ä¸€ç»„ K å’Œ V å¤´ï¼Œæ‰€ä»¥å«åš Multi-Query äº†ã€‚å®ç°æ”¹å˜äº†ä¼šä¸ä¼šå½±å“æ•ˆæœå‘¢ï¼Ÿç¡®å®ä¼šå½±å“ä½†ç›¸å¯¹å®ƒèƒ½å¸¦æ¥çš„æ”¶ç›Šï¼Œæ€§èƒ½çš„äº›å¾®é™ä½æ˜¯å¯ä»¥æ¥å—çš„ã€‚&lt;/p&gt;&#xA;&lt;p&gt;èƒ½å¸¦æ¥å¤šå¤§çš„æ”¶ç›Šå‘¢ï¼Œå®éªŒå‘ç°ä¸€èˆ¬èƒ½æé«˜ 30%-40% çš„ååã€‚&lt;/p&gt;&#xA;&lt;p&gt;æ”¶ç›Šä¸»è¦å°±æ˜¯ç”±é™ä½äº† KV cache å¸¦æ¥çš„ã€‚å®é™…ä¸Š MQA è¿ç®—é‡å’Œ MHA æ˜¯å·®ä¸å¤šçš„ï¼Œå¯ç†è§£ä¸º&lt;strong&gt;è¯»å–ä¸€ç»„ KV å¤´&lt;/strong&gt;ä¹‹åï¼Œ&lt;strong&gt;ç»™æ‰€æœ‰ Q å¤´ç”¨&lt;/strong&gt;ï¼Œä½†å› ä¸ºä¹‹å‰æåˆ°çš„å†…å­˜å’Œè®¡ç®—çš„ä¸å¯¹ç§°ï¼Œæ‰€ä»¥æ˜¯æœ‰åˆ©çš„ã€‚&lt;/p&gt;&#xA;&lt;h3 id=&#34;gqa&#34;&gt;&#xA;  GQA&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gqa&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;è€Œ &lt;strong&gt;GQA&lt;/strong&gt; å‘¢ï¼Œæ˜¯ MHA å’Œ MQA çš„æŠ˜è¡·æ–¹æ¡ˆï¼Œæ—¢ä¸æƒ³æŸå¤±æ€§èƒ½å¤ªå¤šï¼Œåˆæƒ³è·å¾— MQA å¸¦æ¥çš„æ¨ç†åŠ é€Ÿå¥½å¤„ã€‚å…·ä½“æ€æƒ³æ˜¯ï¼Œä¸æ˜¯æ‰€æœ‰ Q å¤´å…±äº«ä¸€ç»„ KVï¼Œè€Œæ˜¯&lt;strong&gt;åˆ†ç»„ä¸€å®šå¤´æ•° Q å…±äº«ä¸€ç»„ KV&lt;/strong&gt;ï¼Œæ¯”å¦‚ä¸Šé¢å›¾ç‰‡å°±æ˜¯ä¸¤ç»„ Q å…±äº«ä¸€ç»„ KVã€‚&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)æ¢¯åº¦ä¼˜åŒ–</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningGradOpt/</link>
      <pubDate>Thu, 04 Apr 2024 17:28:27 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/DeeplearningGradOpt/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;æ¢¯åº¦ä¼˜åŒ–&#34;&gt;&#xA;  æ¢¯åº¦ä¼˜åŒ–&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a2%af%e5%ba%a6%e4%bc%98%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h1 id=&#34;gradient-accumulation&#34;&gt;&#xA;  &lt;strong&gt;Gradient accumulation&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gradient-accumulation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/maonmv7e.bmp&#34; alt=&#34;maonmv7e.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/Acc.png&#34; alt=&#34;Acc.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;gradient-checkpointing-10&#34;&gt;&#xA;  &lt;strong&gt;G&lt;/strong&gt;radient checkpointing [10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gradient-checkpointing-10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;æ˜¾å­˜å ç”¨ä¼˜åŒ–ç®—æ³•&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;memory usage ä¸ computation time ä¹‹é—´çš„ tradeoff ï¼›&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;gradient checkpointing&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;In deep neural networks, backpropagation requires storing &lt;strong&gt;intermediate activations&lt;/strong&gt; for computing gradients during the backward pass.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ä½†æ˜¯å½“å±‚æ•°å˜å¤šæ—¶ï¼Œå­˜å‚¨æ‰€æœ‰çš„ä¸­é—´å±‚çš„æ¿€æ´»å€¼ï¼ˆintermediate activationsï¼‰éå¸¸åœ°å ç”¨æ˜¾å­˜ï¼›&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;gradient checkpointing&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;é€‰æ‹©æ€§åœ°é‡æ–°è®¡ç®—ï¼ˆrecomputeï¼‰ä¸€éƒ¨åˆ†çš„ intermediate activations åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­&lt;/strong&gt;æ¥ç¼“è§£æ˜¾å­˜çš„å‹åŠ›ï¼›&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/checkpoint.png&#34; alt=&#34;checkpoint.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;./images/recompute.png&#34; alt=&#34;recompute.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;gradient-clipping--æ¢¯åº¦è£å‰ª&#34;&gt;&#xA;  &lt;strong&gt;Gradient Clipping  (æ¢¯åº¦è£å‰ª)&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#gradient-clipping--%e6%a2%af%e5%ba%a6%e8%a3%81%e5%89%aa&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;ç›®çš„21&#34;&gt;&#xA;  ç›®çš„[21]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e7%9a%8421&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;æ¢¯åº¦çˆ†ç‚¸é—®é¢˜çš„å¸¸è§åº”å¯¹æ–¹å¼ä¸ºâ€œæ¢¯åº¦è£å‰ªâ€&lt;/strong&gt;ï¼Œä¹Ÿå°±æ˜¯é€šè¿‡â€œclipâ€æ–¹å¼æ¥é˜²æ­¢è¿­ä»£ä¸­æ¢¯åº¦å€¼è¿‡å¤§ã€‚&lt;/p&gt;&#xA;&lt;h3 id=&#34;ä¸¤ç§å¸¸è§å½¢å¼20&#34;&gt;&#xA;  ä¸¤ç§å¸¸è§å½¢å¼[20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%a4%e7%a7%8d%e5%b8%b8%e8%a7%81%e5%bd%a2%e5%bc%8f20&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;æ¢¯åº¦èŒƒæ•°è£å‰ªï¼ˆGradient Norm Clippingï¼‰: è¿™ç§æ–¹æ³•æ¶‰åŠè®¡ç®—æ‰€æœ‰å‚æ•°æ¢¯åº¦çš„èŒƒæ•°ï¼ˆä¾‹å¦‚L2èŒƒæ•°ï¼‰ï¼Œå¦‚æœè¿™ä¸ªèŒƒæ•°è¶…è¿‡äº†è®¾å®šçš„é˜ˆå€¼ï¼Œå°±å°†æ¢¯åº¦ç¼©æ”¾åˆ°è¿™ä¸ªé˜ˆå€¼ä»¥å†…ã€‚åœ¨PyTorchä¸­ï¼Œè¿™å¯ä»¥é€šè¿‡ &lt;strong&gt;torch.nn.utils.clip_grad_norm_&lt;/strong&gt; å‡½æ•°å®ç°ã€‚&lt;/li&gt;&#xA;&lt;li&gt;æ¢¯åº¦å€¼è£å‰ªï¼ˆGradient Value Clippingï¼‰: è¿™ç§æ–¹æ³•å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦å€¼è¿›è¡Œç‹¬ç«‹è£å‰ªï¼Œç¡®ä¿å®ƒä»¬ä¸ä¼šè¶…è¿‡ä¸€ä¸ªè®¾å®šçš„æœ€å¤§å€¼æˆ–æœ€å°å€¼ã€‚åœ¨PyTorchä¸­ï¼Œè¿™å¯ä»¥é€šè¿‡ &lt;strong&gt;torch.nn.utils.clip_grad_value_&lt;/strong&gt; å‡½æ•°å®ç°ã€‚&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;overview&#34;&gt;&#xA;  overview&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/v4.18.0/en/performance&#34;&gt;Performance and Scalability: How To Fit a Bigger Model and Train It Faster&lt;/a&gt;  ***&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)è¿‡æ‹Ÿåˆ</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/basic/DeeplearningOverfitting/</link>
      <pubDate>Mon, 08 Jan 2024 19:40:39 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/basic/DeeplearningOverfitting/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;åŸç†è¿‡æ‹Ÿåˆ&#34;&gt;&#xA;  (åŸç†)è¿‡æ‹Ÿåˆ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8e%9f%e7%90%86%e8%bf%87%e6%8b%9f%e5%90%88&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/1e1bfe2110848025a7b2ffba454daa86?source=copy_link&#34;&gt;(åŸç†)è¿‡æ‹Ÿåˆ&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(å®æˆ˜)Transformer</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/TransformerCode/</link>
      <pubDate>Thu, 16 Feb 2023 13:57:57 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/TransformerCode/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://github.com/www6v/AIGC/blob/master/transformer/transformer.ipynb&#34;&gt;transformer.ipynb&lt;/a&gt; git&#xA;&lt;a href=&#34;https://www.bilibili.com/video/BV1nc411y7m4/&#34;&gt;Transformerä»£ç å®ç°&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://paperswithcode.com/method/transformer&#34;&gt;Transformer&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201&#34;&gt;transformer.py&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;http://arthurchiao.art/blog/transformers-from-scratch-zh/&#34;&gt;[è¯‘] Transformer æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š600 è¡Œ Python ä»£ç å®ç° self-attention å’Œä¸¤ç±» Transformerï¼ˆ2019ï¼‰&lt;/a&gt; V, github&#xA;Transformers from scratch&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://blog.csdn.net/v_JULY_v/article/details/130090649&#34;&gt;ä»é›¶å®ç°Transformerçš„ç®€æ˜“ç‰ˆä¸å¼ºå¤§ç‰ˆï¼šä»300å¤šè¡Œåˆ°3000å¤šè¡Œ&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/398039366&#34;&gt;Transformeræºç è¯¦è§£ï¼ˆPytorchç‰ˆæœ¬ï¼‰&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†&amp;å®æˆ˜)veRL Config</title>
      <link>https://www6v.github.io/www6vAlgo/docs/RL/framework/veRLConfig/</link>
      <pubDate>Mon, 22 Jul 2024 16:14:51 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/RL/framework/veRLConfig/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;verl-config&#34;&gt;&#xA;  veRL Config&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#verl-config&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/veRL-Config-238bfe21108480b48225d75aec0990b2?source=copy_link&#34;&gt;(åŸç†&amp;amp;å®æˆ˜)veRL Config&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tokenizer</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/TrainTokenizer/</link>
      <pubDate>Sun, 26 Feb 2023 17:28:01 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/Transformer/TrainTokenizer/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h3 id=&#34;tokenizer-åˆ†è¯&#34;&gt;&#xA;  tokenizer åˆ†è¯&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tokenizer-%e5%88%86%e8%af%8d&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;å•è¯åˆ†è¯æ³•&lt;/li&gt;&#xA;&lt;li&gt;å•å­—åˆ†è¯æ³•&lt;/li&gt;&#xA;&lt;li&gt;å­è¯åˆ†è¯æ³•&#xA;BPE [GPTç³»åˆ—], WordPiece&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/630696264&#34;&gt;å¤§æ¨¡å‹è¯è¡¨æ‰©å……å¿…å¤‡å·¥å…·SentencePiece&lt;/a&gt;&#xA;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/458452872&#34;&gt;NLPï¼ˆäºŒï¼‰ï¼šæµ…è°ˆåˆ†è¯&lt;/a&gt;&#xA;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1vN411p7t2/&#34;&gt;https://www.bilibili.com/video/BV1vN411p7t2/&lt;/a&gt;&#xA;1xx. &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAxMjc3MjkyMg==&amp;amp;mid=2648400849&amp;amp;idx=1&amp;amp;sn=58006756cccde4d06d273df59e2c8dd8&#34;&gt;å¼€æºå¤§æ¨¡å‹å¦‚ä½•æ›´å¥½åœ°é€‚åº”ä¸­æ–‡åœºæ™¯ï¼šLLAMAæ‰©å……è¯è¡¨ã€BLOOMè£å‰ªè¯è¡¨åŸºæœ¬åŸç†ä¸å¼€æºå®ç°&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(å®æˆ˜)PyTorch</title>
      <link>https://www6v.github.io/www6vAlgo/docs/DeepLearning/basic/Pytorch/</link>
      <pubDate>Tue, 28 Mar 2023 15:47:08 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/DeepLearning/basic/Pytorch/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;pytorch-å®æˆ˜&#34;&gt;&#xA;  PyTorch å®æˆ˜&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pytorch-%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/PyTorch-bae29b5883fd45f7a20c97918382da12?pvs=4&#34;&gt;(å®æˆ˜)PyTorch&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>äººåƒç”Ÿå›¾</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/%E4%BA%BA%E5%83%8F%E7%94%9F%E5%9B%BE/gptMultimodalIDCreate/</link>
      <pubDate>Sat, 03 Aug 2024 14:22:39 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/%E4%BA%BA%E5%83%8F%E7%94%9F%E5%9B%BE/gptMultimodalIDCreate/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;instantid&#34;&gt;&#xA;  InstantID&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#instantid&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/InstantID-0b96b4a8d12340a1900995ca7f33ef05?pvs=4&#34;&gt;InstantID&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;photomaker&#34;&gt;&#xA;  PhotoMaker&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#photomaker&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/PhotoMaker-f4b3e96a9ed046838b7255e026bd1abf?pvs=4&#34;&gt;PhotoMaker&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;æ€»ç»“&#34;&gt;&#xA;  æ€»ç»“&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;ã€InstantID : ipAdaptor +controlnet,  image Contollçš„æ€è·¯ã€‘&lt;br&gt;&#xA;ã€photomaker: image  Edit çš„æ€è·¯ã€‘&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Survey) Test-Time Scaling</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Survey/ReasoningTTS/</link>
      <pubDate>Sun, 21 Jul 2024 12:39:48 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Survey/ReasoningTTS/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;test-time-scaling&#34;&gt;&#xA;  Test-Time Scaling&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#test-time-scaling&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/A-Survey-on-Test-Time-Scaling-in-Large-Language-Models-What-How-Where-and-How-Well-227bfe21108480ff95e4c0c105c3c74b?source=copy_link&#34;&gt;(Survey) Test-Time Scaling&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†|å®æˆ˜)DreamBooth</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionDreamBooth/</link>
      <pubDate>Wed, 17 Jul 2024 10:51:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionDreamBooth/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;approach1&#34;&gt;&#xA;  Approach[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#approach1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Our method takes as input a few images (typically 3-5 images suffice, based on our experiments) of a subject (e.g., a specific dog) and the corresponding class name (e.g. &amp;ldquo;dog&amp;rdquo;), and returns a fine-tuned/&amp;ldquo;personalized&amp;rsquo;&amp;rsquo; text-to-image model that encodes a &lt;strong&gt;unique identifier that refers to the subject&lt;/strong&gt;. Then, at inference, we can implant the unique identifier in different sentences to synthesize the subjects in difference contexts.&#xA;æˆ‘ä»¬çš„æ–¹æ³•å°†ä¸»é¢˜ï¼ˆä¾‹å¦‚ï¼Œç‰¹å®šçš„ç‹—ï¼‰å’Œç›¸åº”çš„ç±»åç§°ï¼ˆä¾‹å¦‚â€œç‹—â€ï¼‰çš„ä¸€äº›å›¾åƒï¼ˆæ ¹æ®æˆ‘ä»¬çš„å®éªŒï¼Œé€šå¸¸ 3-5 ä¸ªå›¾åƒå°±è¶³å¤Ÿäº†ï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªå¾®è°ƒ/ â€œä¸ªæ€§åŒ–â€æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œç¼–ç &lt;strong&gt;æŒ‡å‘ä¸»é¢˜çš„å”¯ä¸€æ ‡è¯†ç¬¦&lt;/strong&gt;ã€‚ç„¶åï¼Œåœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å°†å”¯ä¸€æ ‡è¯†ç¬¦æ¤å…¥ä¸åŒçš„å¥å­ä¸­ï¼Œä»¥åˆæˆä¸åŒä¸Šä¸‹æ–‡ä¸­çš„ä¸»é¢˜ã€‚&lt;/p&gt;</description>
    </item>
    <item>
      <title>pix2pix-zero</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/editing/gptDiffusionEditPix2pix/</link>
      <pubDate>Tue, 16 Apr 2024 16:26:40 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/editing/gptDiffusionEditPix2pix/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡&#34;&gt;&#xA;  è®ºæ–‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;è®ºæ–‡åœ°å€&lt;br&gt;&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2302.03027&#34;&gt;Zero-shot Image-to-Image Translation&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å¼€æºåœ°å€&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/pix2pixzero/pix2pix-zero&#34;&gt;pix2pix-zero&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Project page&lt;br&gt;&#xA;&lt;a href=&#34;https://pix2pixzero.github.io/&#34;&gt;Project page&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;å®˜ç½‘æœ‰ä¸ªä»‹ç»è§†é¢‘ çœ‹è¿‡ä¸é”™&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;method1&#34;&gt;&#xA;  Method[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/e9wsarfq.bmp&#34; alt=&#34;e9wsarfq.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;ä¸Šå›¾å±•ç¤ºäº†pix2pix-zeroæ–¹æ³•çš„æ¦‚è¿°ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å›¾ç‰‡ä»çŒ«å˜æˆç‹—çš„å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä¾‹å­ã€‚é¦–å…ˆï¼Œä½¿ç”¨è§„èŒƒåŒ–çš„DDIMåè½¬æ¥å¾—åˆ°ä¸€ä¸ªåè½¬çš„å™ªå£°æ˜ å°„ï¼Œè¿™æ˜¯ç”±BLIPå›¾åƒå­—å¹•ï¼ˆcaptionï¼‰ç½‘ç»œå’ŒCLIPæ–‡æœ¬åµŒå…¥æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥å¼•å¯¼çš„ã€‚ç„¶åï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬åµŒå…¥å»å™ªä»¥è·å¾—äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œä½œä¸ºè¾“å…¥å›¾åƒç»“æ„çš„å‚è€ƒï¼ˆé¡¶éƒ¨è¡Œï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œä½¿ç”¨ç¼–è¾‘åçš„æ–‡æœ¬åµŒå…¥å»å™ªï¼Œé€šè¿‡æŸå¤±å‡½æ•°ç¡®ä¿è¿™äº›äº¤å‰æ³¨æ„åŠ›å›¾ä¸å‚è€ƒäº¤å‰æ³¨æ„åŠ›å›¾ç›¸åŒ¹é…ï¼ˆç¬¬äºŒè¡Œï¼‰ã€‚è¿™ç¡®ä¿äº†ç¼–è¾‘å›¾åƒçš„ç»“æ„ä¸åŸå§‹å›¾åƒç›¸æ¯”ä¸ä¼šå‘ç”Ÿå‰§çƒˆå˜åŒ–ã€‚æ²¡æœ‰äº¤å‰æ³¨æ„åŠ›å¼•å¯¼çš„å»å™ªç¤ºä¾‹æ˜¾ç¤ºåœ¨ç¬¬ä¸‰è¡Œï¼Œå¯¼è‡´ç»“æ„ä¸Šçš„å¤§åå·®ã€‚æ­¤å¯è§†åŒ–å¼ºè°ƒäº†åœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿æŒå›¾åƒåŸå§‹ç»“æ„çš„äº¤å‰æ³¨æ„åŠ›çš„é‡è¦æ€§ã€‚&lt;/p&gt;&#xA;&lt;h1 id=&#34;discovering-edit-directions-2&#34;&gt;&#xA;  Discovering edit directions [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#discovering-edit-directions-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/hwe1rl5e.bmp&#34; alt=&#34;hwe1rl5e.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/684673737&#34;&gt;pix2pix-zeroï¼šé›¶æ ·æœ¬å›¾åƒåˆ°å›¾åƒè½¬æ¢&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/x1131230123/article/details/132169755&#34;&gt;ã€æ·±åº¦å­¦ä¹ ã€‘ã€é£æ ¼è¿ç§»ã€‘Zero-shot Image-to-Image Translation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;å®æˆ˜&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/docs/diffusers/v0.13.0/en/api/pipelines/stable_diffusion/pix2pix_zero&#34;&gt;Zero-shot Image-to-Image Translation&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>InstructPix2Pix</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/editing/gptDiffusionEditInstruct/</link>
      <pubDate>Tue, 16 Apr 2024 16:25:58 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/editing/gptDiffusionEditInstruct/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡&#34;&gt;&#xA;  è®ºæ–‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;è®ºæ–‡åœ°å€&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2211.09800&#34;&gt;InstructPix2Pix: Learning to Follow Image Editing Instructions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;Repo&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;li&gt;Project page&#xA;&lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix&#34;&gt;Project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;method&#34;&gt;&#xA;  &lt;strong&gt;Method&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;è¿™ç« å°±è®²ä¸¤ä»¶äº‹ï¼š1ï¼‰å¦‚ä½•&lt;strong&gt;ç”Ÿæˆæ•°æ®é›†&lt;/strong&gt;ï¼ˆç« èŠ‚3.1ï¼‰ï¼›2ï¼‰å¦‚ä½•&lt;strong&gt;åŸºäºä¸Šä¸€æ­¥ç”Ÿæˆçš„è®­ç»ƒæ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªå›¾åƒç¼–è¾‘æ‰©æ•£æ¨¡å‹&lt;/strong&gt;ï¼ˆç« èŠ‚3.2ï¼‰ï¼›&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/94xbrqlt.bmp&#34; alt=&#34;94xbrqlt.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/72gzhwfl.bmp&#34; alt=&#34;72gzhwfl.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/655135961&#34;&gt;InstructPix2Pixï¼šç”¨æŒ‡ä»¤ç»™å›¾åƒåšä¿®æ”¹&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;å®æˆ˜&#34;&gt;&#xA;  å®æˆ˜&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.notion.so/5d04c1bbd0d34be4b3fb0087cb670efd?pvs=21&#34;&gt;InstructPix2Pix&lt;/a&gt;  diffusers&#xA;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1Go4y1M7cK?p=3&#34;&gt;InstructPix2Pix&lt;/a&gt; V&#xA;&lt;a href=&#34;https://github.com/www6v/Diffusion_Training_Examples&#34;&gt;Code Repo&lt;/a&gt; git&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prompt-to-Prompt</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/editing/gptDiffusionEditPrompt/</link>
      <pubDate>Tue, 16 Apr 2024 16:25:38 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/editing/gptDiffusionEditPrompt/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡&#34;&gt;&#xA;  è®ºæ–‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;è®ºæ–‡åœ°å€&#xA;&lt;a href=&#34;https://prompt-to-prompt.github.io/ptp_files/Prompt-to-Prompt_preprint.pdf&#34;&gt;PROMPT-TO-PROMPT IMAGE EDITING&#xA;WITH CROSS-ATTENTION CONTROL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://github.com/google/prompt-to-prompt/&#34;&gt;Prompt-to-Prompt&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;li&gt;Project page&#xA;&lt;a href=&#34;https://prompt-to-prompt.github.io/&#34;&gt;Project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;ç¤ºæ„1&#34;&gt;&#xA;  ç¤ºæ„[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%a4%ba%e6%84%8f1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/gvn5hr6b.bmp&#34; alt=&#34;gvn5hr6b.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;åº”ç”¨1&#34;&gt;&#xA;  åº”ç”¨[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ba%94%e7%94%a81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;**åº”ç”¨æ–¹é¢ï¼š**Prompt-to-Prompt è¿™ä¸ªæ–¹æ³•æ˜¯åŸç†ä¸Šçš„åˆ›æ–°ï¼Œåº”ç”¨æ–¹é¢åªé€‚ç”¨äºâ€œå·²ç»ç”Ÿæˆäº†ä¸€å¼ å¤§è‡´æ»¡æ„çš„å›¾ï¼Œæˆ‘ä»¬æƒ³å¯¹å®ƒè¿›è¡Œéƒ¨åˆ†ä¿®æ”¹â€ã€‚&lt;strong&gt;ä½†æ˜¯å¯¹äºâ€œæ‰‹å¤´æœ‰ä¸€å¼ æ¥å†ä¸æ˜çš„å›¾ï¼Œæˆ‘ä»¬æƒ³å¯¹å®ƒè¿›è¡Œä¿®æ”¹â€è¿™ä¸ªä»»åŠ¡å°±å¾ˆéº»çƒ¦äº†&lt;/strong&gt;ï¼Œå› ä¸ºå¾ˆéš¾å»å€’æ¨è¿™å¼ å›¾å¯¹åº”çš„promptæ˜¯å•¥ã€‚&lt;/p&gt;&#xA;&lt;p&gt;æ‰€ä»¥åç»­æœ‰ä¸€é¡¹å·¥ä½œå« &lt;a href=&#34;https://zhuanlan.zhihu.com/p/655135961&#34;&gt;InstructPix2Pix&lt;/a&gt;ï¼Œä½œç”¨æ˜¯â€œ&lt;strong&gt;ä¸€å¼ æ¥å†ä¸æ˜çš„å›¾ï¼Œåªè¦è¯´â€˜æŠŠçŒ«æ”¹æˆç‹—â€™ï¼Œæ¨¡å‹å°±èƒ½æŠŠç”»é¢é‡Œçš„çŒ«æ”¹æˆç‹—ï¼Œå…¶ä»–ä¸å˜&lt;/strong&gt;ã€‚â€éå¸¸å¥½ç”¨ï¼Œå¬è¯´å·²ç»é›†æˆåœ¨ Stable Diffusion WebUI é‡Œå¯ä»¥ç›´æ¥ç”¨äº†ã€‚&lt;/p&gt;&#xA;&lt;h1 id=&#34;method1&#34;&gt;&#xA;  Method[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/vfk2k2bg.bmp&#34; alt=&#34;vfk2k2bg.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;ä¸ŠåŠéƒ¨åˆ†ï¼ŒåŸç‰ˆcross-attentionï¼Œä¸‹åŠéƒ¨åˆ†ï¼Œæœ¬æ–‡çš„cross-attention&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/ck0g81ot.bmp&#34; alt=&#34;ck0g81ot.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/655372592&#34;&gt;Prompt-to-promptï¼šè®©ç”Ÿæˆçš„å›¾åƒä¿æŒä¸€è‡´&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_40779727/article/details/136854062&#34;&gt;diffusion model(åå››)ï¼š prompt-to-prompt æ·±åº¦å‰–æ&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Survey)Reasoning LLM Post-Training</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Survey/ReasoningPostTraining/</link>
      <pubDate>Thu, 04 Apr 2024 22:33:18 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Survey/ReasoningPostTraining/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;reasoning-llm-post-training&#34;&gt;&#xA;  Reasoning LLM Post-Training&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reasoning-llm-post-training&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;iframe src=&#34;https://candied-skunk-1ca.notion.site/ebd/1bbbfe21108480859284e9e8f686fc4a&#34; width=&#34;100%&#34; height=&#34;1000&#34; frameborder=&#34;0&#34; allowfullscreen /&gt;</description>
    </item>
    <item>
      <title>(åŸç†)GRPO</title>
      <link>https://www6v.github.io/www6vAlgo/docs/RL/core/GRPO-family/GRPO/</link>
      <pubDate>Thu, 04 Apr 2024 22:03:52 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/RL/core/GRPO-family/GRPO/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;grpo&#34;&gt;&#xA;  GRPO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#grpo&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/ebd/194bfe21108480129939e44fd7ddacfd&#34;&gt;GRPO&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>å¤šæ¨¡æ€ ç³»åˆ—</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Survey/gptMultimodalSeries/</link>
      <pubDate>Thu, 04 Apr 2024 09:38:25 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Survey/gptMultimodalSeries/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;ç›®å½•&#34;&gt;&#xA;  ç›®å½•&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;stage1-æ¨¡å—ç‹¬ç«‹2&#34;&gt;&#xA;  Stage1: æ¨¡å—ç‹¬ç«‹[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#stage1-%e6%a8%a1%e5%9d%97%e7%8b%ac%e7%ab%8b2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/stage1.webp&#34; alt=&#34;stage1.webp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;model&#34;&gt;&#xA;  model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CLIP&lt;/li&gt;&#xA;&lt;li&gt;ViLT&lt;/li&gt;&#xA;&lt;li&gt;ALBEF&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;stage2-æ¨¡å—å…±äº«2&#34;&gt;&#xA;  Stage2: æ¨¡å—å…±äº«[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#stage2-%e6%a8%a1%e5%9d%97%e5%85%b1%e4%ba%ab2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;model-1&#34;&gt;&#xA;  model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VLMO&lt;/li&gt;&#xA;&lt;li&gt;BLIP&lt;/li&gt;&#xA;&lt;li&gt;BLIP2&lt;/li&gt;&#xA;&lt;li&gt;BEiTv3&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;stage3-èŒƒå¼ç»Ÿä¸€2&#34;&gt;&#xA;  Stage3: èŒƒå¼ç»Ÿä¸€[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#stage3-%e8%8c%83%e5%bc%8f%e7%bb%9f%e4%b8%802&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;model-2&#34;&gt;&#xA;  model&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#model-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Unified-IO&lt;/li&gt;&#xA;&lt;li&gt;Uni-Perceiver&lt;/li&gt;&#xA;&lt;li&gt;PaLi&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;æ€»ç»“-1&#34;&gt;&#xA;  æ€»ç»“ [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/multimodal.webp&#34; alt=&#34;multimodal.webp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;overview&#34;&gt;&#xA;  Overview&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/653902791&#34;&gt;å¤šæ¨¡æ€å¤§æ¨¡å‹ CLIP, BLIP, BLIP2, LLaVA, miniGPT4, InstructBLIP ç³»åˆ—è§£è¯»&lt;/a&gt; ***&lt;/p&gt;</description>
    </item>
    <item>
      <title>VQ-VAE</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Vision-Encoder/gptMultimodalVQVAE/</link>
      <pubDate>Sun, 24 Mar 2024 12:06:39 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Vision-Encoder/gptMultimodalVQVAE/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/647399156&#34;&gt;å…³äº VQ-VAE ç›´è§‚ç†è§£&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1bb4y1i7j6/?vd_source=f6e8c1128f9f264c5ab8d9411a644036&#34;&gt;[è®ºæ–‡ç®€æ]VQ-VAE:Neural discrete representation learning[1711.00937]&lt;/a&gt; v&#xA;çœ‹è¯„è®ºä¸­çš„ç½®é¡¶ æœ‰ä»£ç &lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1yN4y1k794/&#34;&gt;å¦‚ä½•æ­å»ºVQ-VAEæ¨¡å‹ï¼ˆPytorchä»£ç ï¼‰&lt;/a&gt; v&#xA;çœ‹è§†é¢‘ä»‹ç» &lt;a href=&#34;https://github.com/KevinOfCathay/DDPM-demo&#34;&gt;https://github.com/KevinOfCathay/DDPM-demo&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>DINO</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Vision-Encoder/gptMultimodalDINO/</link>
      <pubDate>Sun, 24 Mar 2024 11:18:19 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Vision-Encoder/gptMultimodalDINO/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;dino&#34;&gt;&#xA;  DINO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dino&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/dino&#34;&gt;https://github.com/facebookresearch/dino&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/EGCAX51FTyZrO7-e4y9Egg&#34;&gt;é‡å¡‘è‡ªç›‘ç£å­¦ä¹ : DINO ç½‘ç»œå¦‚ä½•é¢ è¦†è§†è§‰ç‰¹å¾è¡¨ç¤ºçš„å¸¸è§„æ–¹æ³•&lt;/a&gt;   æœ‰åŠ¨å›¾&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;dinov2&#34;&gt;&#xA;  &lt;strong&gt;DINOv2&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dinov2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;è®ºæ–‡&#34;&gt;&#xA;  è®ºæ–‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;è®ºæ–‡åœ°å€&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2304.07193&#34;&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34;&gt;https://github.com/facebookresearch/dinov2&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Project page&#xA;&lt;a href=&#34;https://dinov2.metademolab.com/&#34;&gt;Project page&lt;/a&gt;&#xA;&lt;a href=&#34;https://pierrefdz.github.io/publications/dinov2/&#34;&gt;Project page&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ-1&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2314807&#34;&gt;DINOv2ï¼šæ— éœ€å¾®è°ƒï¼Œå¡«è¡¥ SAM çš„ç©ºç™½ï¼Œæ”¯æŒå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/CVHub/article/details/130304078&#34;&gt;å…¨ç½‘æœ€è¯¦ç»†çš„ DINOv2 è®ºæ–‡è§£è¯»æ¥å•¦ï¼&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_43694096/article/details/135761460&#34;&gt;æ·±åº¦å­¦ä¹ ç®—æ³•åº”ç”¨å®æˆ˜ | DINOv2 å›¾åƒç›¸ä¼¼åº¦å®æˆ˜&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/629042197&#34;&gt;è§†è§‰å¤§æ¨¡å‹DINOv2:è‡ªæˆ‘ç›‘ç£å­¦ä¹ çš„æ–°é¢†åŸŸ&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/650234933&#34;&gt;DINOv2ï¼šæ— éœ€å¾®è°ƒï¼Œå¡«è¡¥SAMç©ºç™½&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;å®æˆ˜&#34;&gt;&#xA;  å®æˆ˜&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://mmpretrain.readthedocs.io/zh-cn/dev/papers/dinov2.html&#34;&gt;DINOv2 on mmpretrain&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(å®æˆ˜)Deepseek è’¸é¦</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Deepseek/post-training/DeepseekDistill/</link>
      <pubDate>Sun, 25 Feb 2024 18:46:57 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Deepseek/post-training/DeepseekDistill/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/R1-1a5bfe211084809eb615f84dc74f4bb6?pvs=4&#34;&gt;(å®æˆ˜)Deepseek è’¸é¦&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(å®æˆ˜)Deepseek R1 SFT</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Deepseek/post-training/DeepseekSFT/</link>
      <pubDate>Fri, 23 Feb 2024 17:29:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Deepseek/post-training/DeepseekSFT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;deepseek-r1-sft&#34;&gt;&#xA;  DeepSeek R1 SFT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#deepseek-r1-sft&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/DeepSeek-R1-SFT-1a3bfe21108480c39346d30d7f189550?pvs=4&#34;&gt;(å®æˆ˜)DeepSeek R1 SFT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)Reasoning LLM</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/ReasoningLLM/</link>
      <pubDate>Sun, 18 Feb 2024 09:01:44 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/ReasoningLLM/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;reasoning-llm&#34;&gt;&#xA;  Reasoning LLM&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reasoning-llm&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Visual-Guide-to-Reasoning-LLMs-Exploring-Test-Time-Compute-Techniques-and-DeepSeek-R1-194bfe2110848099a5c3e0585f332d2a?pvs=4&#34;&gt;(åŸç†)Reasoning LLM&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†) DeepSeek R1</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/DeepSeekR1/</link>
      <pubDate>Tue, 06 Feb 2024 18:42:00 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Reasoning/Deepseek/R1/DeepSeekR1/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;åŸç†-deepseek-r1&#34;&gt;&#xA;  (åŸç†) DeepSeek R1&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8e%9f%e7%90%86-deepseek-r1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/DeepSeek-R1-188bfe21108480778260cd48a6a2417b?pvs=4&#34;&gt;(åŸç†)DeepSeek R1&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)BERT</title>
      <link>https://www6v.github.io/www6vAlgo/docs/LLM/Dense/BERT/</link>
      <pubDate>Fri, 12 Jan 2024 22:16:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/LLM/Dense/BERT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;bert&#34;&gt;&#xA;  BERT&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#bert&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/BERT-16abfe211084808fb8a3d0769c37c3db?pvs=4&#34;&gt;(åŸç†)BERT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)unCLIP</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Diffusion/gptDiffusionunCLIP/</link>
      <pubDate>Sun, 27 Aug 2023 10:21:28 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Diffusion/gptDiffusionunCLIP/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;unclip&#34;&gt;&#xA;  unCLIP&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#unclip&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/unCLIP-7603f64564a54cb4af08a1cf38c890e1?pvs=4&#34;&gt;(åŸç†)unCLIP&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†|å®æˆ˜)ReferenceNet</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionReferenceNet/</link>
      <pubDate>Tue, 22 Aug 2023 18:27:16 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionReferenceNet/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;referencenet&#34;&gt;&#xA;  ReferenceNet&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#referencenet&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/ReferenceNet-22a14d3a669e4d7f8c99409e34252349?pvs=4&#34;&gt;(åŸç†|å®æˆ˜)ReferenceNet&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†|å®æˆ˜)IP-Adapter</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionIPAdapter/</link>
      <pubDate>Tue, 22 Aug 2023 18:26:51 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionIPAdapter/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡ip-adapter&#34;&gt;&#xA;  è®ºæ–‡[IP-Adapter]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87ip-adapter&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;è®ºæ–‡åœ°å€&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2308.06721&#34;&gt;IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;IP-Adapter&lt;/a&gt; git&#xA;enable a pretrained text-to-image diffusion model to generate images &lt;strong&gt;with image prompt&lt;/strong&gt;&#xA;æœ‰å¾ˆå¤šnotebookçš„demo&lt;/li&gt;&#xA;&lt;li&gt;Project page&#xA;&lt;a href=&#34;https://ip-adapter.github.io/&#34;&gt;Project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;ip-adapter10&#34;&gt;&#xA;  IP-Adapter[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ip-adapter10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hf.co/papers/2308.06721&#34;&gt;IP-Adapter&lt;/a&gt; is an image prompt adapter that can be plugged into diffusion models to&#xA;enable image prompting without any changes to the underlying model.&#xA;Furthermore, this adapter can be reused with other models finetuned from&#xA;the same base model and it can be combined with other adapters like &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/using-diffusers/controlnet&#34;&gt;ControlNet&lt;/a&gt;. The key idea behind IP-Adapter is the &lt;em&gt;decoupled cross-attention&lt;/em&gt;&#xA;mechanism which adds a separate cross-attention layer just for image&#xA;features instead of using the same cross-attention layer for both text&#xA;and image features. This allows the model to learn more image-specific&#xA;features.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†|å®æˆ˜)T2I-Adapter</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionT2IAdapter/</link>
      <pubDate>Tue, 22 Aug 2023 18:26:35 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionT2IAdapter/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡t2i-adapter&#34;&gt;&#xA;  è®ºæ–‡[T2I-Adapter]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87t2i-adapter&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;è®ºæ–‡åœ°å€&#xA;&lt;a href=&#34;https://arxiv.org/pdf/2302.08453&#34;&gt;T2I-Adapter&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://github.com/TencentARC/T2I-Adapter&#34;&gt;T2I-Adapter&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;t2i-adapter&#34;&gt;&#xA;  &lt;strong&gt;T2I-Adapter[10]&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#t2i-adapter&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hf.co/papers/2302.08453&#34;&gt;T2I-Adapter&lt;/a&gt; is a lightweight adapter for controlling and providing more accurate&#xA;structure guidance for text-to-image models. It works by learning an alignment between the internal knowledge of the&#xA;text-to-image model and an external control signal, such as edge detection or depth estimation.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://hf.co/papers/2302.08453&#34;&gt;T2I-Adapter&lt;/a&gt; æ˜¯ä¸€ç§&lt;strong&gt;è½»é‡çº§&lt;/strong&gt;é€‚é…å™¨ï¼Œç”¨äºæ§åˆ¶&lt;strong&gt;æ–‡æœ¬åˆ°å›¾åƒ&lt;/strong&gt;æ¨¡å‹å¹¶æä¾›æ›´å‡†ç¡®çš„&lt;strong&gt;ç»“æ„æŒ‡å¯¼&lt;/strong&gt;ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯å­¦ä¹ æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„&lt;strong&gt;å†…éƒ¨çŸ¥è¯†ä¸å¤–éƒ¨æ§åˆ¶ä¿¡å·&lt;/strong&gt;ï¼ˆå¦‚è¾¹ç¼˜æ£€æµ‹æˆ–æ·±åº¦ä¼°è®¡ï¼‰ä¹‹é—´çš„&lt;strong&gt;å¯¹é½&lt;/strong&gt;ã€‚&lt;/p&gt;&#xA;&lt;p&gt;The T2I-Adapter design is simple, the condition is passed to four feature extraction blocks and three downsample blocks. This makes it fast and easy to train different adapters for different conditions which can be plugged into the text-to-image model. T2I-Adapter is similar to &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/using-diffusers/controlnet&#34;&gt;ControlNet&lt;/a&gt; except it is smaller (~77M parameters) and faster because it only runs once during the diffusion process. The downside is that performance may be slightly worse than ControlNet.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†|å®æˆ˜)ControlNet</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionControlNet/</link>
      <pubDate>Tue, 22 Aug 2023 18:26:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionControlNet/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡controlnet&#34;&gt;&#xA;  è®ºæ–‡[ControlNet]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87controlnet&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;è®ºæ–‡åœ°å€&#xA;&lt;a href=&#34;https://arxiv.org/abs/2302.05543&#34;&gt;Adding Conditional Control to Text-to-Image Diffusion Models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;controlnet10&#34;&gt;&#xA;  ControlNet[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#controlnet10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;ControlNet is a type of model for controlling image diffusion models by&#xA;conditioning the model with an additional input image. There are many&#xA;types of conditioning inputs (canny edge, user sketching, human pose,&#xA;depth, and more) you can use to control a diffusion model. This is&#xA;hugely useful because it affords you greater control over image&#xA;generation, making it easier to generate specific images without&#xA;experimenting with different text prompts or denoising values as much.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)Guidance</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Diffusion/gptDiffusionGuidance/</link>
      <pubDate>Tue, 01 Aug 2023 19:19:40 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Diffusion/gptDiffusionGuidance/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;guidance&#34;&gt;&#xA;  Guidance&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#guidance&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Guidance-bcb0b5a85b5f454fa84875eaeb518983?pvs=4&#34;&gt;Guidance&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)SD XL</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Diffusion/gptDiffusionXL/</link>
      <pubDate>Tue, 01 Aug 2023 19:12:57 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Diffusion/gptDiffusionXL/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;sdxl&#34;&gt;&#xA;  SDXL&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sdxl&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/SDXL-0446aba46c8e400d8583fde17d8df264?pvs=4&#34;&gt;SDXL&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Work|å®æˆ˜)Image Editing</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/editing/gptDiffusionImageEditWork/</link>
      <pubDate>Thu, 27 Jul 2023 09:29:58 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/editing/gptDiffusionImageEditWork/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;æ€»ç»“&#34;&gt;&#xA;  æ€»ç»“&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%80%bb%e7%bb%93&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Prompt-to-Prompt&lt;br&gt;&#xA;train-freeï¼Œå¯Ÿè§‰åˆ°äº†attention mapçš„å¦™ç”¨&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;pix2pix-zero&lt;br&gt;&#xA;å¯Ÿè§‰åˆ°äº†attention mapçš„å¦™ç”¨&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;InstructPix2Pix&lt;br&gt;&#xA;trainableï¼Œtrainingæ•°æ®åŸºäºPrompt-to-Prompt&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MGIE&lt;br&gt;&#xA;åŸºäºLMM&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>CV ä»»åŠ¡</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Vision/gptVisionTask/</link>
      <pubDate>Tue, 25 Jul 2023 13:49:28 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Vision/gptVisionTask/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;åˆ†ç±»-1&#34;&gt;&#xA;  åˆ†ç±» [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%88%86%e7%b1%bb-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;image-level&#34;&gt;&#xA;  image-level&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#image-level&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;image recognition&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;(Retrieval)image-text retrieval&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Caption(image captioning)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;VQA(visual question answering)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;region-level&#34;&gt;&#xA;  region-level&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#region-level&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Object Detection object detection&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;DETR -&amp;gt; DINO -&amp;gt; Grounding DINO&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;dense caption&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;phrase grounding&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;pixel-level&#34;&gt;&#xA;  pixel-level&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pixel-level&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Segmentation&#xA;&lt;ul&gt;&#xA;&lt;li&gt;generic segmetation&lt;/li&gt;&#xA;&lt;li&gt;referring segmetation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;å…¶ä»–&#34;&gt;&#xA;  å…¶ä»–&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%85%b6%e4%bb%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å¯¹æ¯”&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;[CNN  æ›´æ·±çš„ç½‘ç»œ]&lt;/li&gt;&#xA;&lt;li&gt;[transformer æ²¡æœ‰å±€é™]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;CVä»»åŠ¡&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;åˆ†ç±»ï¼ˆClassificationï¼‰&lt;/li&gt;&#xA;&lt;li&gt;æ£€æµ‹ï¼ˆDetectionï¼‰&lt;/li&gt;&#xA;&lt;li&gt;åˆ†å‰²ï¼ˆSegmentationï¼‰&lt;/li&gt;&#xA;&lt;li&gt;è·Ÿè¸ªï¼ˆTrackingï¼‰&lt;/li&gt;&#xA;&lt;li&gt;è¡Œä¸ºè¯†åˆ«ï¼ˆAction Recognitionï¼‰&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;[&lt;a href=&#34;https://www.bilibili.com/video/BV1ds4y1k7pj/?vd_source=f6e8c1128f9f264c5ab8d9411a644036&#34;&gt;CVPR Tutorial Talk] Towards General Vision Understanding Interface&lt;/a&gt;&#xA;&lt;a href=&#34;https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Jianwei_CVPR2023_Tutorial.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(åŸç†|å®æˆ˜)DiT</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/gptDiffusionDiT/</link>
      <pubDate>Fri, 21 Jul 2023 13:59:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/gptDiffusionDiT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡&#34;&gt;&#xA;  è®ºæ–‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;è®ºæ–‡åœ°å€&lt;br&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2212.09748&#34;&gt;Scalable Diffusion Models with Transformers&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://www.wpeebles.com/DiT&#34;&gt;Scalable Diffusion Models with Transformers&lt;/a&gt; git&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT Repo&lt;/a&gt;  git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;arch&#34;&gt;&#xA;  Arch&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#arch&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://www.wpeebles.com/images/DiT/block.png&#34; alt=&#34;DiT&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;code10&#34;&gt;&#xA;  code[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#code10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;DiT&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/www6v/mnist-dits/blob/main/dit.py&#34;&gt;dit&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;li&gt;DiTBlock&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/www6v/mnist-dits/blob/main/dit_block.py&#34;&gt;dit_block&lt;/a&gt; git&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV12E421T7Zi/&#34;&gt;AIå¤§è®²å ‚ï¼šæ–‡ç”Ÿè§†é¢‘è°èƒ½æ•Œï¼Ÿä¸“ä¸šæ‹†è§£ã€DiTæ¨¡å‹ã€‘&lt;/a&gt; V&lt;br&gt;&#xA;DiTåŸæ–‡: &lt;a href=&#34;https://arxiv.org/abs/2212.09748&#34;&gt;https://arxiv.org/abs/2212.09748&lt;/a&gt;&lt;br&gt;&#xA;Code: &lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;https://github.com/facebookresearch/DiT&lt;/a&gt;&lt;br&gt;&#xA;Huggingface: &lt;a href=&#34;https://huggingface.co/spaces/wpeebles/DiT&#34;&gt;https://huggingface.co/spaces/wpeebles/DiT&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV12J4m1379T/&#34;&gt;14æ­¥æ‰‹æ“sora!Diffusion Transformer, DiTå·¥ä½œåŸç†&lt;/a&gt;  V&lt;/p&gt;&#xA;&lt;h3 id=&#34;å®æˆ˜&#34;&gt;&#xA;  å®æˆ˜&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;10&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV13K421h79z/&#34;&gt;ã€Soraé‡è¦æŠ€æœ¯ã€‘å¤ç°DiTï¼ˆDiffusion Transformerï¼‰æ¨¡å‹&lt;/a&gt; V ***&lt;br&gt;&#xA;&lt;a href=&#34;https://github.com/owenliang/mnist-dits&#34;&gt;dits Repo&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>(work|å®æˆ˜) fine-tuning</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionFineTuning/</link>
      <pubDate>Thu, 06 Jul 2023 19:24:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Controllable/gptDiffusionFineTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;å¯¹æ¯”æ€»ç»“1&#34;&gt;&#xA;  å¯¹æ¯”æ€»ç»“[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%af%b9%e6%af%94%e6%80%bb%e7%bb%931&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;è®­ç»ƒæ–¹æ³•&lt;/th&gt;&#xA;          &lt;th&gt;æ–¹æ³•&lt;/th&gt;&#xA;          &lt;th&gt;å±€é™æ€§&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Text Inversion&lt;/td&gt;&#xA;          &lt;td&gt;ä½¿ç”¨æä¾›çš„ä¸€ç»„å›¾ç‰‡&lt;strong&gt;è®­ç»ƒä¸€ä¸ªæ–°å•è¯çš„Embedding&lt;/strong&gt; ï¼Œå¹¶å°†å…¶ä¸è¯æ±‡è¡¨ä¸­çš„å·²æœ‰å•è¯å…³è”èµ·æ¥ï¼Œè¿™ä¸ªæ–°å•è¯å³ä¸ºè¿™ç»„å›¾ç‰‡æ¦‚å¿µçš„æŒ‡ä»£ã€‚&lt;/td&gt;&#xA;          &lt;td&gt;è®­ç»ƒè¿‡ç¨‹åªå¯¹åº” Embeddingï¼Œæ‰©æ•£æ¨¡å‹æ²¡æœ‰æ–°çŸ¥è¯†è¾“å…¥ï¼Œæ‰€ä»¥ä¹Ÿæ— æ³•äº§ç”Ÿæ–°çš„å†…å®¹ã€‚&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Full FineTune&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;æœ€æœ´ç´ &lt;/strong&gt;çš„æ–¹å¼ï¼Œä½¿ç”¨å›¾ç‰‡+ æ ‡æ³¨çš„æ•°æ®é›†ï¼Œè¿›è¡Œè¿­ä»£è®­ç»ƒï¼Œæ•°æ®é›†æ ‡æ³¨å¯ä»¥é€‰æ‹©BLIPæ¥ç”Ÿæˆã€‚è®­ç»ƒç›´æ¥å¯¹åŸæ¨¡å‹çš„æ‰€æœ‰æƒé‡è¿›è¡Œè°ƒæ•´ã€‚&lt;/td&gt;&#xA;          &lt;td&gt;å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¼è‡´ç”Ÿæˆå›¾ç‰‡çš„å¤šæ ·æ€§ä¸å¤Ÿï¼Œç»“æœéš¾ä»¥æ§åˆ¶ã€‚æ¨¡å‹ä½“ç§¯å¤§ï¼Œä¸ä¾¿äºä¼ æ’­ã€‚&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Dreambooth&lt;/td&gt;&#xA;          &lt;td&gt;æä¾›ä»£è¡¨æŸä¸ªæ–°æ¦‚å¿µï¼ˆinstanceï¼‰ å¯¹åº”çš„ä¸€ç»„å›¾åƒï¼Œå¹¶ä½¿ç”¨&lt;strong&gt;ç½•è§å­—ç¬¦ï¼ˆidentifierï¼‰&lt;/strong&gt; è¿›è¡Œæ¦‚å¿µMappingï¼Œè®­ç»ƒè¿‡ç¨‹å……åˆ†è€ƒè™‘&lt;strong&gt;åŸæœ‰ç›¸å…³ä¸»é¢˜ï¼ˆclassï¼‰ç”Ÿæˆ&lt;/strong&gt;ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚è®­ç»ƒç›´æ¥å¯¹&lt;strong&gt;åŸæ¨¡å‹çš„æ‰€æœ‰æƒé‡è¿›è¡Œè°ƒæ•´&lt;/strong&gt;ã€‚&lt;/td&gt;&#xA;          &lt;td&gt;è®­ç»ƒè¿‡ç¨‹åªé’ˆå¯¹æ–°æ¦‚å¿µ ï¼ˆinstanceï¼‰ï¼Œ&lt;strong&gt;å¤šæ ·æ€§å·®&lt;/strong&gt;ã€‚å¦‚æœéœ€è¦å¤šæ¦‚å¿µç”Ÿæˆï¼Œéœ€è¦å¤šæ¬¡è®­ç»ƒã€‚æ¨¡å‹ä½“ç§¯å¤§ï¼Œä¸ä¾¿äºä¼ æ’­ã€‚&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;LoRAï¼ˆw Dreamboothï¼‰&lt;/td&gt;&#xA;          &lt;td&gt;å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œåœ¨æ¯ä¸ªTransformerå—æ’å…¥&lt;strong&gt;å¯è®­ç»ƒå±‚&lt;/strong&gt;ï¼Œä¸éœ€è¦å®Œæ•´è°ƒæ•´ UNet æ¨¡å‹çš„å…¨éƒ¨å‚æ•°ã€‚&lt;strong&gt;è®­ç»ƒç»“æœåªä¿ç•™æ–°å¢çš„ç½‘ç»œå±‚ï¼Œæ¨¡å‹ä½“ç§¯å°&lt;/strong&gt;ã€‚&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;è®­ç»ƒæ•ˆæœä¸å¦‚Dreambooth&lt;/strong&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h1 id=&#34;å¯¹æ¯”æ€»ç»“2&#34;&gt;&#xA;  å¯¹æ¯”æ€»ç»“[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%af%b9%e6%af%94%e6%80%bb%e7%bb%932&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/fine-tuning.jpg&#34; alt=&#34;fine-tuning.jpg&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;å¯¹æ¯”æ€»ç»“3&#34;&gt;&#xA;  å¯¹æ¯”æ€»ç»“[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%af%b9%e6%af%94%e6%80%bb%e7%bb%933&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/gmcjfrdl.bmp&#34; alt=&#34;gmcjfrdl.bmp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2302436&#34;&gt;Stable Diffusion å¾®è°ƒåŠæ¨ç†ä¼˜åŒ–&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV19h4y1475y/&#34;&gt;ã€è®ºæ–‡ä¸²è¯»ã€‘Stable Diffusionæ¨¡å‹å¾®è°ƒæ–¹æ³•ä¸²è¯»&lt;/a&gt; V&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/669895990&#34;&gt;Stable Diffusionâ€”â€”å››ç§æ¨¡å‹ LoRAï¼ˆåŒ…æ‹¬LyCORISï¼‰ã€Embeddingsã€Dreamboothã€Hypernetwork&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;å®æˆ˜&#34;&gt;&#xA;  å®æˆ˜&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1184y1g7pG/?p=4&#34;&gt;Text Inversion&lt;/a&gt; V&#xA;ã€è¿™ä¸ªæ¯”è¾ƒè¯¦ç»†ã€‘&#xA;1xx. &lt;a href=&#34;https://www.bilibili.com/video/BV1184y1g7pG?p=7&#34;&gt;lora Dreambooth&lt;/a&gt; V&#xA;ã€å†»ç»“ä¸è®­ç»ƒunetï¼Œåªè®­ç»ƒloraã€‘&#xA;ã€ä¸ºunetæ¨¡å‹æ·»åŠ æ³¨æ„åŠ›å±‚ï¼Œæ³¨æ„åŠ›å±‚æ˜¯è¦è®­ç»ƒçš„å‚æ•°ã€‘&#xA;ã€å¤§éƒ¨åˆ†ä»£ç å’ŒDreamboothå·®ä¸å¤šã€‘&lt;/p&gt;&#xA;&lt;h3 id=&#34;å®æˆ˜-1&#34;&gt;&#xA;  å®æˆ˜&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1xx.  &lt;a href=&#34;https://www.notion.so/concept-customization-067033e842b044729d81aed1d96608fd?pvs=21&#34;&gt;+ concept customization&lt;/a&gt;  &lt;br&gt;&#xA;dreambooth lora + textual_inversion   diffusers&lt;/p&gt;</description>
    </item>
    <item>
      <title>(ç»¼è¿°)Image Editing</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/editing/gptDiffusionImageEdit/</link>
      <pubDate>Thu, 06 Jul 2023 19:10:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/editing/gptDiffusionImageEdit/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;ç›®å½•&#34;&gt;&#xA;  ç›®å½•&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡&#34;&gt;&#xA;  è®ºæ–‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;è®ºæ–‡åœ°å€&#xA;&lt;a href=&#34;https://arxiv.org/abs/2402.17525&#34;&gt;ã€ŠDiffusion Model-Based Image Editing: A Surveyã€‹&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods&#34;&gt;Repo&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;å›¾åƒç¼–è¾‘1&#34;&gt;&#xA;  å›¾åƒç¼–è¾‘[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%9b%be%e5%83%8f%e7%bc%96%e8%be%911&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;å¤§ç±»&#34;&gt;&#xA;  å¤§ç±»&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%a7%e7%b1%bb&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ä»å›¾ç‰‡ç¼–è¾‘çš„ä»»åŠ¡æ–¹é¢å¯ä»¥è¢«åˆ†ä¸º3ä¸ªå¤§ç±»&#xA;&lt;ul&gt;&#xA;&lt;li&gt;è¯­ä¹‰ç¼–è¾‘semantic editing&lt;/li&gt;&#xA;&lt;li&gt;é£æ ¼ç¼–è¾‘stylistic editing&lt;/li&gt;&#xA;&lt;li&gt;ç»“æ„ç¼–è¾‘structural editing&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;approaches&#34;&gt;&#xA;  APPROACHES&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#approaches&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;TRAINING-BASED APPROACHES&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;InstructPix2Pix&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;TESTING-TIME FINETUNING APPROACHES&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;TRAINING AND FINETUNING FREE APPROACHES&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡2&#34;&gt;&#xA;  è®ºæ–‡[2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%872&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;è®ºæ–‡åœ°å€&#xA;ã€ŠA Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Modelsã€‹ å¤æ—¦ã€å—æ´‹ç†å·¥&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://github.com/xinchengshuai/Awesome-Image-Editing&#34;&gt;A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Agent - UI-assistants</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Multimodal-Agent/gptAgentMultimodalApp/</link>
      <pubDate>Fri, 30 Jun 2023 21:12:35 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Multimodal-Agent/gptAgentMultimodalApp/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;app-agent&#34;&gt;&#xA;  App Agent&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#app-agent&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/677071947&#34;&gt;AppAgentæºç åˆ†æ&amp;amp;æ€è€ƒ&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/mnotgod96/AppAgent&#34;&gt;https://github.com/mnotgod96/AppAgent&lt;/a&gt;&#xA;&lt;a href=&#34;https://icoz69.github.io/&#34;&gt;https://icoz69.github.io/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/681424409&#34;&gt;ã€LLM-agentã€‘MOBILE-AGENT: å…·æœ‰è§†è§‰æ„ŸçŸ¥èƒ½åŠ›çš„è‡ªæ²»å¤šæ¨¡ç§»åŠ¨è®¾å¤‡agent&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/X-PLUG/MobileAgent&#34;&gt;https://github.com/X-PLUG/MobileAgent&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://github.com/OpenAdaptAI/OpenAdapt&#34;&gt;https://github.com/OpenAdaptAI/OpenAdapt&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)Stable Diffusion</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Diffusion/gptMultimodalDiffusion/</link>
      <pubDate>Thu, 29 Jun 2023 17:28:21 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Diffusion/gptMultimodalDiffusion/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;stable-diffusion-åŸç†&#34;&gt;&#xA;  Stable Diffusion åŸç†&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#stable-diffusion-%e5%8e%9f%e7%90%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Diffusion-8cdcc8079d7d42c2a193ae6baf06246e?pvs=4&#34;&gt;(åŸç†)Stable Diffusion&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(survey)å¤šæ¨¡æ€  æ•°æ®é›†</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Data/gptDatasetMulitmodal/</link>
      <pubDate>Sat, 01 Apr 2023 15:09:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Data/gptDatasetMulitmodal/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;ç›®å½•&#34;&gt;&#xA;  ç›®å½•&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;survey0&#34;&gt;&#xA;  Survey[0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#survey0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Pre-training&lt;/li&gt;&#xA;&lt;li&gt;Adaptation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;pre-trainingæ•°æ®é›†&#34;&gt;&#xA;  Pre-trainingæ•°æ®é›†&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pre-training%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;LAION[1]&#xA;&lt;a href=&#34;https://laion.ai/projects/&#34;&gt;LAION&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;wukong[1]&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/473794131&#34;&gt;[è®ºæ–‡]ä¸­æ–‡å¤šæ¨¡æ€æ•°æ®é›†WuKong &amp;amp; FILIP &amp;amp; LiT-tuning&lt;/a&gt;&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/551622338&#34;&gt;Wukongï¼šä¸€äº¿è§„æ¨¡çš„ä¸­æ–‡è·¨æ¨¡æ€é¢„è®­ç»ƒåŸºå‡†&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MMDialog&#xA;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/584894471&#34;&gt;ç™¾ä¸‡é‡çº§çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®é›†æ¥äº†ï¼Œ153ä¸‡å¼ å›¾ç‰‡4000å¤šä¸»é¢˜&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;OBELISC[2]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ShareGPT4V[3]&#xA;opensource&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;sftæ•°æ®é›†&#34;&gt;&#xA;  SFTæ•°æ®é›†&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sft%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LAMM&lt;/li&gt;&#xA;&lt;li&gt;MultiIntruct&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;å‚è€ƒ&#34;&gt;&#xA;  å‚è€ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;survey&#34;&gt;&#xA;  survey&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#survey&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol start=&#34;0&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/_fi2odhKITs4fs7MbWpWaw&#34;&gt;å¤šæ¨¡æ€æ¨¡å‹å¤§å¸¸ç”¨æ•°æ®é›†åŠå¤„ç†ç­–ç•¥ï¼šå…¼çœ‹Chatlawæ³•å¾‹é—®ç­”ä¸­çš„çŸ¥è¯†å›¾è°±èåˆæ€è·¯ &lt;/a&gt;&#xA;ã€ŠA Survey of Multimodal Large Language Model from A Data-centric Perspectiveã€‹&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;é¢„è®­ç»ƒæ•°æ®é›†&#34;&gt;&#xA;  é¢„è®­ç»ƒæ•°æ®é›†&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e9%a2%84%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/686757824&#34;&gt;å¤šæ¨¡æ€æ•°æ®é›†æ”¶é›†&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/670149958&#34;&gt;[è®ºæ–‡é˜…è¯»] å¼€æºçš„å¤šæ¨¡æ€æ–‡æ¡£æ•°æ®é›†ï¼ŒOBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents&lt;/a&gt;&lt;br&gt;&#xA;ä»ç½‘é¡µæ–‡æ¡£é‡Œå¾—åˆ°çš„æ•°æ®é›†&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/669485001&#34;&gt;è¶…è¶ŠåŒçº§7Bæ¨¡å‹ï¼ ä¸­å›½å›¢é˜Ÿå¼€æºå¤§è§„æ¨¡é«˜è´¨é‡å›¾æ–‡æ•°æ®é›†ShareGPT4Vï¼Œå¤§å¹…æå‡å¤šæ¨¡æ€æ€§èƒ½&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V&#34;&gt;ShareGPT4V&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;1xx. &lt;a href=&#34;https://zhuanlan.zhihu.com/p/527182857&#34;&gt;å¤šæ¨¡æ€é¢„è®­ç»ƒæ•°æ®é›†&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Survey)å¤šæ¨¡æ€</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Survey/gptMultimodalSurvey/</link>
      <pubDate>Thu, 16 Mar 2023 12:45:30 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Survey/gptMultimodalSurvey/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;ç›®å½•&#34;&gt;&#xA;  ç›®å½•&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡&#34;&gt;&#xA;  è®ºæ–‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;è®ºæ–‡åœ°å€&#xA;ã€ŠMultimodal Foundation Models:From Specialists to General-Purpose Assistantsã€‹ .Sep 2023   - microsoft&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings&#34;&gt;Computer Vision in the Wild (CVinW)&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;overview-0&#34;&gt;&#xA;  overview [0]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview-0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/overview.jpeg&#34; alt=&#34;overview.jpeg&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;è§†è§‰ç†è§£-1&#34;&gt;&#xA;  è§†è§‰ç†è§£ [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a7%86%e8%a7%89%e7%90%86%e8%a7%a3-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/understanding.png&#34; alt=&#34;understanding.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/understanding-method.png&#34; alt=&#34;understanding-method.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;è§†è§‰ç”Ÿæˆ-1&#34;&gt;&#xA;  è§†è§‰ç”Ÿæˆ [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%a7%86%e8%a7%89%e7%94%9f%e6%88%90-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;human-alignments-in-visual-generation--10&#34;&gt;&#xA;  Human Alignments in Visual Generation  [10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#human-alignments-in-visual-generation--10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;å››ç§alignmentçš„æ–¹å¼&lt;/p&gt;&#xA;&lt;h3 id=&#34;spatial-controllable-t2i-generation&#34;&gt;&#xA;  spatial controllable T2I generation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#spatial-controllable-t2i-generation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;ç»“åˆä½ç½®åˆ†å¸ƒçš„æ–‡å­—æè¿°&lt;/strong&gt;ï¼ˆæ¯”è¾ƒéº»çƒ¦çš„ç”¨æˆ·äº¤äº’ï¼Œä¸ä»…éœ€è¦æ–‡å­—ï¼Œè€Œä¸”éœ€è¦ä½ç½®ï¼‰ï¼Œå¸¸ç”¨äº&lt;strong&gt;å¯¹ä½ç½®è¦æ±‚æ¯”è¾ƒé«˜çš„åˆ›æ„è®¾è®¡ï¼ˆæµ·æŠ¥ç­‰ï¼‰&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(å›¾ç”Ÿæ–‡)BLIP-2, Flamingo</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalBlip/</link>
      <pubDate>Wed, 15 Mar 2023 23:00:59 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalBlip/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;ç›®å½•&#34;&gt;&#xA;  ç›®å½•&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;blip-2&#34;&gt;&#xA;  BLIP-2&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#blip-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;overview-1&#34;&gt;&#xA;  Overview [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;ç”¨ä¸€ä¸ªQformeræ¥æå–å›¾åƒç‰¹å¾ï¼ˆç­‰åŒä¸Flamingoçš„perceiver resamplerï¼‰ï¼Œç„¶åç”¨cross- attentionè¿›è¡Œå¤šæ¨¡æ€äº¤äº’ï¼Œæ­¤æ—¶è§†è§‰ç¼–ç å™¨å’ŒLLMéƒ½ä¼šè¢«å†»ç»“ï¼Œ&lt;strong&gt;åªè®­ç»ƒQformer&lt;/strong&gt;ï¼Œè€Œåœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ—¶ï¼Œå¯ä»¥å†è§£é”è§†è§‰ç¼–ç å™¨ï¼Œè®©å®ƒè·ŸQformerä¸€èµ·è®­ç»ƒ&lt;/p&gt;&#xA;&lt;h3 id=&#34;ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥-1&#34;&gt;&#xA;  ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e4%b8%a4%e9%98%b6%e6%ae%b5%e7%9a%84%e8%ae%ad%e7%bb%83%e7%ad%96%e7%95%a5-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;BLIP-2è®¾è®¡äº†ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼Œä»¥ä½¿è§†è§‰ç¼–ç å™¨èƒ½å­¦ä¼šæå–æ›´å…³é”®çš„ä¿¡æ¯ã€‚&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨å¤šç§é¢„è®­ç»ƒä»»åŠ¡ï¼Œå¦‚Image-Text Contrastive Learning(&lt;strong&gt;ITC&lt;/strong&gt;)ï¼ŒImage-grounded Text Generation(&lt;strong&gt;ITG&lt;/strong&gt;)ï¼ŒImage-Text Matching(&lt;strong&gt;ITM&lt;/strong&gt;)è®©Qformerå­¦ä¼šå¦‚ä½•ä»&lt;strong&gt;è§†è§‰ç¼–ç å™¨ä¸­æŠ½å–æ–‡æœ¬ç›¸å…³çš„ç‰¹å¾&lt;/strong&gt;ã€‚&lt;/li&gt;&#xA;&lt;li&gt;ç¬¬äºŒé˜¶æ®µï¼Œå°†Qformeræ’å…¥åˆ°LLMsä¸­ï¼Œç”¨language modelingè¿›è¡Œè®­ç»ƒã€‚&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;æ¶æ„3&#34;&gt;&#xA;  æ¶æ„[3]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%843&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒ&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;é˜¶æ®µä¸€&#xA;è·å¾—é«˜è´¨é‡çš„ &lt;strong&gt;å›¾æ–‡å¯¹é½å‘é‡è¡¨å¾&lt;/strong&gt;&#xA;é€šè¿‡&lt;strong&gt;ITC ITM  ITG ä¸‰ä¸ªæŸå¤±å‡½æ•°&lt;/strong&gt;è·å¾—äº†å¾ˆå¥½çš„å›¾ç‰‡æ–‡æœ¬ &lt;strong&gt;å¯¹é½å‘é‡è¡¨å¾èƒ½åŠ›&lt;/strong&gt;ï¼Œä»…è®­ç»ƒ&lt;strong&gt;Qformer&lt;/strong&gt;ä¸­å¾ˆå°‘çš„å‚æ•°&#xA;ã€ITM:  image-text æ˜¯å¦æ˜¯åŒ¹é…çš„ |    image å’Œtext éƒ½èƒ½ç›¸äº’çœ‹åˆ°ã€‘&#xA;ã€ITG: imageç”Ÿæˆtext |    image èƒ½å…¨çœ‹åˆ°, textåªèƒ½é€ä¸ªçš„çœ‹ã€‘&#xA;ã€ITC: imageå’Œtextçš„å¯¹æ¯”å­¦ä¹ , å¯¹æ¯”å­¦ä¹ åˆ†ç±»åˆ†é”™äº†çš„  é€å…¥ITM è´Ÿæ ·æœ¬ |  imageå’Œ text  ä¹‹é—´æ˜¯ä¸èƒ½çœ‹åˆ°çš„ã€‘&lt;/li&gt;&#xA;&lt;li&gt;é˜¶æ®µäºŒ&#xA;é€šè¿‡å‘é‡è¡¨å¾è¿›è¡Œ&lt;strong&gt;æ–‡å­—ç”Ÿæˆ&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;code-2&#34;&gt;&#xA;  code [2]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#code-2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;flamingo1&#34;&gt;&#xA;  Flamingo[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#flamingo1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;æ¶æ„&#34;&gt;&#xA;  æ¶æ„&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;å®ƒåœ¨Frozenæ¨¡å‹çš„åŸºç¡€ä¸Šåšè¿›ä¸€æ­¥çš„æ”¹è¿›ï¼Œä¸åŒç‚¹ä¸»è¦æœ‰ä¸¤ä¸ªï¼šä¸€æ˜¯ä½¿ç”¨äº†æ›´å¤§çš„LLMsï¼ŒäºŒæ˜¯&lt;strong&gt;å†»ç»“è§†è§‰ç¼–ç å™¨&lt;/strong&gt;ï¼Œå¼•å…¥&lt;strong&gt;perceiver resampler&lt;/strong&gt;å’Œ&lt;strong&gt;XAttn-Dense&lt;/strong&gt;ä¸¤ä¸ªé€‚é…å•å…ƒä½œä¸ºå¯è®­ç»ƒçš„æ¨¡å—ã€‚&lt;/p&gt;</description>
    </item>
    <item>
      <title>(ç»¼è¿°)å¤šæ¨¡æ€InstructTuning</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalInstructTuning/</link>
      <pubDate>Wed, 15 Mar 2023 16:09:00 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalInstructTuning/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;ç›®å½•&#34;&gt;&#xA;  ç›®å½•&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;datasets-for-visual-instruction-tuning1&#34;&gt;&#xA;  Datasets for Visual Instruction Tuning[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#datasets-for-visual-instruction-tuning1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;single-turn&#34;&gt;&#xA;  Single-turn&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#single-turn&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MiniGPT-4&#xA;&lt;strong&gt;MiniGPT-4&lt;/strong&gt; [37] curates an image description dataset that contains 3439 image-text pairs for instruction fine-tuning. MiniGPT-4 &lt;strong&gt;randomly selects 5000 images from the Conceptual Caption dataset&lt;/strong&gt; [38], [39] and prompts its &lt;strong&gt;pre-trained VLM model&lt;/strong&gt; to generate detailed descriptions for each image. The generated descriptions are then** refined and filtered** both manually and by using ChatGPT, resulting in 3439 highquality image-text pairs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†|å®æˆ˜)MiniGPT4</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalMinigpt4/</link>
      <pubDate>Wed, 15 Mar 2023 15:56:48 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalMinigpt4/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;ç›®å½•&#34;&gt;&#xA;  ç›®å½•&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;introduction1&#34;&gt;&#xA;  INTRODUCTION[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#introduction1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;MiniGPT-4 å¢åŠ äº†ä¸€ä¸ª&lt;strong&gt;æŠ•å½±å±‚&lt;/strong&gt;ï¼Œå°†&lt;strong&gt;ç¼–ç çš„è§†è§‰ç‰¹å¾ä¸ Vicuna è¯­è¨€æ¨¡å‹å¯¹é½&lt;/strong&gt;ï¼Œå¹¶&lt;strong&gt;å†»ç»“äº†æ‰€æœ‰å…¶ä»–è§†è§‰å’Œè¯­è¨€ç»„ä»¶&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;method1&#34;&gt;&#xA;  METHOD[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#method1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å›¾ 1&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;MiniGPT-4 çš„ç›®æ ‡æ˜¯å°†æ¥è‡ªé¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨çš„è§†è§‰ä¿¡æ¯ä¸å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½ï¼ˆAlignmentï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œ&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ä½¿ç”¨ &lt;strong&gt;Vicunaä½œä¸ºè¯­è¨€è§£ç å™¨&lt;/strong&gt;ï¼Œè¯¥è§£ç å™¨åŸºäº LLaMAæ„å»ºï¼Œå¯ä»¥æ‰§è¡Œå„ç§å¤æ‚çš„è¯­è¨€ä»»åŠ¡ã€‚&lt;/li&gt;&#xA;&lt;li&gt;è§†è§‰æ„ŸçŸ¥æ–¹ï¼šé‡‡ç”¨ä¸ &lt;strong&gt;BLIP-2&lt;/strong&gt; ç›¸åŒçš„&lt;strong&gt;è§†è§‰ç¼–ç å™¨&lt;/strong&gt;ï¼Œ&lt;strong&gt;ViT Backbone&lt;/strong&gt;åŠå…¶é¢„å…ˆè®­ç»ƒå¥½çš„ &lt;strong&gt;Q-Former&lt;/strong&gt;ã€‚&#xA;è¯­è¨€å’Œè§†è§‰æ¨¡å‹éƒ½æ˜¯å¼€æºçš„ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ©ç”¨çº¿æ€§æŠ•å½±å±‚å¼¥åˆè§†è§‰ç¼–ç å™¨ä¸ LLM ä¹‹é—´çš„å·®è·ï¼Œå›¾ 1 æ˜¾ç¤ºäº†æ¨¡å‹æ¦‚è§ˆã€‚&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;first-pretraining-stage&#34;&gt;&#xA;  FIRST &lt;strong&gt;PRETRAINING&lt;/strong&gt; STAGE&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#first-pretraining-stage&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;ç¬¬ä¸€é˜¶æ®µï¼šåœ¨å¤§é‡å¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹ä¸Šå¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥è·å–è§†è§‰è¯­è¨€çŸ¥è¯†ã€‚&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Traditional alignment method [2]&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Input: Image&lt;/li&gt;&#xA;&lt;li&gt;Output: Caption&lt;/li&gt;&#xA;&lt;li&gt;Training Objective: Maximize the likelihood of GT captions&lt;/li&gt;&#xA;&lt;li&gt;Training Dataset ç»„åˆæ•°æ®é›† [postprocessed by BLIP]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Conceptual Caption&lt;/li&gt;&#xA;&lt;li&gt;SBU&lt;/li&gt;&#xA;&lt;li&gt;LAION&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;curating-a-high-quality-alignment-dataset-for-vision-language-domain&#34;&gt;&#xA;  CURATING A &lt;strong&gt;HIGH-QUALITY ALIGNMENT DATASET&lt;/strong&gt; FOR VISION-LANGUAGE DOMAIN.&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#curating-a-high-quality-alignment-dataset-for-vision-language-domain&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Create a dataset with detailed, human-perfered descriptions[2][1]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;model  generates descriptions&#xA;åœ¨åˆå§‹é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ä»ç¬¬ä¸€ä¸ªé¢„è®­ç»ƒé˜¶æ®µå¾—åˆ°çš„æ¨¡å‹æ¥&lt;strong&gt;ç”Ÿæˆè¾“å…¥å›¾åƒçš„æè¿°&lt;/strong&gt;ã€‚&lt;/li&gt;&#xA;&lt;li&gt;polishing and filtering by chatgpt&#xA;ä¸Šè¿°è‡ªåŠ¨ç”Ÿæˆçš„å›¾ç‰‡è¯´æ˜åŒ…å«&lt;strong&gt;å™ªéŸ³æˆ–ä¸è¿è´¯çš„æè¿°&lt;/strong&gt;ï¼Œä¾‹å¦‚å•è¯æˆ–å¥å­é‡å¤ï¼Œå¥å­æ”¯ç¦»ç ´ç¢æˆ–å†…å®¹ä¸ç›¸å…³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†&lt;strong&gt;ChatGPT&lt;/strong&gt;ï¼Œé€šè¿‡ä»¥ä¸‹æç¤ºå¯¹æè¿°è¿›è¡Œ&lt;strong&gt;ä¿®è¡¥&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;further polishing and filtering by rules &amp;amp; human&#xA;å®Œæˆåå¤„ç†é˜¶æ®µåï¼Œæˆ‘ä»¬ä¼šæ‰‹åŠ¨éªŒè¯æ¯å¼ å›¾ç‰‡è¯´æ˜çš„æ­£ç¡®æ€§ï¼Œä»¥ä¿è¯å…¶é«˜è´¨é‡ã€‚&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;second-stage-finetuning&#34;&gt;&#xA;  SECOND-STAGE &lt;strong&gt;FINETUNING&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#second-stage-finetuning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ä¸€ä¸ªè¾ƒå°ä½†é«˜è´¨é‡çš„å›¾åƒ-æ–‡æœ¬æ•°æ®é›†å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶è®¾è®¡äº†å¯¹è¯æ¨¡æ¿ï¼Œä»¥æé«˜ç”Ÿæˆçš„å¯é æ€§å’Œå¯ç”¨æ€§ã€‚&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;ã€blip2èƒ½è¯†åˆ«å›¾åƒï¼Œä½†æ˜¯å¯¹è¯èƒ½åŠ›æ¯”è¾ƒå¼±ï¼Œä¸èƒ½è¯´å‡ºå›¾åƒä¸­çš„ç»†èŠ‚ã€‚åœ¨pre-trainé˜¶æ®µè·å–è§†è§‰è¯­è¨€çŸ¥è¯†ï¼Œ åœ¨fine-tuning é˜¶æ®µè·å–å¯¹è¯èƒ½åŠ›ã€‘  [2]&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†|å®æˆ˜) LLaVa æ¼”åŒ–</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalLlava/</link>
      <pubDate>Tue, 14 Mar 2023 23:02:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalLlava/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;llava-æ¼”åŒ–&#34;&gt;&#xA;  LLaVa æ¼”åŒ–&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llava-%e6%bc%94%e5%8c%96&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/LLaVa-cef875377c394636a64cf57edbb0026e?pvs=4&#34;&gt;(åŸç†|å®æˆ˜) LLaVa æ¼”åŒ–&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;llava-å®æˆ˜&#34;&gt;&#xA;  LLaVa å®æˆ˜&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#llava-%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/LLaVa-0bf9f127dc7c41e796050bcb8f7fb1b3?pvs=4&#34;&gt;(å®æˆ˜) LLaVa &lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>(åŸç†)Web Agent</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Multimodal-Agent/gptAgentWeb/</link>
      <pubDate>Sun, 05 Mar 2023 10:30:03 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Multimodal-Agent/gptAgentWeb/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;web-scenarios-1&#34;&gt;&#xA;  web scenarios [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#web-scenarios-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;åœ¨ç½‘ç»œåœºæ™¯ä¸­ï¼Œä»£è¡¨ç”¨æˆ·æ‰§è¡Œç‰¹å®šä»»åŠ¡è¢«ç§°ä¸ºWebå¯¼èˆªé—®é¢˜[390]ã€‚ä»£ç†ç¨‹åºè§£é‡Šç”¨æˆ·æŒ‡ä»¤ï¼Œå°†å…¶åˆ†è§£ä¸ºå¤šä¸ªåŸºæœ¬æ“ä½œï¼Œå¹¶ä¸è®¡ç®—æœºè¿›è¡Œäº¤äº’ã€‚è¿™é€šå¸¸æ¶‰åŠåˆ°å¡«å†™è¡¨å•ã€åœ¨çº¿è´­ç‰©å’Œå‘é€ç”µå­é‚®ä»¶ç­‰ç½‘ç»œä»»åŠ¡ã€‚ä»£ç†ç¨‹åºéœ€è¦å…·å¤‡ç†è§£å¤æ‚ç½‘ç»œåœºæ™¯ä¸­çš„æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œé€‚åº”å˜åŒ–ï¼ˆå¦‚å˜ˆæ‚çš„æ–‡æœ¬å’ŒåŠ¨æ€HTMLç½‘é¡µï¼‰ï¼Œå¹¶æ¨å¹¿æˆåŠŸçš„æ“ä½œ[391]ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä»£ç†ç¨‹åºå¯ä»¥åœ¨å¤„ç†æœªçŸ¥ä»»åŠ¡æ—¶å®ç°å¯è®¿é—®æ€§å’Œè‡ªåŠ¨åŒ–[435]ï¼Œæœ€ç»ˆä½¿äººç±»å…äºä¸è®¡ç®—æœºç”¨æˆ·ç•Œé¢çš„é‡å¤äº¤äº’ã€‚&lt;/p&gt;&#xA;&lt;p&gt;é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ä»£ç†ç¨‹åºå¯ä»¥æœ‰æ•ˆåœ°æ¨¡ä»¿äººç±»è¡Œä¸ºï¼Œä½¿ç”¨é¢„å®šä¹‰çš„æ“ä½œï¼Œå¦‚é”®å…¥ã€æœç´¢ã€å¯¼èˆªåˆ°ä¸‹ä¸€é¡µç­‰ã€‚å®ƒä»¬åœ¨åŸºæœ¬ä»»åŠ¡ï¼ˆå¦‚åœ¨çº¿è´­ç‰©[392]å’Œæœç´¢å¼•æ“æ£€ç´¢[90]ï¼‰ä¸­è¡¨ç°è‰¯å¥½ï¼Œè¿™äº›ä»»åŠ¡å·²ç»å¾—åˆ°å¹¿æ³›æ¢ç´¢ã€‚ç„¶è€Œï¼Œæ²¡æœ‰è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„ä»£ç†ç¨‹åºå¯èƒ½éš¾ä»¥é€‚åº”ç°å®ä¸–ç•Œäº’è”ç½‘ä¸­æ›´çœŸå®å’Œå¤æ‚çš„åœºæ™¯ã€‚åœ¨åŠ¨æ€ã€å†…å®¹ä¸°å¯Œçš„ç½‘é¡µä¸Šï¼Œå¦‚åœ¨çº¿è®ºå›æˆ–åœ¨çº¿ä¸šåŠ¡ç®¡ç†[391]ï¼Œä»£ç†ç¨‹åºå¸¸å¸¸é¢ä¸´æ€§èƒ½æ–¹é¢çš„æŒ‘æˆ˜ã€‚&lt;/p&gt;&#xA;&lt;p&gt;ä¸ºäº†å®ç°ä»£ç†ç¨‹åºä¸æ›´çœŸå®çš„ç½‘é¡µä¹‹é—´çš„æˆåŠŸäº¤äº’ï¼Œä¸€äº›ç ”ç©¶äººå‘˜[393ï¼›394]å¼€å§‹åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å¼ºå¤§HTMLè¯»å–å’Œç†è§£èƒ½åŠ›ã€‚é€šè¿‡è®¾è®¡æç¤ºï¼Œä»–ä»¬è¯•å›¾ä½¿ä»£ç†ç¨‹åºç†è§£æ•´ä¸ªHTMLæºä»£ç ï¼Œå¹¶é¢„æµ‹æ›´åˆç†çš„ä¸‹ä¸€æ­¥æ“ä½œã€‚Mind2Web[389]ç»“åˆäº†ä¸ºHTMLè¿›è¡Œå¾®è°ƒçš„å¤šä¸ªè¯­è¨€æ¨¡å‹ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œçš„åœºæ™¯ä¸­æ€»ç»“å†—é•¿çš„HTMLä»£ç [388]å¹¶æå–æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒWebGum[390]é€šè¿‡ä½¿ç”¨åŒ…å«HTMLæˆªå±çš„å¤šæ¨¡æ€è¯­æ–™åº“ï¼Œèµ‹äºˆä»£ç†ç¨‹åºè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚å®ƒåŒæ—¶è¿›è¡Œäº†è¯­è¨€æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨çš„å¾®è°ƒï¼ŒåŠ æ·±äº†ä»£ç†ç¨‹åºå¯¹ç½‘é¡µçš„å…¨é¢ç†è§£ã€‚&lt;/p&gt;&#xA;&lt;p&gt;Performing specific tasks on behalf of users in a web scenario is known as the web navigation problem [390]. Agents interpret user instructions, break them down into multiple basic operations, and interact with computers. This often includes web tasks such as filling out forms, online shopping, and sending emails. Agents need to possess the ability to understand instructions within complex web scenarios, adapt to changes (such as noisy text and dynamic HTML web pages), and generalize successful operations [391]. In this way, agents can achieve accessibility and automation when dealing with unseen tasks in the future [435], ultimately freeing humans from repeated interactions with computer UIs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)å¤šæ¨¡æ€é¢„è®­ç»ƒ æ¦‚è¿°</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalPretrain/</link>
      <pubDate>Sat, 04 Mar 2023 13:23:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83LLMLMM/gptMultimodalPretrain/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;ç›®å½•&#34;&gt;&#xA;  ç›®å½•&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;overview&#34;&gt;&#xA;  Overview&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#overview&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/overview.png&#34; alt=&#34;overview.png&#34; /&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;å¤šæ¨¡æ€é¢„è®­ç»ƒ&#34;&gt;&#xA;  å¤šæ¨¡æ€é¢„è®­ç»ƒ&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%9a%e6%a8%a1%e6%80%81%e9%a2%84%e8%ae%ad%e7%bb%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;æ•°æ®é›†&#34;&gt;&#xA;  æ•°æ®é›†&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%95%b0%e6%8d%ae%e9%9b%86&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;å¤§è§„æ¨¡æ— æ ‡æ³¨&lt;/li&gt;&#xA;&lt;li&gt;å†…å®¹æ‚  å™ªéŸ³å¤š&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;æ¶æ„transformer&#34;&gt;&#xA;  æ¶æ„Transformer&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84transformer&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;åŸºäºtransformer encoder-ç†è§£ä»»åŠ¡&#xA;å•æµ - vl-bert  UNITER&#xA;åŒæµ - ViLBERTï¼Œ CLIPï¼ˆåŒæµç»“æ„ï¼Œå¯¹æ¯”å­¦ä¹ ï¼‰&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;åŸºäºtransformer decoder-ç”Ÿæˆä»»åŠ¡&#xA;DALL-E  ï¼ˆVQVAE+GPT,  Text-to-Image Generationï¼‰&#xA;ç°åœ¨éƒ½ç”¨ â†’ SD æ‰©æ•£æ¨¡å‹&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;åŸºäºencoder+decoder-ç†è§£+ç”Ÿæˆ&#xA;æ–‡æœ¬çš„decoder&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;encoder + decoder ä¸²è¡Œ,  äº¤å‰æ³¨æ„åŠ›&lt;/li&gt;&#xA;&lt;li&gt;encoder + decoder å¹¶è¡Œ&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;æ¨¡å‹---è‡ªç›‘ç£å­¦ä¹ &#34;&gt;&#xA;  æ¨¡å‹ - è‡ªç›‘ç£å­¦ä¹ &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%a8%a1%e5%9e%8b---%e8%87%aa%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;æ¨¡æ€å†…æ©ç å­¦ä¹ &#xA;æ–‡æœ¬ è¯­éŸ³ è§†è§‰è‡ªèº«tokençº§åˆ«mask&lt;/p&gt;</description>
    </item>
    <item>
      <title>(åŸç†)Agent å¤šæ¨¡æ€</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Multimodal-Agent/gptAgentMultimodal/</link>
      <pubDate>Tue, 21 Feb 2023 10:10:51 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Multimodal-Agent/gptAgentMultimodal/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;ç›®å½•&#34;&gt;&#xA;  ç›®å½•&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡&#34;&gt;&#xA;  è®ºæ–‡&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;è®ºæ–‡åœ°å€&#xA;&lt;a href=&#34;https://arxiv.org/abs/2402.15116&#34;&gt;ã€ŠLarge Multimodal Agents: A Surveyã€‹&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å¼€æºåœ°å€&#xA;&lt;a href=&#34;https://github.com/jun0wanan/awesome-large-multimodal-agents&#34;&gt;Repo&lt;/a&gt; git&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;survey&#34;&gt;&#xA;  Survey&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#survey&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;ç±»å‹-iæ— é•¿æœŸè®°å¿†çš„é—­æº-llms-ä½œä¸ºè§„åˆ’å™¨&#34;&gt;&#xA;  ç±»å‹ Iï¼šæ— é•¿æœŸè®°å¿†çš„é—­æº LLMs ä½œä¸ºè§„åˆ’å™¨ã€‚&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%b1%bb%e5%9e%8b-i%e6%97%a0%e9%95%bf%e6%9c%9f%e8%ae%b0%e5%bf%86%e7%9a%84%e9%97%ad%e6%ba%90-llms-%e4%bd%9c%e4%b8%ba%e8%a7%84%e5%88%92%e5%99%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.04671.pdf&#34;&gt;&lt;strong&gt;Visual ChatGPT&lt;/strong&gt;&lt;/a&gt;  ***&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.11381.pdf&#34;&gt;&lt;strong&gt;MM-REACT&lt;/strong&gt;&lt;/a&gt;  ***&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.08128.pdf&#34;&gt;&lt;strong&gt;ViperGPT&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.17580.pdf&#34;&gt;&lt;strong&gt;HuggingGPT&lt;/strong&gt;&lt;/a&gt;  ***&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.09842.pdf&#34;&gt;&lt;strong&gt;Chameleon&lt;/strong&gt;&lt;/a&gt; ***&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.00571.pdf&#34;&gt;&lt;strong&gt;LLaVA-Interactive&lt;/strong&gt;&lt;/a&gt; ***&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.01614&#34;&gt;&lt;strong&gt;SeeAct&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.01415.pdf&#34;&gt;&lt;strong&gt;GPT-Driver&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.16158.pdf&#34;&gt;&lt;strong&gt;Mobile-Agent&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;ç±»å‹-iiæ— é•¿æœŸè®°å¿†çš„å¾®è°ƒ-llms-ä½œä¸ºè§„åˆ’å™¨&#34;&gt;&#xA;  ç±»å‹ IIï¼šæ— é•¿æœŸè®°å¿†çš„å¾®è°ƒ LLMs ä½œä¸ºè§„åˆ’å™¨ã€‚&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%b1%bb%e5%9e%8b-ii%e6%97%a0%e9%95%bf%e6%9c%9f%e8%ae%b0%e5%bf%86%e7%9a%84%e5%be%ae%e8%b0%83-llms-%e4%bd%9c%e4%b8%ba%e8%a7%84%e5%88%92%e5%99%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.08640.pdf&#34;&gt;&lt;strong&gt;LLaVA-Plus&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.18752.pdf&#34;&gt;&lt;strong&gt;GPT4Tools&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;ç±»å‹-ivå…·æœ‰æœ¬åœ°é•¿æœŸè®°å¿†çš„è§„åˆ’å™¨&#34;&gt;&#xA;  ç±»å‹ IVï¼šå…·æœ‰æœ¬åœ°é•¿æœŸè®°å¿†çš„è§„åˆ’å™¨ã€‚&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%b1%bb%e5%9e%8b-iv%e5%85%b7%e6%9c%89%e6%9c%ac%e5%9c%b0%e9%95%bf%e6%9c%9f%e8%ae%b0%e5%bf%86%e7%9a%84%e8%a7%84%e5%88%92%e5%99%a8&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.05997.pdf&#34;&gt;&lt;strong&gt;JARV IS-1&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.13771.pdf&#34;&gt;&lt;strong&gt;AppAgent&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.07162.pdf&#34;&gt;&lt;strong&gt;DLAH&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;å¤šæ¨¡æ€-agent1&#34;&gt;&#xA;  å¤šæ¨¡æ€ Agent[1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%a4%9a%e6%a8%a1%e6%80%81-agent1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;æ ¸å¿ƒç»„ä»¶&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;æ„ŸçŸ¥&lt;/strong&gt;ç»„ä»¶å…³æ³¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;è§„åˆ’å™¨&lt;/strong&gt;è´Ÿè´£æ¨ç†å’Œåˆ¶å®šè®¡åˆ’&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;è¡ŒåŠ¨&lt;/strong&gt;ç»„ä»¶æ‰§è¡Œè®¡åˆ’&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;è®°å¿†&lt;/strong&gt;ç»„ä»¶åˆ™æ¶‰åŠé•¿æœŸå’ŒçŸ­æœŸè®°å¿†&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å››ç§ç±»å‹&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;æ— é•¿æœŸè®°å¿†çš„é—­æº LLMs ä½œä¸ºè§„åˆ’å™¨&lt;/li&gt;&#xA;&lt;li&gt;æ— é•¿æœŸè®°å¿†çš„å¾®è°ƒ LLMs ä½œä¸ºè§„åˆ’å™¨&lt;/li&gt;&#xA;&lt;li&gt;å…·æœ‰é—´æ¥é•¿æœŸè®°å¿†çš„è§„åˆ’å™¨&lt;/li&gt;&#xA;&lt;li&gt;å…·æœ‰æœ¬åœ°é•¿æœŸè®°å¿†çš„è§„åˆ’å™¨&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;å¤šæ™ºèƒ½ä½“åä½œ&lt;/p&gt;</description>
    </item>
    <item>
      <title>(ç»¼è¿°)å¤šæ¨¡æ€</title>
      <link>https://www6v.github.io/www6vAlgo/docs/Vision/Survey/gptMultimodal/</link>
      <pubDate>Wed, 18 Jan 2023 09:48:37 +0000</pubDate>
      <guid>https://www6v.github.io/www6vAlgo/docs/Vision/Survey/gptMultimodal/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;ç›®å½•&#34;&gt;&#xA;  ç›®å½•&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%9b%ae%e5%bd%95&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;!-- toc --&gt;&#xA;&lt;h1 id=&#34;è®ºæ–‡foundational-models-defining&#34;&gt;&#xA;  è®ºæ–‡[Foundational Models Defining]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e8%ae%ba%e6%96%87foundational-models-defining&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;è®ºæ–‡åœ°å€&#xA;ã€ŠFoundational Models Defining a New Era in Vision: A Survey and Outlookã€‹å¤§å­¦&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;åŸºç¡€æ¨¡å‹åˆ†ç±»-1&#34;&gt;&#xA;  åŸºç¡€æ¨¡å‹åˆ†ç±» [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e5%88%86%e7%b1%bb-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;åˆ†ç±»&#xA;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/pattern.webp&#34; alt=&#34;pattern.webp&#34; /&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;åˆ†ç±»&#xA;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;images/pattern1.webp&#34; alt=&#34;pattern1.webp&#34; /&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;textually-prompted-models&#34;&gt;&#xA;  textually prompted models&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#textually-prompted-models&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;contrastive&#xA;CLIP  åŒå¡”&lt;/li&gt;&#xA;&lt;li&gt;generative&#xA;Flamingo&lt;/li&gt;&#xA;&lt;li&gt;hybrid&#xA;BLIP&lt;/li&gt;&#xA;&lt;li&gt;conversational&#xA;GPT-4ï¼Œ miniGPT4, LLaVa&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;ä¼ ç»Ÿä¸Šï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ä¸»è¦ç”¨äºéœ€è¦åŒæ—¶ç†è§£è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œéšç€CLIPå±•ç¤ºå‡ºçš„å“è¶Šæ€§èƒ½ï¼ŒåŸºäº&lt;strong&gt;è¯­è¨€ç›‘ç£çš„æ¨¡å‹&lt;/strong&gt;åœ¨æ˜¾è‘—ä¸Šå‡ï¼Œå¹¶æˆä¸ºä¸»æµæ–¹æ³•ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ¢ç´¢ä¾èµ–&lt;strong&gt;è¯­è¨€ä½œä¸ºä¸»è¦ç›‘ç£æ¥æº&lt;/strong&gt;çš„æ–¹æ³•ã€‚è¿™äº›ä»¥æ–‡æœ¬ä¸ºæç¤ºçš„æ¨¡å‹å¯ä»¥å¹¿æ³›åˆ†ä¸ºä¸‰ç§ä¸»è¦ç±»å‹ï¼šå¯¹æ¯”ã€ç”Ÿæˆå’Œæ··åˆæ–¹æ³•ã€‚&lt;/p&gt;&#xA;&lt;h3 id=&#34;visually-prompted-models&#34;&gt;&#xA;  visually prompted models&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#visually-prompted-models&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Foundational&#xA;SAM&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;heterogeneous--models&#34;&gt;&#xA;  heterogeneous  models&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#heterogeneous--models&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h1 id=&#34;æ¶æ„-1&#34;&gt;&#xA;  æ¶æ„ [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e6%9e%b6%e6%9e%84-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;{% asset_img  &amp;rsquo;&amp;rsquo; %}&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
